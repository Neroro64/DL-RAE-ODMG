{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "MIX4_withLabel.py\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import time\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import shutil\n",
    "import tempfile\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from cytoolz import sliding_window\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "# Prepare train data\n",
    "DATA_PATH = \"/home/nuoc/Documents/MEX/data\"\n",
    "MODEL_PATH = \"/home/nuoc/Documents/MEX/models\"\n",
    "RESULTS_PATH = \"/home/nuoc/Documents/MEX/results\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Test torch lightning + ray tune\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None, extra_feature_len:int=0,\n",
    "                 train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                 single_module:int=0):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        self.single_module = single_module\n",
    "        self.extra_feature_len = extra_feature_len\n",
    "\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.loss_fn = config[\"ae_loss_fn\"]\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "            dimensions.append(self.k)\n",
    "            self.dimensions = dimensions\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.test_set = test_set\n",
    "\n",
    "            self.best_val_loss = np.inf\n",
    "\n",
    "            self.build()\n",
    "            self.encoder.apply(self.init_params)\n",
    "            self.decoder.apply(self.init_params)\n",
    "\n",
    "    def build(self):\n",
    "        layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "        if self.single_module == -1 or self.single_module == 0:\n",
    "            layers = []\n",
    "\n",
    "            for i, size in enumerate(layer_sizes):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.encoder = nn.Sequential()\n",
    "\n",
    "        if self.single_module == 0 or self.single_module == 1:\n",
    "            layers = []\n",
    "            layer_sizes[-1] = (layer_sizes[-1][0], layer_sizes[-1][1] + self.extra_feature_len)\n",
    "            for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.decoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "    def encode(self, x):\n",
    "        _x, label = x[:, :-self.extra_feature_len], x[:, -self.extra_feature_len:]\n",
    "        # raise ValueError(\"x:{}, this:{}\".format(_x.size(), self.encoder))\n",
    "        h = self.encoder(_x)\n",
    "        return h, label\n",
    "\n",
    "    def decode(self, h, label):\n",
    "        hr = torch.cat((h, label), dim=1)\n",
    "        return self.decoder(hr)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_checkpoint(self, best_val_loss:float, checkpoint_dir=MODEL_PATH):\n",
    "        model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                 \"extra_feature_len\" : self.extra_feature_len,\n",
    "                 \"encoder\":self.encoder.state_dict(),\n",
    "                 \"decoder\":self.decoder.state_dict()}\n",
    "\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        filePath = os.path.join(path, str(best_val_loss)+\".\"+str(self.k)+str(time.time())+\".pbz2\")\n",
    "        with bz2.BZ2File(filePath, \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        return filePath\n",
    "    def load(self, state_dict1: 'OrderedDict[str, Tensor]'=None, state_dict2: 'OrderedDict[str, Tensor]'=None,\n",
    "                        strict: bool = True):\n",
    "\n",
    "        if self.single_module == -1 or self.single_module == 0:\n",
    "            self.encoder.load_state_dict(state_dict1, strict)\n",
    "        if self.single_module == 1 or self.single_module == 0:\n",
    "            self.decoder.load_state_dict(state_dict2, strict)\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filePath):\n",
    "        with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        model = MLP(name=obj[\"name\"], dimensions=obj[\"dimensions\"], extra_feature_len=obj[\"extra_feature_len\"], load=True)\n",
    "        model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def setup_data(self):\n",
    "        pass\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MIX(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dim1:int=None, dim2:int=None, dim3:int=None, dim4:int=None,\n",
    "                 extra_feature_len:int=0,  extra_feature_len2:int=0, train_set=None, val_set=None, test_set=None,\n",
    "                 name:str=\"model\"):\n",
    "\n",
    "        super(MIX, self).__init__()\n",
    "        self.name = name\n",
    "        if not config is None:\n",
    "            self.k = config[\"k\"]\n",
    "            self.hidden_dim = config[\"hidden_dim\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.loss_fn = config[\"loss_fn\"]\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "        self.dim3 = dim3\n",
    "        self.dim4 = dim4\n",
    "\n",
    "        self.dimension1 = [dim1-extra_feature_len , self.hidden_dim , self.k]\n",
    "        self.dimension2 = [dim2-extra_feature_len2, self.hidden_dim , self.k]\n",
    "        self.dimension3 = [dim3-extra_feature_len, self.hidden_dim , self.k]\n",
    "        self.dimension4 = [dim4-extra_feature_len2, self.hidden_dim , self.k]\n",
    "\n",
    "        self.model1 = MLP(config=config, dimensions=self.dimension1,\n",
    "                          extra_feature_len=extra_feature_len, name=\"M1\")\n",
    "        self.model2 = MLP(config=config, dimensions=self.dimension2,\n",
    "                          extra_feature_len=extra_feature_len2, name=\"M2\")\n",
    "        self.model3 = MLP(config=config, dimensions=self.dimension3,\n",
    "                          extra_feature_len=extra_feature_len, name=\"M3\")\n",
    "        self.model4 = MLP(config=config, dimensions=self.dimension4,\n",
    "                          extra_feature_len=extra_feature_len2, name=\"M4\")\n",
    "\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        self.test_set = test_set\n",
    "        self.best_val_loss = np.inf\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> (torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor):\n",
    "        i1 = self.dim1\n",
    "        i2 = i1+self.dim2\n",
    "        i3 = i2+self.dim3\n",
    "        input1, input2, input3, input4 = \\\n",
    "            x[:, :i1], x[:, i1:i2], x[:, i2:i3], x[:, i3:]\n",
    "\n",
    "\n",
    "        h1, l1 = self.model1.encode(input1)\n",
    "        h2, l2 = self.model2.encode(input2)\n",
    "        h3, l3 = self.model3.encode(input3)\n",
    "        h4, l4 = self.model4.encode(input4)\n",
    "\n",
    "        # raise ValueError(\n",
    "        #     \"x:{}, input1:{}, input2:{}, input3:{}, input4:{} \\n m1:{}, m2:{}, m3:{}, m4:{}\".format(\n",
    "        #         x.size(), input1.size(),input2.size(),input3.size(),input4.size(),\n",
    "        #         self.model1.encoder, self.model2.encoder, self.model3.encoder,self.model4.encoder\n",
    "        #     ))\n",
    "\n",
    "        # raise ValueError(\n",
    "        #     \"h1:{}, h2:{}, h3:{}, h4:{}, l1:{} l2:{}, l3:{}, l4:{}\".format(\n",
    "        #         h1.size(),h2.size(),h3.size(),h4.size(),l1.size(),l2.size(),l3.size(),l4.size()\n",
    "        #     ))\n",
    "\n",
    "        out1, out2, out3, out4 = \\\n",
    "            self.model1.decode(h1, l1), self.model2.decode(h2, l2), self.model3.decode(h3, l3), self.model4.decode(h4, l4)\n",
    "        return (out1, out2, out3, out4), (h1, h2, h3, h4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out, h = self(x)\n",
    "        loss = self.loss_fn(out, h, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        out, h = self(x)\n",
    "        loss = self.loss_fn(out, h, y)\n",
    "\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        out, h = self(x)\n",
    "        loss = self.loss_fn(out, h, y)\n",
    "\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir=MODEL_PATH):\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        loss = self.best_val_loss.cpu().numpy()\n",
    "        model_path1 = self.model1.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "        model_path2 = self.model2.save_checkpoint(best_val_loss=loss,checkpoint_dir=path)\n",
    "        model_path3 = self.model3.save_checkpoint(best_val_loss=loss,checkpoint_dir=path)\n",
    "        model_path4 = self.model4.save_checkpoint(best_val_loss=loss,checkpoint_dir=path)\n",
    "\n",
    "\n",
    "        model = {\"name\":self.name,\n",
    "                 \"model1_name\":self.model1.name,\n",
    "                 \"k\": self.k,\n",
    "                 \"model1_single_module\":self.model1.single_module, \"model1_path\" : model_path1,\n",
    "                 \"model2_name\":self.model2.name,\n",
    "                 \"model2_single_module\":self.model2.single_module, \"model2_path\" : model_path2,\n",
    "                 \"model3_name\":self.model3.name,\n",
    "                 \"model3_single_module\":self.model3.single_module, \"model3_path\" : model_path3,\n",
    "                 \"model4_name\":self.model4.name,\n",
    "                 \"model4_single_module\":self.model4.single_module, \"model4_path\" : model_path4,\n",
    "                 }\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        with bz2.BZ2File(os.path.join(path,\n",
    "                                      str(best_val_loss=loss)+\".\"+str(self.k)+\".pbz2\"), \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filename):\n",
    "        # return torch.load(os.path.join(checkpoint_dir,filename))\n",
    "        with bz2.BZ2File(filename, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        print(obj[\"model1_path\"])\n",
    "        model1 = MLP.load_checkpoint(obj[\"model1_path\"])\n",
    "        model2 = MLP.load_checkpoint(obj[\"model2_path\"])\n",
    "        model3 = MLP.load_checkpoint(obj[\"model3_path\"])\n",
    "        model4 = MLP.load_checkpoint(obj[\"model4_path\"])\n",
    "\n",
    "        model = MIX(name=obj[\"name\"])\n",
    "        model.model1 = model1\n",
    "        model.model2 = model2\n",
    "        model.model3 = model3\n",
    "        model.model4 = model4\n",
    "\n",
    "        return model\n",
    "        # self.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        # self.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        # self.best_val_loss = obj[\"val_loss\"]\n",
    "\n",
    "    def setup_data(self): pass\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_tune(config, dim1, dim2, dim3,dim4, extra_feature_len:int=0,extra_feature_len2:int=0,\n",
    "                 train_set=None, val_set=None, test_set=None,\n",
    "                 num_epochs=300, num_cpus=24, num_gpus=1, model_name=\"model\"):\n",
    "\n",
    "    model = MIX(config=config, dim1=dim1, dim2=dim2, dim3=dim3, dim4=dim4,\n",
    "                extra_feature_len=extra_feature_len, extra_feature_len2=extra_feature_len2,\n",
    "                train_set=train_set, val_set=val_set, test_set=test_set, name=model_name)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(save_dir=\"logs/\", name=model_name, version=\"0.0\"),\n",
    "        progress_bar_refresh_rate=20,\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"loss\":\"avg_val_loss\",}, on=\"validation_end\"),\n",
    "            EarlyStopping(monitor=\"avg_val_loss\")\n",
    "        ],\n",
    "        precision=16,\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def prepare_data(datasets:list, featureList:list, extra_feature_len:int=0,\n",
    "                 train_ratio:float=0.8, val_ratio:float=0.2, test_size:int=100, SEED:int=2021):\n",
    "   # process data\n",
    "    data = [func.processData(d, featureList, shutdown=False) for d in datasets]\n",
    "    input_data = [np.vstack(d) for d in data]\n",
    "    x_tensors = [func.normaliseT(torch.from_numpy(x).float()) for x in input_data]\n",
    "    y_tensors = [torch.from_numpy(x[:, :-extra_feature_len]).float() for x in input_data]\n",
    "\n",
    "    # prepare datasets\n",
    "    test_sets = [(x_tensor[-test_size:], y_tensor[-test_size:]) for x_tensor, y_tensor in zip(x_tensors, y_tensors)]\n",
    "    x_training = torch.vstack([x_tensor[:-test_size] for x_tensor in x_tensors])\n",
    "    y_training = torch.vstack([y_tensor[:-test_size] for y_tensor in y_tensors])\n",
    "    dataset = TensorDataset(x_training, y_training)\n",
    "    N = len(x_training)\n",
    "\n",
    "    train_ratio = int(train_ratio*N)\n",
    "    val_ratio = int(val_ratio*N)\n",
    "    print(\"Train: \", train_ratio, \", Validation: \", val_ratio)\n",
    "    train_set, val_set = random_split(dataset, [train_ratio, val_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "    return train_set, val_set, test_sets\n",
    "\n",
    "def train(train_set:Dataset, val_set:Dataset, dims:list,\n",
    "          config:dict, EPOCHS:int=300, extra_feature_len:int=0,extra_feature_len2:int=0,\n",
    "          n_gpu=1, n_samples=20, model_name=\"model\",\n",
    "          ):\n",
    "\n",
    "    dim1, dim2, dim3, dim4 = dims[0], dims[1], dims[2],dims[3]\n",
    "\n",
    "    scheduler = ASHAScheduler(max_t = EPOCHS, grace_period=1, reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"k\", \"lr\", \"batch_size\", \"loss_fn\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"],\n",
    "        max_error_rows=5,\n",
    "        max_progress_rows=5,\n",
    "        max_report_frequency=10)\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_tune,\n",
    "            dim1=dim1, dim2=dim2, dim3=dim3, dim4=dim4,\n",
    "            extra_feature_len=extra_feature_len,\n",
    "            extra_feature_len2=extra_feature_len2,\n",
    "            train_set = train_set, val_set = val_set,\n",
    "            num_epochs = EPOCHS,\n",
    "            num_gpus=n_gpu,\n",
    "            model_name=model_name\n",
    "        ),\n",
    "        resources_per_trial= {\"cpu\":1, \"gpu\":n_gpu},\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=n_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=model_name,\n",
    "        verbose=False,\n",
    "        checkpoint_freq=0,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"loss\",\n",
    "        checkpoint_at_end=True\n",
    "    )\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(\"Done\")\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    print(\"Best achieved loss was: \", analysis.best_result)\n",
    "    print(\"-\"*70)\n",
    "\n",
    "def clean_checkpoints(num_keep=3, path=\"../../models\"):\n",
    "    for dir, dname, files in os.walk(path):\n",
    "        saved_checkpoints = []\n",
    "        for fname in files:\n",
    "            fname = fname.split(\".\")\n",
    "            saved_checkpoints.append(fname)\n",
    "        print(\"Num checkpoints in {}: {}\".format(dir, len(saved_checkpoints)))\n",
    "\n",
    "        saved_checkpoints.sort(key = lambda x: x[0]+x[1])\n",
    "        for filename in saved_checkpoints[num_keep:]:\n",
    "            os.remove(os.path.join(dir,\".\".join(filename)))\n",
    "\n",
    "def test(model:torch.nn.Module, test_sets:list, loss_fn, set_names=list,\n",
    "         save=True, path=\"../../results\", model_name=\"model\"):\n",
    "    # Intra test performance\n",
    "    with torch.no_grad():\n",
    "        df = {}\n",
    "        if set_names is None: set_names = np.arange(len(test_sets))\n",
    "        for i,t1 in enumerate(test_sets):\n",
    "            for j, t2 in enumerate(test_sets):\n",
    "                x = t1[0]\n",
    "                y = t2[1]\n",
    "                out1, out2, h1, h2 = model(x)\n",
    "                loss = loss_fn(out1, out2, h1, h2, y)\n",
    "                df[\"{}-{}\".format(set_names[i], set_names[j])] = [loss.cpu().numpy()]\n",
    "                print(\"Test encoding {} to {}, MSE={:.2f}\".format(set_names[i], set_names[j], loss))\n",
    "        filepath = os.path.join(path, model_name)\n",
    "        if not os.path.exists(filepath): os.mkdir(filepath)\n",
    "        pd.DataFrame(df).to_csv(os.path.join(filepath, \"tests.csv\"))\n",
    "\n",
    "def test2(model:torch.nn.Module, test_sets:list, loss_fn, set_names=list,\n",
    "         save=True, path=\"../../results\", model_name=\"model\"):\n",
    "    # Intra test performance\n",
    "    with torch.no_grad():\n",
    "        df = {}\n",
    "        if set_names is None: set_names = np.arange(len(test_sets))\n",
    "        for i,t1 in enumerate(test_sets):\n",
    "            x = t1[0]\n",
    "            y = t1[1]\n",
    "            out, h = model(x)\n",
    "            loss = loss_fn(out, h, y)\n",
    "            df[\"{}\".format(set_names[i])] = [loss.cpu().numpy()]\n",
    "            print(\"Test encod>ing {} = {:.2f}\".format(set_names[i], loss))\n",
    "        filepath = os.path.join(path, model_name)\n",
    "        if not os.path.exists(filepath): os.mkdir(filepath)\n",
    "        pd.DataFrame(df).to_csv(os.path.join(filepath, \"tests.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_multi_model(datapaths:list, featureList:list, config:dict=None,\n",
    "                      extra_feature_len:int=0, extra_feature_len2:int=0,\n",
    "                      n_samples:int=30, model_name:str=\"model\", loss_fn=nn.functional.mse_loss,\n",
    "                      dataset_names:list=None):\n",
    "    # load data\n",
    "    datasets1 = [func.load(os.path.join(DATA_PATH,path)) for path in datapaths[0]]\n",
    "    datasets2 = [func.load(os.path.join(DATA_PATH,path)) for path in datapaths[1]]\n",
    "    datasets3 = [func.load(os.path.join(DATA_PATH,path)) for path in datapaths[2]]\n",
    "    datasets4 = [func.load(os.path.join(DATA_PATH,path)) for path in datapaths[3]]\n",
    "\n",
    "    train_set1, val_set1, test_set1 = prepare_data(datasets1, featureList, extra_feature_len=extra_feature_len)\n",
    "    train_set2, val_set2, test_set2 = prepare_data(datasets2, featureList, extra_feature_len=extra_feature_len2)\n",
    "    train_set3, val_set3, test_set3 = prepare_data(datasets3, featureList, extra_feature_len=extra_feature_len)\n",
    "    train_set4, val_set4, test_set4 = prepare_data(datasets4, featureList, extra_feature_len=extra_feature_len2)\n",
    "\n",
    "    dims = [len(train_set1[0][0]),len(train_set2[0][0]),len(train_set3[0][0]),len(train_set4[0][0])]\n",
    "    train_set = [(torch.cat([x[0],y[0],z[0],w[0]],dim=0),torch.cat([x[1],y[1],z[1],w[1]],dim=0))\n",
    "                 for x, y, z, w in zip(train_set1, train_set2, train_set3, train_set4)]\n",
    "    val_set = [(torch.cat([x[0],y[0],z[0],w[0]],dim=0),torch.cat([x[1],y[1],z[1],w[1]],dim=0))\n",
    "                 for x, y, z, w in zip(val_set1, val_set2, val_set3, val_set4)]\n",
    "\n",
    "    train(train_set=train_set, val_set=val_set, config=config, dims=dims,\n",
    "          extra_feature_len=extra_feature_len, extra_feature_len2=extra_feature_len2,\n",
    "          n_samples=n_samples, model_name=model_name)\n",
    "\n",
    "\n",
    "    # test_set = [(torch.cat([x[0],y[0],z[0],w[0]],dim=0),torch.cat([x[1],y[1],z[1],w[1]],dim=0))\n",
    "                 # for x, y, z, w in zip(test_set1, test_set2, test_set3, test_set4)]\n",
    "    # clean_checkpoints(path=MODEL_PATH)\n",
    "    # best_model = MIX.load_checkpoint(best_model)\n",
    "    return [test_set1, test_set2, test_set3, test_set4]\n",
    "    # test2(best_model, test_set, loss_fn=loss_fn, set_names=dataset_names, path=RESULTS_PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def mse_loss(out, h, y):\n",
    "    return nn.functional.mse_loss(torch.cat((out[0], out[1], out[2], out[3]), dim=1), y)\n",
    "\n",
    "def mse_mse_loss(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.mse_loss(out[0], y0)\n",
    "    mse1 = nn.functional.mse_loss(out[1], y1)\n",
    "    mse2 = nn.functional.mse_loss(out[2], y2)\n",
    "    mse3 = nn.functional.mse_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.mse_loss(h[0], h[1])\n",
    "    s_loss1 = nn.functional.mse_loss(h[0], h[2])\n",
    "    s_loss2 = nn.functional.mse_loss(h[0], h[3])\n",
    "    s_loss3 = nn.functional.mse_loss(h[1], h[2])\n",
    "    s_loss4 = nn.functional.mse_loss(h[1], h[3])\n",
    "    s_loss5 = nn.functional.mse_loss(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3) / 4\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5) / 6\n",
    "    return (mse + similarity_loss) / 2.0\n",
    "\n",
    "\n",
    "def mae_mae_loss(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.smooth_l1_loss(out[0], y0)\n",
    "    mse1 = nn.functional.smooth_l1_loss(out[1], y1)\n",
    "    mse2 = nn.functional.smooth_l1_loss(out[2], y2)\n",
    "    mse3 = nn.functional.smooth_l1_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.smooth_l1_loss(h[0], h[1])\n",
    "    s_loss1 = nn.functional.smooth_l1_loss(h[0], h[2])\n",
    "    s_loss2 = nn.functional.smooth_l1_loss(h[0], h[3])\n",
    "    s_loss3 = nn.functional.smooth_l1_loss(h[1], h[2])\n",
    "    s_loss4 = nn.functional.smooth_l1_loss(h[1], h[3])\n",
    "    s_loss5 = nn.functional.smooth_l1_loss(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3) / 4\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5) / 6\n",
    "    return (mse + similarity_loss) / 2.0\n",
    "\n",
    "def mse_mae_loss(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.mse_loss(out[0], y0)\n",
    "    mse1 = nn.functional.mse_loss(out[1], y1)\n",
    "    mse2 = nn.functional.mse_loss(out[2], y2)\n",
    "    mse3 = nn.functional.mse_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.smooth_l1_loss(h[0], h[1])\n",
    "    s_loss1 = nn.functional.smooth_l1_loss(h[0], h[2])\n",
    "    s_loss2 = nn.functional.smooth_l1_loss(h[0], h[3])\n",
    "    s_loss3 = nn.functional.smooth_l1_loss(h[1], h[2])\n",
    "    s_loss4 = nn.functional.smooth_l1_loss(h[1], h[3])\n",
    "    s_loss5 = nn.functional.smooth_l1_loss(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3) / 4\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5) / 6\n",
    "    return (mse + similarity_loss) / 2.0\n",
    "\n",
    "def mse_mae_loss2(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.mse_loss(out[0], y0)\n",
    "    mse1 = nn.functional.mse_loss(out[1], y1)\n",
    "    mse2 = nn.functional.mse_loss(out[2], y2)\n",
    "    mse3 = nn.functional.mse_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.smooth_l1_loss(h[0], h[1])\n",
    "    s_loss1 = nn.functional.smooth_l1_loss(h[0], h[2])\n",
    "    s_loss2 = nn.functional.smooth_l1_loss(h[0], h[3])\n",
    "    s_loss3 = nn.functional.smooth_l1_loss(h[1], h[2])\n",
    "    s_loss4 = nn.functional.smooth_l1_loss(h[1], h[3])\n",
    "    s_loss5 = nn.functional.smooth_l1_loss(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3)\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5)\n",
    "    return (mse + similarity_loss) / 2.0\n",
    "\n",
    "def mse_kl_loss(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.mse_loss(out[0], y0)\n",
    "    mse1 = nn.functional.mse_loss(out[1], y1)\n",
    "    mse2 = nn.functional.mse_loss(out[2], y2)\n",
    "    mse3 = nn.functional.mse_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.kl_div(h[0], h[1])\n",
    "    s_loss1 = nn.functional.kl_div(h[0], h[2])\n",
    "    s_loss2 = nn.functional.kl_div(h[0], h[3])\n",
    "    s_loss3 = nn.functional.kl_div(h[1], h[2])\n",
    "    s_loss4 = nn.functional.kl_div(h[1], h[3])\n",
    "    s_loss5 = nn.functional.kl_div(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3) / 4\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5) / 6\n",
    "    return (mse + similarity_loss) / 2.0\n",
    "\n",
    "def mse_nll_loss(out, h, y):\n",
    "    i1 = out[0].size()[-1]\n",
    "    i2 = i1 + out[1].size()[-1]\n",
    "    i3 = i2 + out[2].size()[-1]\n",
    "    y0, y1, y2, y3 = y[:, :i1], y[:, i1:i2],y[:, i2:i3], y[:, i3:]\n",
    "\n",
    "    mse0 = nn.functional.mse_loss(out[0], y0)\n",
    "    mse1 = nn.functional.mse_loss(out[1], y1)\n",
    "    mse2 = nn.functional.mse_loss(out[2], y2)\n",
    "    mse3 = nn.functional.mse_loss(out[3], y3)\n",
    "\n",
    "    s_loss0 = nn.functional.nll_loss(h[0], h[1])\n",
    "    s_loss1 = nn.functional.nll_loss(h[0], h[2])\n",
    "    s_loss2 = nn.functional.nll_loss(h[0], h[3])\n",
    "    s_loss3 = nn.functional.nll_loss(h[1], h[2])\n",
    "    s_loss4 = nn.functional.nll_loss(h[1], h[3])\n",
    "    s_loss5 = nn.functional.nll_loss(h[2], h[3])\n",
    "\n",
    "    mse = (mse0 + mse1 + mse2 + mse3) / 4\n",
    "    similarity_loss = (s_loss0 + s_loss1 + s_loss2 + s_loss3 + s_loss4 + s_loss5) / 6\n",
    "    return (mse + similarity_loss) / 2.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "datapath1 = [\"LOCO_R1-default-locomotion.pbz2\",\n",
    "             \"LOCO_R1-default-locomotion-small.pbz2\",\n",
    "             \"LOCO_R1-default-locomotion-large.pbz2\"]\n",
    "datapath2 = [\"LOCO_R3-default-locomotion.pbz2\",\n",
    "             \"LOCO_R3-default-locomotion-small.pbz2\",\n",
    "             \"LOCO_R3-default-locomotion-large.pbz2\"]\n",
    "datapath3 = [\"LOCO_R2-default-locomotion.pbz2\",\n",
    "             \"LOCO_R2-default-locomotion-small.pbz2\",\n",
    "             \"LOCO_R2-default-locomotion-large.pbz2\"]\n",
    "datapath4 = [\"LOCO_R4-default-locomotion.pbz2\",\n",
    "             \"LOCO_R4-default-locomotion-small.pbz2\",\n",
    "             \"LOCO_R4-default-locomotion-large.pbz2\"]\n",
    "\n",
    "featureList = [\"pos\", \"rotMat\", \"velocity\", \"isLeft\", \"chainPos\", \"geoDistanceNormalised\"]\n",
    "\n",
    "extra_feature_len = 21 * 3\n",
    "extra_feature_len2 = 31 * 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Keeping the previous outputs\n",
    "# config = {\n",
    "#     \"k\": tune.randint(6, 256),\n",
    "#     \"hidden_dim\" : tune.choice([64, 128, 256, 512]),\n",
    "#     \"lr\": tune.loguniform(1e-2, 1e-7),\n",
    "#     \"batch_size\":tune.choice([5, 15, 30, 60]),\n",
    "#     \"loss_fn\":tune.choice([mse_mse_loss, mse_mae_loss, mae_mae_loss, mse_kl_loss, mse_mae_loss2]),\n",
    "#     \"ae_loss_fn\":tune.choice([mse_loss])\n",
    "# }\n",
    "#\n",
    "# test_sets = train_multi_model([datapath1,datapath2,datapath3,datapath4], featureList, config,\n",
    "#                   extra_feature_len=extra_feature_len, extra_feature_len2=extra_feature_len2,\n",
    "#                     n_samples=30, model_name=\"MIX4_extra2\", loss_fn=mse_loss,\n",
    "#                        dataset_names=[\"R1\",\"R2\",\"R3\",\"R4\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# obj = {\"data\":test_sets}\n",
    "# with bz2.BZ2File(os.path.join(MODEL_PATH, \"MIX4_extra2\", \"test_sets.pbz2\"), \"w\") as f:\n",
    "#         pickle.dump(obj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MIX4_extra3: 21\n",
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MIX4_extra3/M1: 21\n",
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MIX4_extra3/M2: 21\n",
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MIX4_extra3/M3: 21\n",
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MIX4_extra3/M4: 21\n"
     ]
    }
   ],
   "source": [
    "clean_checkpoints(path=os.path.join(MODEL_PATH, \"MIX4_extra3\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-5d533a32e9ae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m }\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m test_sets = train_multi_model([datapath1,datapath2,datapath3,datapath4], featureList, config,\n\u001B[0m\u001B[1;32m     11\u001B[0m                   \u001B[0mextra_feature_len\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mextra_feature_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextra_feature_len2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mextra_feature_len2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m                     \u001B[0mn_samples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m30\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"MIX4_extra3\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmse_loss\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-23a7e5f3d120>\u001B[0m in \u001B[0;36mtrain_multi_model\u001B[0;34m(datapaths, featureList, config, extra_feature_len, extra_feature_len2, n_samples, model_name, loss_fn, dataset_names)\u001B[0m\n\u001B[1;32m      4\u001B[0m                       dataset_names:list=None):\n\u001B[1;32m      5\u001B[0m     \u001B[0;31m# load data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mdatasets1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mdatasets2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mdatasets3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-23a7e5f3d120>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m                       dataset_names:list=None):\n\u001B[1;32m      5\u001B[0m     \u001B[0;31m# load data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mdatasets1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mdatasets2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mdatasets3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatapaths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/MEX/src/func.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(file_path)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \"\"\"\n\u001B[1;32m     31\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mbz2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBZ2File\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/bz2.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_can_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/_compression.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mmemoryview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mview\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mview\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"B\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mbyte_view\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbyte_view\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m             \u001B[0mbyte_view\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/_compression.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    101\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m                     \u001B[0mrawblock\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mb\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_decompressor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecompress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrawblock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"k\": tune.randint(6, 256),\n",
    "    \"hidden_dim\" : tune.choice([64, 128, 256, 512]),\n",
    "    \"lr\": tune.loguniform(1e-2, 1e-7),\n",
    "    \"batch_size\":tune.choice([5, 15, 30, 60]),\n",
    "    \"loss_fn\":tune.choice([mse_mse_loss, mse_mae_loss, mae_mae_loss, mse_kl_loss, mse_mae_loss2]),\n",
    "    \"ae_loss_fn\":tune.choice([mse_loss])\n",
    "}\n",
    "\n",
    "test_sets = train_multi_model([datapath1,datapath2,datapath3,datapath4], featureList, config,\n",
    "                  extra_feature_len=extra_feature_len, extra_feature_len2=extra_feature_len2,\n",
    "                    n_samples=30, model_name=\"MIX4_extra3\", loss_fn=mse_loss,\n",
    "                       dataset_names=[\"R1\",\"R2\",\"R3\",\"R4\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj = {\"data\":test_sets}\n",
    "with bz2.BZ2File(os.path.join(MODEL_PATH, \"MIX4_extra3\", \"test_sets.pbz2\"), \"w\") as f:\n",
    "        pickle.dump(obj, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_checkpoints(num_keep=3, path=\"../../models\"):\n",
    "    for dir, dname, files in os.walk(path):\n",
    "        saved_checkpoints = []\n",
    "        for fname in files:\n",
    "            fname = fname.split(\".\")\n",
    "            saved_checkpoints.append(fname)\n",
    "\n",
    "        saved_checkpoints.sort(key=lambda x: x[0] + x[1])\n",
    "        for filename in saved_checkpoints[num_keep:]:\n",
    "            os.remove(os.path.join(dir, \".\".join(filename)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}