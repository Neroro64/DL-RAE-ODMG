{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from cytoolz import sliding_window, accumulate, get\n",
    "import pytorch_lightning as pl\n",
    "from operator import add\n",
    "from tabulate import tabulate\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../rig_agnostic_encoding\")\n",
    "sys.path.append(\"../rig_agnostic_encoding/models\")\n",
    "sys.path.append(\"../rig_agnostic_encoding/functions\")\n",
    "import func\n",
    "# from MLP_withLabel import MLP_withLabel\n",
    "# from MLP import MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/nuoc/Documents/MEX/data\"\n",
    "MODEL_PATH = \"/home/nuoc/Documents/MEX/models\"\n",
    "RESULTS_PATH = \"/home/nuoc/Documents/MEX/results\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MLP_withLabel(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None, extra_feature_len:int=0,\n",
    "                 train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                 single_module:int=0):\n",
    "\n",
    "        super(MLP_withLabel, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        self.single_module = single_module\n",
    "        self.extra_feature_len = extra_feature_len\n",
    "        self.act = nn.ELU\n",
    "        self.k = 0\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.hidden_dim = config[\"hidden_dim\"]\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.act = config[\"activation\"]\n",
    "            self.loss_fn = config[\"ae_loss_fn\"]\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "            self.dimensions = [self.dimensions[0]-extra_feature_len, self.hidden_dim, self.k]\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.test_set = test_set\n",
    "\n",
    "            self.best_val_loss = np.inf\n",
    "\n",
    "        self.build()\n",
    "        self.encoder.apply(self.init_params)\n",
    "        self.decoder.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "        if self.single_module == -1 or self.single_module == 0:\n",
    "            layers = []\n",
    "            for i, size in enumerate(layer_sizes):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), self.act()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.encoder = nn.Sequential()\n",
    "\n",
    "        if self.single_module == 0 or self.single_module == 1:\n",
    "            layers = []\n",
    "            layer_sizes[-1] = (layer_sizes[-1][0], layer_sizes[-1][1] + self.extra_feature_len)\n",
    "            for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), self.act()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.decoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(*self.encode(x))\n",
    "\n",
    "    def encode(self, x):\n",
    "        _x, label = x[:, :-self.extra_feature_len], x[:, -self.extra_feature_len:]\n",
    "        h = self.encoder(_x)\n",
    "        return h, label\n",
    "\n",
    "    def decode(self, h, label):\n",
    "        hr = torch.cat((h, label), dim=1)\n",
    "        return self.decoder(hr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint(best_val_loss=self.best_val_loss.cpu().numpy())\n",
    "\n",
    "    def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "        model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                 \"extra_feature_len\" : self.extra_feature_len,\n",
    "                 \"encoder\":self.encoder.state_dict(),\n",
    "                 \"decoder\":self.decoder.state_dict()}\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        filePath = os.path.join(path, str(best_val_loss)+\".\"+str(self.k)+\".pbz2\")\n",
    "        with bz2.BZ2File(filePath, \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        return filePath\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filePath):\n",
    "        with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        model = MLP_withLabel(name=obj[\"name\"], dimensions=obj[\"dimensions\"], extra_feature_len=obj[\"extra_feature_len\"], keep_prob=obj[\"keep_prob\"], load=True)\n",
    "        model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def setup_data(self):\n",
    "        pass\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None,\n",
    "                 train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                 single_module:int=0):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        self.single_module = single_module\n",
    "        self.act = nn.ELU\n",
    "        self.k = 0\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.hidden_dim = config[\"hidden_dim\"]\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.act = config[\"activation\"]\n",
    "            self.loss_fn = config[\"loss_fn\"]\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "            self.dimensions = dimensions + [self.hidden_dim, self.k]\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.test_set = test_set\n",
    "\n",
    "            self.best_val_loss = np.inf\n",
    "\n",
    "            self.build()\n",
    "        self.encoder.apply(self.init_params)\n",
    "        self.decoder.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "        if self.single_module == -1 or self.single_module == 0:\n",
    "            layers = []\n",
    "            for i, size in enumerate(layer_sizes):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), self.act()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.encoder = nn.Sequential()\n",
    "\n",
    "        if self.single_module == 0 or self.single_module == 1:\n",
    "            layers = []\n",
    "            for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), self.act()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.decoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, h):\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint(best_val_loss=self.best_val_loss.cpu().numpy())\n",
    "\n",
    "    def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "        model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                 \"single_module\":self.single_module,\n",
    "                 \"encoder\":self.encoder.state_dict(),\n",
    "                 \"decoder\":self.decoder.state_dict()}\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        filePath = os.path.join(path, str(best_val_loss)+\".\"+str(self.k)+\".pbz2\")\n",
    "        with bz2.BZ2File(filePath, \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        return filePath\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filePath):\n",
    "        with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        model = MLP(name=obj[\"name\"], dimensions=obj[\"dimensions\"], keep_prob=obj[\"keep_prob\"],\n",
    "                  load=True)\n",
    "        model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        # model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def setup_data(self):\n",
    "        pass\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, config=None, dimensions=None, phase_input_dim:int=0,\n",
    "                 gate_size=0, k_experts=1, keep_prob=.2,\n",
    "                 name=\"model\", load=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.phase_input_dim = phase_input_dim\n",
    "        self.dimensions = dimensions\n",
    "        self.act_fn = nn.ELU\n",
    "        self.name = name\n",
    "        self.config=config\n",
    "        self.gate_size=gate_size\n",
    "        self.k_experts = k_experts\n",
    "        self.keep_prob = .2\n",
    "        if not load:\n",
    "            self.k_experts = config[\"k_experts\"]\n",
    "            self.gate_size = config[\"gate_size\"]\n",
    "            self.keep_prob = config[\"keep_prob\"]\n",
    "            self.dimensions = [self.dimensions[0], config[\"hidden_dim\"], config[\"hidden_dim\"], self.dimensions[-1]]\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.build()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(phase_input_dim, self.gate_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.gate_size, self.gate_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.gate_size, self.k_experts)\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor, phase) -> torch.Tensor:\n",
    "        coefficients = F.softmax(self.gate(phase), dim=1)\n",
    "\n",
    "        layer_out = x\n",
    "        for (weight, bias, activation) in self.layers:\n",
    "            if weight is None:\n",
    "                layer_out = activation(layer_out, p=self.keep_prob)\n",
    "            else:\n",
    "                flat_weight = weight.flatten(start_dim=1, end_dim=2)\n",
    "                mixed_weight = torch.matmul(coefficients, flat_weight).view(\n",
    "                    coefficients.shape[0], *weight.shape[1:3]\n",
    "                )\n",
    "\n",
    "                input = layer_out.unsqueeze(1)\n",
    "                mixed_bias = torch.matmul(coefficients, bias).unsqueeze(1)\n",
    "                out = torch.baddbmm(mixed_bias, input, mixed_weight).squeeze(1)\n",
    "                layer_out = activation(out) if activation is not None else out\n",
    "\n",
    "        return layer_out\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            if i < len(self.dimensions) - 2:\n",
    "                layers.append(\n",
    "                    (\n",
    "                        nn.Parameter(torch.empty(self.k_experts, size[0], size[1])),\n",
    "                        nn.Parameter(torch.empty(self.k_experts, size[1])),\n",
    "                        self.act_fn()\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    (\n",
    "                        nn.Parameter(torch.empty(self.k_experts, size[0], size[1])),\n",
    "                        nn.Parameter(torch.empty(self.k_experts, size[1])),\n",
    "                        None\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if self.keep_prob > 0:\n",
    "                layers.append((None, None, F.dropout))\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def init_params(self):\n",
    "        for i, (w, b, _) in enumerate(self.layers):\n",
    "            if w is None:\n",
    "                continue\n",
    "\n",
    "            i = str(i)\n",
    "            torch.nn.init.kaiming_uniform_(w)\n",
    "            b.data.fill_(0.01)\n",
    "            self.register_parameter(\"w\" + i, w)\n",
    "            self.register_parameter(\"b\" + i, b)\n",
    "\n",
    "    def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "        model = {\"dimensions\":self.dimensions,\n",
    "                 \"name\":self.name,\n",
    "                 \"gate\":self.gate.state_dict(), \"phase_input_dim\":self.phase_input_dim,\n",
    "                 \"generationNetwork\":self.state_dict(),\n",
    "                 \"gate_size\":self.gate_size,\n",
    "                 \"k_experts\":self.k_experts,\n",
    "                 }\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        filePath = os.path.join(path, str(best_val_loss)+\".pbz2\")\n",
    "        with bz2.BZ2File(filePath, \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        return filePath\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filePath):\n",
    "        with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        model = MoE(name=obj[\"name\"], dimensions=obj[\"dimensions\"], gate_size=obj[\"gate_size\"],k_experts=obj[\"k_experts\"],\n",
    "                    phase_input_dim=obj[\"phase_input_dim\"], load=True)\n",
    "        model.gate.load_state_dict(obj[\"gate\"])\n",
    "        model.load_state_dict(obj[\"generationNetwork\"])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.99)\n",
    "        return optimizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MotionGenerationModel(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, pose_autoencoder=None, cost_input_dimension=None, feature_dims=None,\n",
    "                 input_slicers:list=None, output_slicers:list=None, train_set=None, val_set=None, name=\"model\", load=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if not load:\n",
    "            self.pose_autoencoder = pose_autoencoder # start with 3\n",
    "            cost_hidden_dim = config[\"cost_hidden_dim\"]\n",
    "            cost_output_dim = config[\"cost_output_dim\"]\n",
    "            self.feature_dims = feature_dims\n",
    "            self.phase_dim = feature_dims[\"phase_vec\"]\n",
    "            # self.trajectory_dim = feature_dims[\"tPos\"]\n",
    "            # self.cost_dim = trajectory_dim + feature_dims[\"posCost\"]\n",
    "            self.cost_encoder = MLP(dimensions=[cost_dim, cost_hidden_dim, cost_hidden_dim, cost_output_dim],\n",
    "                                    name=\"CostEncoder\", load=True, single_module=-1)\n",
    "\n",
    "           # phase_input_dimension = input_slicers[0]\n",
    "            moe_input_dim = pose_autoencoder.dimensions[-1] # + cost_output_dim\n",
    "            moe_output_dim = pose_autoencoder.dimensions[-1] +  self.phase_dim # + self.trajectory_dim\n",
    "            self.generationModel =  MoE(config=config, dimensions=[moe_input_dim, moe_output_dim], phase_input_dim=self.phase_dim + feature_dims[\"posCost\"],\n",
    "                                        name=\"MixtureOfExperts\")\n",
    "\n",
    "            self.in_slices = [0] + list(accumulate(add, input_slicers))\n",
    "            self.out_slices = [0] + list(accumulate(add, output_slicers))\n",
    "            # self.phase_dim = phase_dim\n",
    "\n",
    "            self.config=config\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.loss_fn = config[\"loss_fn\"]\n",
    "            self.window_size = config[\"window_size\"]\n",
    "            self.autoregress_chunk_size = config[\"autoregress_chunk_size\"]\n",
    "            self.autoregress_prob = config[\"autoregress_prob\"]\n",
    "            self.autoregress_inc = config[\"autoregress_inc\"]\n",
    "            self.best_val_loss = np.inf\n",
    "            self.phase_smooth_factor = 0.9\n",
    "\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        self.name = name\n",
    "        self.epochs = 0\n",
    "        self.automatic_optimization = False\n",
    "        self.left_id = 14*3\n",
    "        self.right_id = 20*3\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_tensors = [x[:, d0:d1] for d0, d1 in zip(self.in_slices[:-1], self.in_slices[1:])]\n",
    "        pose_h, pose_label = self.pose_autoencoder.encode(x_tensors[1])\n",
    "        phase = x_tensors[0][:, :self.phase_dim]\n",
    "        targets = x_tensors[0][:, self.phase_dim:]\n",
    "        # embedding = torch.cat([pose_h, (x_tensors[2])], dim=1)\n",
    "        # embedding = torch.cat([pose_h)], dim=1)\n",
    "        # embedding = torch.cat([pose_h, x_tensors[2]], dim=1)\n",
    "\n",
    "        # posCost = self.computeCost(targets, x_tensors[-1][:, :self.trajectory_dim])\n",
    "        # out = self.generationModel(embedding, torch.cat([phase, x_tensors[-1][:, self.trajectory_dim:]], dim=1))\n",
    "        out = self.generationModel(pose_h, torch.cat([phase, x_tensors[-1]], dim=1))\n",
    "\n",
    "        out_tensors = [out[:, d0:d1] for d0, d1 in zip(self.out_slices[:-1], self.out_slices[1:])] # phase_update, new_pose\n",
    "\n",
    "        phase = self.update_phase(phase, out_tensors[0]) # phase_0, phase_1, phase_update\n",
    "\n",
    "        new_pose = self.pose_autoencoder.decode(out_tensors[1], pose_label)\n",
    "\n",
    "        # out_tensors[-1][:, 9:12] = new_pose[:, self.left_id:self.left_id+3]\n",
    "        #\n",
    "        # out_tensors[-1][:, 27:30] = new_pose[:, self.right_id:self.right_id+3]\n",
    "\n",
    "\n",
    "        posCost = self.computeCost(targets, torch.cat([new_pose[:, self.left_id:self.left_id+3],new_pose[:, self.right_id:self.right_id+3]], dim=1))\n",
    "\n",
    "        # print(phase.size(), targets.size(), new_pose.size(), pose_label.size(), out_tensors[-1].size(), posCost.size(), rotCost.size())\n",
    "        # return [phase, targets, new_pose, pose_label, out_tensors[-1], posCost, rotCost]\n",
    "        return [phase, targets, new_pose, pose_label, posCost]\n",
    "        # return [phase, targets, new_pose, pose_label, out_tensors[-1]]\n",
    "\n",
    "    def computeCost(self, targets, trajectory):\n",
    "        targetPos = targets\n",
    "        # targetRot = targets[:, self.feature_dims[\"targetPosition\"]:]\n",
    "        posT = trajectory\n",
    "        # rotT = trajectory[:, self.feature_dims[\"tPos\"]:]\n",
    "\n",
    "        targetPos = targetPos.reshape((-1, 2, 3))\n",
    "        posT = posT.reshape((-1, 2, 3))\n",
    "        # targetRot = targetRot.reshape((-1, 12, 3,3))\n",
    "        # rotT = rotT.reshape((-1, 12, 3, 3))\n",
    "\n",
    "        posCost = torch.sum(((targetPos - posT)**2), axis=2).reshape((-1, self.feature_dims[\"posCost\"]))\n",
    "        # colLength = torch.sqrt(torch.clip(torch.sum(rotT**2, axis=2), 0))\n",
    "        # rotT = rotT / colLength[:, :, :, None]\n",
    "\n",
    "        # rotT = torch.transpose(rotT, dim0=2, dim1=3)\n",
    "        # trace =torch.diagonal(targetRot @ rotT, offset=0, dim1=2, dim2=3).sum(dim=2)\n",
    "        # rotCost = torch.abs(torch.arccos((torch.clamp( (trace - 1) / 2.0, -1, 1))))\n",
    "        # torch.nan_to_num_(rotCost, 0)\n",
    "        # rotCost = rotCost.reshape((-1, self.feature_dims[\"rotCost\"]))\n",
    "\n",
    "        return posCost\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, x, y, validation=False):\n",
    "        if not validation:\n",
    "           opt = self.optimizers()\n",
    "        x = x.squeeze(dim=2)\n",
    "        y = y.squeeze(dim=2)\n",
    "\n",
    "        n = x.size()[1]\n",
    "        tot_loss = 0\n",
    "        tot_posLoss = 0\n",
    "        tot_rotLoss = 0\n",
    "        x_c = x[:,0,:]\n",
    "\n",
    "        if self.autoregress_prob < 1:\n",
    "            autoregress_bools = torch.randn(n) < self.autoregress_prob\n",
    "            for i in range(1, n):\n",
    "                y_c = y[:,i-1,:]\n",
    "                # y_c.requires_grad_(True)\n",
    "                if torch.sum(y_c) == 0:\n",
    "                    break\n",
    "\n",
    "                out= self(x_c)\n",
    "                recon = torch.cat([out[0], out[2], out[4]], dim=1)\n",
    "                loss = self.loss_fn(recon, y_c)\n",
    "                posLoss = torch.mean(out[-1])\n",
    "                # rotLoss = torch.mean(out[-1])\n",
    "                # rotLoss = 0\n",
    "                #\n",
    "                tot_loss += loss.detach()\n",
    "                tot_posLoss += posLoss.detach() * float(i)/float(n)\n",
    "                # tot_rotLoss += rotLoss.detach()\n",
    "                tot_rotLoss += 0\n",
    "\n",
    "                # elif not recon.requires_grad:\n",
    "                #     raise ValueError(\"recon no grad, i : \", i, \" \\n\", recon, y_c)\n",
    "                # elif not loss.requires_grad:\n",
    "                #     raise ValueError(\"loss no grad\")\n",
    "                if not validation:\n",
    "                    opt.zero_grad()\n",
    "                    # self.optimizer.zero_grad()\n",
    "                    self.manual_backward(loss)\n",
    "                    # self.optimizer.step()\n",
    "                    opt.step()\n",
    "\n",
    "                if self.autoregress_prob > 0 and autoregress_bools[i]:\n",
    "                    x_c = torch.cat(out, dim=1).detach()\n",
    "                else:\n",
    "                    x_c = x[:,i,:]\n",
    "\n",
    "            tot_loss /= float(i+1)\n",
    "            # tot_posLoss /= float(i+1)\n",
    "            tot_rotLoss /= float(i+1)\n",
    "        else:\n",
    "            for i in range(1, n):\n",
    "                y_c = y[:,i-1,:]\n",
    "                if torch.sum(y_c) == 0:\n",
    "                    break\n",
    "\n",
    "                out= self(x_c)\n",
    "                recon = torch.cat([out[0], out[2], out[4]], dim=1)\n",
    "                loss = self.loss_fn(recon, y_c)\n",
    "                # posLoss = torch.mean(out[-2])\n",
    "                posLoss = torch.mean(out[-1])\n",
    "                # rotLoss = torch.mean(out[-1])\n",
    "                # rotLoss = 0\n",
    "\n",
    "                tot_loss += loss.detach()\n",
    "                tot_posLoss = posLoss.detach()* float(i)/float(n)\n",
    "                # tot_rotLoss += rotLoss.detach()\n",
    "                tot_rotLoss += 0\n",
    "                # self.optimizer.zero_grad()\n",
    "                # (loss + posLoss + rotLoss).backward()\n",
    "                if not validation:\n",
    "                    opt.zero_grad()\n",
    "                    # self.optimizer.zero_grad()\n",
    "                    self.manual_backward(loss)\n",
    "                    # self.optimizer.step()\n",
    "                    opt.step()\n",
    "\n",
    "                x_c = torch.cat(out, dim=1).detach()\n",
    "\n",
    "            tot_loss /= float(i+1)\n",
    "            # tot_posLoss /= float(i+1)\n",
    "            tot_rotLoss /= float(i+1)\n",
    "\n",
    "        return tot_loss, tot_posLoss, tot_rotLoss\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss, posLoss, rotLoss = self.step(x,y, False)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"ptl/train_posLoss\", posLoss)\n",
    "        self.log(\"ptl/train_rotLoss\", rotLoss)\n",
    "        # return loss#+posLoss+rotLoss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss, posLoss, rotLoss = self.step(x,y, True)\n",
    "        self.log(\"ptl/val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"ptl/val_posLoss\", posLoss, prog_bar=True)\n",
    "        self.log(\"ptl/val_rotLoss\", rotLoss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.epochs > 0 and self.epochs % 20==0:\n",
    "            self.autoregress_prob = min(1, self.autoregress_prob+self.autoregress_inc)\n",
    "            self.autoregress_chunk_size = min(120, self.autoregress_chunk_size+self.autoregress_inc)\n",
    "        elif self.epochs > 0 and self.epochs % 10==0:\n",
    "            self.scheduler.step()\n",
    "        self.epochs += 1\n",
    "\n",
    "\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir=MODEL_PATH):\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        loss = self.best_val_loss.cpu().numpy()\n",
    "\n",
    "        pose_autoencoder_path = self.pose_autoencoder.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "        cost_encoder_path = self.cost_encoder.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "        generationModel_path = self.generationModel.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "\n",
    "        model = {\"name\":self.name,\n",
    "                 \"pose_autoencoder_path\":pose_autoencoder_path,\n",
    "                 \"cost_encoder_path\": cost_encoder_path,\n",
    "                 \"motionGenerationModelPath\":generationModel_path,\n",
    "                 \"in_slices\":self.in_slices,\n",
    "                 \"out_slices\":self.out_slices,\n",
    "                 }\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        with bz2.BZ2File(os.path.join(path,\n",
    "                                      str(loss)+\".pbz2\"), \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filename, pose_ae_model, cost_encoder_model, generation_model):\n",
    "        with bz2.BZ2File(filename, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        pose_autoencoder = pose_ae_model.load_checkpoint(obj[\"pose_autoencoder_path\"])\n",
    "        cost_encoder = cost_encoder_model.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "        generationModel = generation_model.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "        model = MotionGenerationModel(name=obj[\"name\"])\n",
    "        model.pose_autoencoder = pose_autoencoder\n",
    "        model.cost_encoder = cost_encoder\n",
    "        model.generationModel = generationModel\n",
    "        model.in_slices = obj[\"in_slices\"]\n",
    "        model.out_slices = obj[\"out_slices\"]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def update_phase(self, p1, p2):\n",
    "        return self.phase_smooth_factor * p2 + (1-self.phase_smooth_factor)*p1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "        self.scheduler = scheduler\n",
    "        self.optimizer = optimizer\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "data_path = [\n",
    "             \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two.pbz2\",\n",
    "            # \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One-large.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One-small.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two-small.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two-large.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two-large.pbz2\",\n",
    "            #  \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two-small.pbz2\",\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "# pose_features = [\"pos\", \"rotMat\", \"velocity\", \"isLeft\", \"chainPos\", \"geoDistanceNormalised\"]\n",
    "# cost_features = [\"tPos\", \"tRot\", \"posCost\", \"rotCost\"]\n",
    "# phase_features = [\"phase_vec\", \"targetPosition\", \"targetRotation\"]\n",
    "\n",
    "pose_features = [\"pos\", \"rotMat\", \"velocity\", \"isLeft\", \"chainPos\", \"geoDistanceNormalised\"]\n",
    "cost_features = [\"posCost\"]\n",
    "phase_features = [\"phase_vec\", \"targetPosition\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def load(file_path):\n",
    "    with bz2.BZ2File(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "        return obj\n",
    "\n",
    "data = [load(path) for path in data_path]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "frame_window = 15\n",
    "sampling_step = frame_window / window_size\n",
    "\n",
    "data_tensors = []\n",
    "\n",
    "data_dims = []\n",
    "feature_list = []\n",
    "feature_data = [{}, {}]\n",
    "features = phase_features + pose_features + cost_features\n",
    "for f in features:\n",
    "    feature_data[0][f] = []\n",
    "    feature_data[1][f] = []\n",
    "\n",
    "first_row = True\n",
    "first_time = True\n",
    "key_joints = []\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "for Data in data:\n",
    "    for clip in Data:\n",
    "        d = pickle.loads(clip)\n",
    "        sequence = []\n",
    "        n_frames = len(d[\"frames\"])\n",
    "        if first_time:\n",
    "            key_joints = [i for i in range(len(d[\"frames\"][0])) if d[\"frames\"][0][i][\"key\"]]\n",
    "            first_time = False\n",
    "\n",
    "        idx = [int(n_frames/2)] * len(key_joints)\n",
    "        for i, jo in enumerate(key_joints):\n",
    "            for f_id, frame in enumerate(d[\"frames\"]):\n",
    "                if frame[jo][\"contact\"]:\n",
    "                    idx[i] = f_id\n",
    "                    break\n",
    "\n",
    "        max_id = max(idx)\n",
    "        max_id2 = max_id + 3\n",
    "        min_id = min(idx)\n",
    "        min_id2 = min(idx) - 3\n",
    "        start1 = 0\n",
    "        end1 = max_id2 if max_id2 <= n_frames else max_id\n",
    "        end2 = min_id2 if min_id2 >= 0 else min_id\n",
    "        start2 = n_frames-1\n",
    "        clip_idx = [(start1, end1), (start2, end2)]\n",
    "\n",
    "\n",
    "        for i, (start, end) in enumerate(clip_idx):\n",
    "            frames = d[\"frames\"]\n",
    "            if start < end:\n",
    "                idx = np.arange(start+1, end-1)\n",
    "                intervals = [sorted(np.random.choice(idx, 28, replace=False).tolist()) for i in range(3)]\n",
    "                # print(i, intervals, sorted(np.random.choice(idx, 28, replace=False).tolist()))\n",
    "            else:\n",
    "                idx = np.arange(end+1, start-1)\n",
    "                intervals = [sorted(np.random.choice(idx, 28, replace=False).tolist(),reverse=True) for i in range(3)]\n",
    "            # print(intervals[0])\n",
    "            for interval in intervals:\n",
    "                if start < end:\n",
    "                    interval = [idx[0]-1] + interval + [idx[-1]+1]\n",
    "                else:\n",
    "                    interval = [idx[-1]+1] + interval + [idx[0]-1]\n",
    "                for f in interval:\n",
    "                    row_vec = []\n",
    "            # if i == 1: continue\n",
    "                # if i == 0:\n",
    "                #     n = end-start\n",
    "                # else:\n",
    "                #     n = start\n",
    "                #\n",
    "                # if i == 0:\n",
    "                #     f_idx = np.arange(f-frame_window, f+frame_window, sampling_step, dtype=int)\n",
    "                # else:\n",
    "                #     f_idx = np.arange(f+frame_window, f-frame_window, -sampling_step, dtype=int)\n",
    "                #\n",
    "                # f_idx[f_idx < 0] = 0\n",
    "                # f_idx[f_idx >= n] = n\n",
    "                # f_idx = f_idx.tolist()\n",
    "                    f_idx = [f]\n",
    "                    for feature in phase_features:\n",
    "                        if feature == \"phase_vec\":\n",
    "                            sin = np.asarray([frames[idx][jj][\"phase_vec\"] for jj in key_joints for idx in f_idx])\n",
    "                            vel = np.concatenate([frames[idx][jj][\"velocity\"] for jj in key_joints for idx in f_idx])\n",
    "                            vel = np.reshape(vel, (3,-1))\n",
    "                            vel = np.sqrt(np.sum(vel**2, axis=0))\n",
    "                            cos = np.cos(np.arcsin(np.asarray([frames[idx][jj][\"sin_normalised_contact\"] for jj in key_joints for idx in f_idx])))\n",
    "                            cos = cos * vel\n",
    "                            row_vec.append(np.concatenate([np.asarray([sin[i], cos[i]]) for i in range(len(sin))]))\n",
    "                        elif feature == \"targetRotation\" or feature == \"targetPosition\":\n",
    "                            row_vec.append(np.concatenate([frames[idx][jj][feature].ravel() for jj in key_joints for idx in f_idx]))\n",
    "                        elif feature == \"contact\":\n",
    "                            row_vec.append(np.asarray([frames[idx][jj][\"contact\"] for jj in key_joints for idx in f_idx]))\n",
    "                        else:\n",
    "                            row_vec.append(np.concatenate([frames[f][jj][feature] for jj in key_joints]))\n",
    "\n",
    "                        # feature_data[i][feature].append(row_vec)\n",
    "                        if first_row:\n",
    "                            data_dims.append(row_vec[-1].shape)\n",
    "                            feature_list.append(feature)\n",
    "\n",
    "                    for feature in pose_features:\n",
    "                        if feature==\"rotMat\":\n",
    "                            joRot = np.concatenate([jo[\"rotMat\"].ravel() for jo in frames[f]])\n",
    "                            joRot = joRot.reshape((-1, 3, 3))\n",
    "                            joRot_s = np.sqrt(np.sum(joRot**2, axis=1))\n",
    "                            joRot = joRot / joRot_s[:, :, None]\n",
    "\n",
    "                            if np.sum(np.isnan(joRot)) > 0:\n",
    "                                raise ValueError(\"RotMat has nan\")\n",
    "                            row_vec.append(joRot.ravel())\n",
    "                        elif feature == \"isLeft\" or feature == \"chainPos\" or feature == \"geoDistanceNormalised\":\n",
    "                            row_vec.append(np.concatenate([[jo[feature]] for jo in frames[f]]))\n",
    "                        else:\n",
    "                            row_vec.append(np.concatenate([jo[feature] for jo in frames[f]]))\n",
    "\n",
    "                        # feature_data[i][feature].append(row_vec)\n",
    "                        if first_row:\n",
    "                            data_dims.append(row_vec[-1].shape)\n",
    "                            feature_list.append(feature)\n",
    "                    for feature in cost_features:\n",
    "                        if feature == \"posCost\":\n",
    "                            targetPos = np.concatenate([frames[idx][jj][\"targetPosition\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                            targetPos = targetPos.reshape((len(key_joints), len(f_idx), 3))\n",
    "                            joPos = np.concatenate([frames[idx][jj][\"pos\"] for jj in key_joints for idx in f_idx])\n",
    "                            joPos = joPos.reshape((len(key_joints), len(f_idx), 3))\n",
    "                            posCost = np.sum(((targetPos - joPos)**2), axis=2).ravel()\n",
    "                            row_vec.append(posCost)\n",
    "                        elif feature == \"rotCost\":\n",
    "                            targetRot = np.concatenate([frames[idx][jj][\"targetRotation\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                            targetRot = targetRot.reshape((len(key_joints), len(f_idx), 3, 3))\n",
    "\n",
    "                            joRot = np.concatenate([frames[idx][jj][\"rotMat\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                            joRot = joRot.reshape((len(key_joints), len(f_idx), 3, 3))\n",
    "                            joRot_s = np.sqrt(np.sum(joRot**2, axis=2))\n",
    "                            joRot = joRot / joRot_s[:, :, :, None]\n",
    "\n",
    "                            joRotT = np.transpose(joRot, (0, 1, 3, 2))\n",
    "                            rotDiff = np.arccos(np.clip((np.trace(targetRot @ joRotT, axis1=2, axis2=3) - 1) / 2.0, -1, 1).ravel())\n",
    "\n",
    "                            if np.sum(np.isnan(rotDiff)) > 0:\n",
    "                                raise ValueError(\"RotCost has nan\")\n",
    "                            row_vec.append(rotDiff)\n",
    "                        elif feature == \"tPos\" or feature == \"tRot\":\n",
    "                            feature = \"pos\" if feature == \"tPos\" else \"rotMat\"\n",
    "                            row_vec.append(np.concatenate([frames[idx][jj][feature].ravel() for jj in key_joints for idx in f_idx]))\n",
    "                        else:\n",
    "                            row_vec.append(np.concatenate([frames[f][jj][feature] for jj in key_joints]))\n",
    "\n",
    "                        # feature_data[i][feature].append(row_vec)\n",
    "                        if first_row:\n",
    "                            data_dims.append(row_vec[-1].shape)\n",
    "                            feature_list.append(feature)\n",
    "\n",
    "                    if first_row: first_row = False\n",
    "                # if i == 1:\n",
    "                #     row_vec = np.flip(np.concatenate(row_vec))\n",
    "                #     sequence.append(row_vec)\n",
    "                # else:\n",
    "                    sequence.append(np.concatenate(row_vec))\n",
    "                data_tensors.append(np.vstack(sequence))\n",
    "                sequence = []\n",
    "        # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 20]\n"
     ]
    }
   ],
   "source": [
    "print(key_joints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    return nn.functional.mse_loss(x,y)\n",
    "def loss_fn2(x, y):\n",
    "    return nn.functional.smooth_l1_loss(x,y)\n",
    "def normalise(x):\n",
    "    std = torch.std(x, dim=0)\n",
    "    std[std==0] = 1\n",
    "    return (x-torch.mean(x, dim=0)) / std\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  --------------  -----  ------  --------  ------  --------  ---------------------  -------\n",
      "phase_vec  targetPosition  pos    rotMat  velocity  isLeft  chainPos  geoDistanceNormalised  posCost\n",
      "(4,)       (6,)            (63,)  (189,)  (63,)     (21,)   (21,)     (21,)                  (2,)\n",
      "---------  --------------  -----  ------  --------  ------  --------  ---------------------  -------\n",
      "phase dim:  10\n",
      "pose dim:  378\n",
      "cost dim:  2\n"
     ]
    }
   ],
   "source": [
    "extra_feature_len = 21 * 3\n",
    "n_phase_features = len(phase_features)\n",
    "n_pose_features = len(pose_features)\n",
    "phase_dim = np.sum(data_dims[0:n_phase_features])\n",
    "pp_dim = data_dims[0][0]\n",
    "pose_dim = np.sum(data_dims[n_phase_features:n_phase_features+n_pose_features])\n",
    "cost_dim = np.sum(data_dims[n_phase_features+n_pose_features:])\n",
    "\n",
    "table = [feature_list, data_dims]\n",
    "print(tabulate(table))\n",
    "print(\"phase dim: \",phase_dim)\n",
    "print(\"pose dim: \", pose_dim)\n",
    "print(\"cost dim: \", cost_dim)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# x_tensors = torch.nn.utils.rnn.pad_sequence([normalise(torch.from_numpy(clip[:-1])).float().unsqueeze(dim=1) for clip in data_tensors], batch_first=True)\n",
    "# y_tensors = torch.nn.utils.rnn.pad_sequence([normalise(torch.from_numpy(clip[1:])).float().unsqueeze(dim=1) for clip in data_tensors], batch_first=True)\n",
    "\n",
    "x_tensors = torch.stack([normalise(torch.from_numpy(clip[:-1])).float() for clip in data_tensors])\n",
    "y_tensors = torch.stack([torch.from_numpy(clip[1:]).float() for clip in data_tensors])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 29, 390])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensors.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 58 58\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(torch.Tensor(x_tensors), torch.Tensor(y_tensors))\n",
    "N = len(x_tensors)\n",
    "\n",
    "train_ratio = int(.7*N)\n",
    "val_ratio = int((N-train_ratio) / 2.0)\n",
    "test_ratio = N - train_ratio - val_ratio\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(2021))\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data_1 = {\"data_sets\":[train_set, val_set, test_set], \"table\":table}\n",
    "with bz2.BZ2File(\"data_sets_8_30hz_phaes+pos+cost_single_frame.pbz2\", \"w\") as f:\n",
    "    pickle.dump(data_1, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# with bz2.BZ2File(\"data_sets_7_terminateAtContact_cost.pbz2\", \"rb\") as f:\n",
    "#     obj = pickle.load(f)\n",
    "#\n",
    "# train_set = obj[\"data_sets\"][0]\n",
    "# val_set = obj[\"data_sets\"][1]\n",
    "# test_set = obj[\"data_sets\"][2]\n",
    "# table = obj[\"table\"]\n",
    "feature_dims = {}\n",
    "for feat, dim in zip(table[0], table[1]):\n",
    "    if feat in feature_dims:\n",
    "        if feat == \"pos\": feat = \"tPos\"\n",
    "        elif feat == \"rotMat\": feat = \"tRot\"\n",
    "    feature_dims[feat] = dim[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"k_experts\": 4,\n",
    "    \"gate_size\": 128,\n",
    "    \"keep_prob\": 0.2,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"cost_hidden_dim\" : 256,\n",
    "    \"cost_output_dim\" : 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"loss_fn\": loss_fn2,\n",
    "    \"window_size\":1,\n",
    "    \"autoregress_prob\" : 0,\n",
    "    \"autoregress_inc\" : .2,\n",
    "    \"autoregress_chunk_size\": 120\n",
    "}\n",
    "\n",
    "model_name = \"Test_11_phase+pose+cost_single\"\n",
    "epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 378, 2]\n",
      "[4, 512]\n"
     ]
    }
   ],
   "source": [
    "phase_dim = sum([feature_dims[feat] for feat in phase_features])\n",
    "pose_dim = sum([feature_dims[feat] for feat in pose_features])\n",
    "cost_dim = sum([feature_dims[feat] for feat in cost_features])\n",
    "# trajectory_dim = feature_dims[\"tPos\"]\n",
    "\n",
    "pose_autoencoder = MLP_withLabel.load_checkpoint(\"/home/nuoc/Documents/MEX/models/MLP4_withLabel_best/M3/0.00324857.512.pbz2\")\n",
    "pose_encoder_out_dim = pose_autoencoder.dimensions[-1]\n",
    "\n",
    "input_slices=[phase_dim, pose_dim, cost_dim]\n",
    "output_slices=[feature_dims[\"phase_vec\"], pose_encoder_out_dim]\n",
    "print(input_slices)\n",
    "print(output_slices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def reformLabel(dataset, slices, phase_dim, extra_feature_len):\n",
    "    set_y = list(dataset[:][1])\n",
    "    slices = [0] + list(accumulate(add, slices))\n",
    "    for i, y in enumerate(set_y):\n",
    "        y = y.squeeze(1)\n",
    "        y_tensors = [y[:, d0:d1] for d0, d1 in zip(slices[:-1], slices[1:])]\n",
    "        phase = y_tensors[0][:, :phase_dim]\n",
    "        pose = y_tensors[1][:, :-extra_feature_len]\n",
    "        trajectory = y_tensors[2]\n",
    "        set_y[i] = torch.cat([phase, pose, trajectory], dim=1).unsqueeze(dim=1)\n",
    "    return [(x, y) for x, y in zip(dataset[:][0], set_y)]\n",
    "\n",
    "train_set = reformLabel(train_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n",
    "val_set = reformLabel(val_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n",
    "test_set = reformLabel(test_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "model = MotionGenerationModel(config=config, pose_autoencoder=pose_autoencoder, cost_input_dimension=cost_dim,\n",
    "                          input_slicers=input_slices, output_slicers=output_slices,feature_dims=feature_dims,\n",
    "                          train_set=train_set, val_set=val_set, name=model_name)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type          | Params\n",
      "---------------------------------------------------\n",
      "0 | pose_autoencoder | MLP_withLabel | 440 K \n",
      "1 | cost_encoder     | MLP           | 132 K \n",
      "2 | generationModel  | MoE           | 3.2 M \n",
      "---------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.005    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d411d108958b42648d1eb0d189d1132c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcb1c9d5d66f471581bf02d3501d7c91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81287800e179438a9e2d3fde28f54611"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1cdb651afc04040b85ddb15ef178594"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bc591d53f3f43f2b8bc07ae9d643f76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28a805ebdade4081823b517642812160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d4adf7470124badb5e35dce02405d73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0d7f900e48c4c1bb55ccb645a251ff4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddc0de6a949a4fd0aa30e6d3f65395a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "453ab92e04954052a9812c11565dd0b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ed6da63cd6142cd96be2ac2137488a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0486ace7d92404991aa60b30a480c0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5db5e17d784e4f3fb215b0069aebb80d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5af31216b4864c0186571e202a8843d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00aabc547b8740faa4e91ceb0b4b29f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b460c515b19b437fbe894558562313ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e8752ec04f4e59a993ba10c029f4c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb910aadf26649239199c5977742d880"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cb15fd574b14df2a878859504018622"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5d9bd4b4d044967a4f7181d845731c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf14b6d43e2846089021da1350d49281"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f25e2543770464f93dcceb2d66c859e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9723ff3555df45ebbde5f4957fc3e81c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ead010304ee34bb59e7a30d08f61fa38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "563817c9c721464b8abc63accf67d0b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24e768a021ec48b190272f7f94947583"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "321235d09840408187ed510aded3f6cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c145d510a5f3476aba94c77b098e2435"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "424b024291fb4ccf91aaed6e0fe9f5a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86c43bf41391408490fcdc0f5e9e952e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f2d0e3b411147d79cd6115b1f3b5dc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb1ef0b3b2284ff193c6357657ca3e14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83e3de1025b54e15966aaed16669cbd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "# prob\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"avg_val_loss\", save_top_k=3)\n",
    "# earlystopping = EarlyStopping(monitor=\"avg_val_loss\", patience=20)\n",
    "logger=TensorBoardLogger(save_dir=\"logs/\", name=model_name, version=\"0.0\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"/home/nuoc/Documents/MEX/src/motion_generation/checkpoints\",\n",
    "    gpus=1, precision=16,\n",
    "    # callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    min_epochs=50,\n",
    "    max_epochs=epochs,\n",
    "    stochastic_weight_avg=True\n",
    ")\n",
    "\n",
    "# train_set = datasets[0][0]\n",
    "# val_set = datasets[0][1]\n",
    "train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "val_loader = DataLoader(val_set, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "trainer.fit(model,train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "A GradScaler instance may only be pickled at the beginning of an iteration, or at the end after scaler.update().",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-30-c0f38fac6ed1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# for i in range(10):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# model.scheduler.step()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001B[0m\n\u001B[1;32m    457\u001B[0m         \u001B[0;31m# ----------------------------\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    458\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_setup_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 459\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"on_before_accelerator_backend_setup\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    460\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccelerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# note: this sets up self.lightning_module\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mcall_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1092\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1093\u001B[0m                 \u001B[0mtrainer_hook\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1094\u001B[0;31m                 \u001B[0mtrainer_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1095\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1096\u001B[0m             \u001B[0;31m# next call hook in lightningModule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\u001B[0m in \u001B[0;36mon_before_accelerator_backend_setup\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;34m\"\"\"Called in the beginning of fit and test\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mcallback\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m             \u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_before_accelerator_backend_setup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msetup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/swa.py\u001B[0m in \u001B[0;36mon_before_accelerator_backend_setup\u001B[0;34m(self, trainer, pl_module)\u001B[0m\n\u001B[1;32m    140\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_before_accelerator_backend_setup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'pl.Trainer'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpl_module\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'pl.LightningModule'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m         \u001B[0;31m# copy the model before moving it to accelerator device.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 142\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_average_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpl_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_fit_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'pl.Trainer'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpl_module\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'pl.LightningModule'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    170\u001B[0m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_reconstruct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mrv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;31m# If is its own copy, don't memoize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_reconstruct\u001B[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdeep\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'__setstate__'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setstate__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mcopier\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dispatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcopier\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0mmemo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 230\u001B[0;31m         \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    231\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    170\u001B[0m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_reconstruct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mrv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;31m# If is its own copy, don't memoize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_reconstruct\u001B[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdeep\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'__setstate__'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setstate__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mcopier\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dispatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcopier\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0mmemo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 230\u001B[0;31m         \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    231\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    170\u001B[0m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_reconstruct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mrv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;31m# If is its own copy, don't memoize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_reconstruct\u001B[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdeep\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'__setstate__'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setstate__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mcopier\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dispatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcopier\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0mmemo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 230\u001B[0;31m         \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    231\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    170\u001B[0m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m                     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_reconstruct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mrv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;31m# If is its own copy, don't memoize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_reconstruct\u001B[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdeep\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'__setstate__'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setstate__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mcopier\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dispatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcopier\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0mmemo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 230\u001B[0;31m         \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    231\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_deepcopy_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/copy.py\u001B[0m in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    159\u001B[0m                     \u001B[0mreductor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"__reduce_ex__\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mreductor\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                         \u001B[0mrv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreductor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m                         \u001B[0mreductor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"__reduce__\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\u001B[0m in \u001B[0;36m__getstate__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    514\u001B[0m         \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__dict__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    515\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_enabled\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 516\u001B[0;31m             \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_per_optimizer_states\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"A GradScaler instance may only be pickled at the beginning \"\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    517\u001B[0m                                                          \u001B[0;34m\"of an iteration, or at the end after scaler.update().\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    518\u001B[0m             \u001B[0;31m# Pickling _scale and _growth_tracker Tensors directly triggers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: A GradScaler instance may only be pickled at the beginning of an iteration, or at the end after scaler.update()."
     ]
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "    # model.scheduler.step()\n",
    "trainer.fit(model,train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with bz2.BZ2File(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "        return obj\n",
    "mo = load(\"/home/nuoc/Documents/MEX/src/motion_generation/Test_9_phase+pose+cost+trajectory_autoregress_prob_curriculum/0.07476503.pbz2\")\n",
    "ae = load(mo[\"pose_autoencoder_path\"])\n",
    "cost = load(mo[\"cost_encoder_path\"])\n",
    "gene = load(mo[\"motionGenerationModelPath\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# model.pose_autoencoder.encoder.load_state_dict(ae[\"encoder\"])\n",
    "# model.pose_autoencoder.decoder.load_state_dict(ae[\"decoder\"])\n",
    "# model.cost_encoder.encoder.load_state_dict(cost[\"encoder\"])\n",
    "# model.generationModel.gate.load_state_dict(gene[\"gate\"])\n",
    "# model.generationModel.load_state_dict(gene[\"generationNetwork\"])\n",
    "# model.name = \"\"\n",
    "# pose_autoencoder = pose_ae_model.load_checkpoint(mo[\"pose_autoencoder_path\"])\n",
    "# cost_encoder = cost_encoder_model.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "# generationModel = generation_model.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "model.save_checkpoint(\"\")\n",
    "# model = MotionGenerationModel.load_checkpoint(\n",
    "#     filename=\",\n",
    "# pose_ae_model=MLP_withLabel, cost_encoder_model=MLP, generation_model=MoE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044012121856212616\n",
      "0.24371576309204102\n"
     ]
    }
   ],
   "source": [
    "# test_set2 = reformLabel(test_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3, trajectory_dim=trajectory_dim)\n",
    "model.autoregress_prob = 1\n",
    "test_loader = DataLoader(test_set, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    loss, posLoss = 0, 0\n",
    "    for x,y in test_loader:\n",
    "        x = x.to(\"cpu\")\n",
    "        y = y.to(\"cpu\")\n",
    "        l, pll, rl = model.step(x, y, validation=True)\n",
    "        loss += l\n",
    "        posLoss += pll\n",
    "    loss /= float(len(test_loader))\n",
    "    posLoss /= float(len(test_loader))\n",
    "    print(loss.item())\n",
    "    print(posLoss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}