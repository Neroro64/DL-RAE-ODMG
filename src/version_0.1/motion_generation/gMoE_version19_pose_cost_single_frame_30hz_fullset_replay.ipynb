{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from cytoolz import sliding_window, accumulate, get\n",
    "import pytorch_lightning as pl\n",
    "from operator import add\n",
    "from tabulate import tabulate\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../rig_agnostic_encoding\")\n",
    "sys.path.append(\"../rig_agnostic_encoding/models\")\n",
    "sys.path.append(\"../rig_agnostic_encoding/functions\")\n",
    "import func\n",
    "# from MLP_withLabel import MLP_withLabel\n",
    "# from MLP import MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/nuoc/Documents/MEX/data\"\n",
    "MODEL_PATH = \"/home/nuoc/Documents/MEX/models\"\n",
    "RESULTS_PATH = \"/home/nuoc/Documents/MEX/results\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP_withLabel(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None, extra_feature_len:int=0,\n",
    "                 train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                 single_module:int=0):\n",
    "\n",
    "        super(MLP_withLabel, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        self.single_module = single_module\n",
    "        self.extra_feature_len = extra_feature_len\n",
    "        self.act = nn.ELU\n",
    "        self.k = 0\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.hidden_dim = config[\"hidden_dim\"]\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            self.act = config[\"activation\"]\n",
    "            self.loss_fn = config[\"ae_loss_fn\"]\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "                self.dimensions = [self.dimensions[0]-extra_feature_len, self.hidden_dim, self.k]\n",
    "                self.train_set = train_set\n",
    "                self.val_set = val_set\n",
    "                self.test_set = test_set\n",
    "\n",
    "                self.best_val_loss = np.inf\n",
    "\n",
    "            self.build()\n",
    "            self.encoder.apply(self.init_params)\n",
    "            self.decoder.apply(self.init_params)\n",
    "\n",
    "\n",
    "        def build(self):\n",
    "            layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "            if self.single_module == -1 or self.single_module == 0:\n",
    "                layers = []\n",
    "                for i, size in enumerate(layer_sizes):\n",
    "                    layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                    if i < len(self.dimensions)-2:\n",
    "                        layers.append((\"act\"+str(i), self.act()))\n",
    "                        layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "                self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "            else:\n",
    "                self.encoder = nn.Sequential()\n",
    "\n",
    "            if self.single_module == 0 or self.single_module == 1:\n",
    "                layers = []\n",
    "                layer_sizes[-1] = (layer_sizes[-1][0], layer_sizes[-1][1] + self.extra_feature_len)\n",
    "                for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                    layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                    if i < len(self.dimensions)-2:\n",
    "                        layers.append((\"act\"+str(i), self.act()))\n",
    "                        layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "                self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "            else:\n",
    "                self.decoder = nn.Sequential()\n",
    "\n",
    "        def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "            return self.decode(*self.encode(x))\n",
    "\n",
    "        def encode(self, x):\n",
    "            _x, label = x[:, :-self.extra_feature_len], x[:, -self.extra_feature_len:]\n",
    "            h = self.encoder(_x)\n",
    "            return h, label\n",
    "\n",
    "        def decode(self, h, label):\n",
    "            hr = torch.cat((h, label), dim=1)\n",
    "            return self.decoder(hr)\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log(\"ptl/train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "            return {\"val_loss\":loss}\n",
    "\n",
    "        def test_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "            return {\"val_loss\":loss}\n",
    "\n",
    "        def validation_epoch_end(self, outputs):\n",
    "            avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            self.log(\"avg_val_loss\", avg_loss)\n",
    "            if avg_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_loss\n",
    "                self.save_checkpoint(best_val_loss=self.best_val_loss.cpu().numpy())\n",
    "\n",
    "        def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "            model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                     \"extra_feature_len\" : self.extra_feature_len,\n",
    "                     \"encoder\":self.encoder.state_dict(),\n",
    "                     \"decoder\":self.decoder.state_dict()}\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.mkdir(checkpoint_dir)\n",
    "            path = os.path.join(checkpoint_dir, self.name)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "\n",
    "            filePath = os.path.join(path, str(best_val_loss)+\".\"+str(self.k)+\".pbz2\")\n",
    "            with bz2.BZ2File(filePath, \"w\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            return filePath\n",
    "\n",
    "        @staticmethod\n",
    "        def load_checkpoint(filePath):\n",
    "            with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "                obj = pickle.load(f)\n",
    "\n",
    "            model = MLP_withLabel(name=obj[\"name\"], dimensions=obj[\"dimensions\"], extra_feature_len=obj[\"extra_feature_len\"], keep_prob=obj[\"keep_prob\"], load=True)\n",
    "            model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "            model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "            return model\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "            return optimizer\n",
    "\n",
    "        def setup_data(self):\n",
    "            pass\n",
    "        def train_dataloader(self):\n",
    "            return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def val_dataloader(self):\n",
    "            return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def test_dataloader(self):\n",
    "            return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        @staticmethod\n",
    "        def init_params(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(.01)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    class MLP(pl.LightningModule):\n",
    "        def __init__(self, config:dict=None, dimensions:list=None,\n",
    "                     train_set=None, val_set=None, test_set=None,\n",
    "                     keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                     single_module:int=0):\n",
    "\n",
    "            super(MLP, self).__init__()\n",
    "            self.name = name\n",
    "            self.dimensions = dimensions\n",
    "            self.keep_prob = keep_prob\n",
    "            self.single_module = single_module\n",
    "            self.act = nn.ELU\n",
    "            self.k = 0\n",
    "            if load:\n",
    "                self.build()\n",
    "            else:\n",
    "                self.hidden_dim = config[\"hidden_dim\"]\n",
    "                self.k = config[\"k\"]\n",
    "                self.learning_rate = config[\"lr\"]\n",
    "                self.act = config[\"activation\"]\n",
    "                self.loss_fn = config[\"loss_fn\"]\n",
    "                self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "                self.dimensions = dimensions + [self.hidden_dim, self.k]\n",
    "                self.train_set = train_set\n",
    "                self.val_set = val_set\n",
    "                self.test_set = test_set\n",
    "\n",
    "                self.best_val_loss = np.inf\n",
    "\n",
    "                self.build()\n",
    "            self.encoder.apply(self.init_params)\n",
    "            self.decoder.apply(self.init_params)\n",
    "\n",
    "\n",
    "        def build(self):\n",
    "            layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "            if self.single_module == -1 or self.single_module == 0:\n",
    "                layers = []\n",
    "                for i, size in enumerate(layer_sizes):\n",
    "                    layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                    if i < len(self.dimensions)-2:\n",
    "                        layers.append((\"act\"+str(i), self.act()))\n",
    "                        layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "                self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "            else:\n",
    "                self.encoder = nn.Sequential()\n",
    "\n",
    "            if self.single_module == 0 or self.single_module == 1:\n",
    "                layers = []\n",
    "                for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                    layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                    if i < len(self.dimensions)-2:\n",
    "                        layers.append((\"act\"+str(i), self.act()))\n",
    "                        layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "                self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "            else:\n",
    "                self.decoder = nn.Sequential()\n",
    "\n",
    "        def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "            return self.decode(self.encode(x))\n",
    "\n",
    "        def encode(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "        def decode(self, h):\n",
    "            return self.decoder(h)\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log(\"ptl/train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "            return {\"val_loss\":loss}\n",
    "\n",
    "        def test_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            prediction = self(x)\n",
    "            loss = self.loss_fn(prediction, y)\n",
    "\n",
    "            self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "            return {\"val_loss\":loss}\n",
    "\n",
    "        def validation_epoch_end(self, outputs):\n",
    "            avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            self.log(\"avg_val_loss\", avg_loss)\n",
    "            if avg_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_loss\n",
    "                self.save_checkpoint(best_val_loss=self.best_val_loss.cpu().numpy())\n",
    "\n",
    "        def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "            model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                     \"single_module\":self.single_module,\n",
    "                     \"encoder\":self.encoder.state_dict(),\n",
    "                     \"decoder\":self.decoder.state_dict()}\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.mkdir(checkpoint_dir)\n",
    "            path = os.path.join(checkpoint_dir, self.name)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "\n",
    "            filePath = os.path.join(path, str(best_val_loss)+\".\"+str(self.k)+\".pbz2\")\n",
    "            with bz2.BZ2File(filePath, \"w\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            return filePath\n",
    "\n",
    "        @staticmethod\n",
    "        def load_checkpoint(filePath):\n",
    "            with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "                obj = pickle.load(f)\n",
    "\n",
    "            model = MLP(name=obj[\"name\"], dimensions=obj[\"dimensions\"], keep_prob=obj[\"keep_prob\"],\n",
    "                      load=True)\n",
    "            model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "            # model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "            return model\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "            return optimizer\n",
    "\n",
    "        def setup_data(self):\n",
    "            pass\n",
    "        def train_dataloader(self):\n",
    "            return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def val_dataloader(self):\n",
    "            return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def test_dataloader(self):\n",
    "            return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        @staticmethod\n",
    "        def init_params(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(.01)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    class MoE(nn.Module):\n",
    "        def __init__(self, config=None, dimensions=None, phase_input_dim:int=0,\n",
    "                     gate_size=0, k_experts=1, keep_prob=.2,\n",
    "                     name=\"model\", load=False):\n",
    "            super().__init__()\n",
    "\n",
    "            self.phase_input_dim = phase_input_dim\n",
    "            self.dimensions = dimensions\n",
    "            self.act_fn = nn.ELU\n",
    "            self.name = name\n",
    "            self.config=config\n",
    "            self.gate_size=gate_size\n",
    "            self.k_experts = k_experts\n",
    "            self.keep_prob = .2\n",
    "            if not load:\n",
    "                self.k_experts = config[\"k_experts\"]\n",
    "                self.gate_size = config[\"gate_size\"]\n",
    "                self.keep_prob = config[\"keep_prob\"]\n",
    "                self.dimensions = [self.dimensions[0], config[\"hidden_dim\"], config[\"hidden_dim\"], self.dimensions[-1]]\n",
    "\n",
    "            self.layers = []\n",
    "\n",
    "            self.build()\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(phase_input_dim, self.gate_size),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(self.gate_size, self.gate_size),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(self.gate_size, self.k_experts)\n",
    "            )\n",
    "            self.init_params()\n",
    "\n",
    "\n",
    "        def forward(self, x:torch.Tensor, phase) -> torch.Tensor:\n",
    "            coefficients = F.softmax(self.gate(phase), dim=1)\n",
    "\n",
    "            layer_out = x\n",
    "            for (weight, bias, activation) in self.layers:\n",
    "                if weight is None:\n",
    "                    layer_out = activation(layer_out, p=self.keep_prob)\n",
    "                else:\n",
    "                    flat_weight = weight.flatten(start_dim=1, end_dim=2)\n",
    "                    mixed_weight = torch.matmul(coefficients, flat_weight).view(\n",
    "                        coefficients.shape[0], *weight.shape[1:3]\n",
    "                    )\n",
    "\n",
    "                    input = layer_out.unsqueeze(1)\n",
    "                    mixed_bias = torch.matmul(coefficients, bias).unsqueeze(1)\n",
    "                    out = torch.baddbmm(mixed_bias, input, mixed_weight).squeeze(1)\n",
    "                    layer_out = activation(out) if activation is not None else out\n",
    "\n",
    "            return layer_out\n",
    "\n",
    "        def build(self):\n",
    "            layers = []\n",
    "            for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "                if i < len(self.dimensions) - 2:\n",
    "                    layers.append(\n",
    "                        (\n",
    "                            nn.Parameter(torch.empty(self.k_experts, size[0], size[1])),\n",
    "                            nn.Parameter(torch.empty(self.k_experts, size[1])),\n",
    "                            self.act_fn()\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    layers.append(\n",
    "                        (\n",
    "                            nn.Parameter(torch.empty(self.k_experts, size[0], size[1])),\n",
    "                            nn.Parameter(torch.empty(self.k_experts, size[1])),\n",
    "                            None\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if self.keep_prob > 0:\n",
    "                    layers.append((None, None, F.dropout))\n",
    "\n",
    "            self.layers = layers\n",
    "\n",
    "        def init_params(self):\n",
    "            for i, (w, b, _) in enumerate(self.layers):\n",
    "                if w is None:\n",
    "                    continue\n",
    "\n",
    "                i = str(i)\n",
    "                torch.nn.init.kaiming_uniform_(w)\n",
    "                b.data.fill_(0.01)\n",
    "                self.register_parameter(\"w\" + i, w)\n",
    "                self.register_parameter(\"b\" + i, b)\n",
    "\n",
    "        def save_checkpoint(self, best_val_loss:float=np.inf, checkpoint_dir=MODEL_PATH):\n",
    "\n",
    "            model = {\"dimensions\":self.dimensions,\n",
    "                     \"name\":self.name,\n",
    "                     \"gate\":self.gate.state_dict(), \"phase_input_dim\":self.phase_input_dim,\n",
    "                     \"generationNetwork\":self.state_dict(),\n",
    "                     \"gate_size\":self.gate_size,\n",
    "                     \"k_experts\":self.k_experts,\n",
    "                     }\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.mkdir(checkpoint_dir)\n",
    "            path = os.path.join(checkpoint_dir, self.name)\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "\n",
    "            filePath = os.path.join(path, str(best_val_loss)+\".pbz2\")\n",
    "            with bz2.BZ2File(filePath, \"w\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            return filePath\n",
    "\n",
    "        @staticmethod\n",
    "        def load_checkpoint(filePath):\n",
    "            with bz2.BZ2File(filePath, \"rb\") as f:\n",
    "                obj = pickle.load(f)\n",
    "\n",
    "            model = MoE(name=obj[\"name\"], dimensions=obj[\"dimensions\"], gate_size=obj[\"gate_size\"],k_experts=obj[\"k_experts\"],\n",
    "                        phase_input_dim=obj[\"phase_input_dim\"], load=True)\n",
    "            model.gate.load_state_dict(obj[\"gate\"])\n",
    "            model.load_state_dict(obj[\"generationNetwork\"])\n",
    "            return model\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.99)\n",
    "            return optimizer\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    class MotionGenerationModel(pl.LightningModule):\n",
    "        def __init__(self, config:dict=None, pose_autoencoder=None, cost_input_dimension=None, feature_dims=None,\n",
    "                     input_slicers:list=None, output_slicers:list=None, train_set=None, val_set=None, name=\"model\", load=False):\n",
    "            super().__init__()\n",
    "\n",
    "            if not load:\n",
    "                self.pose_autoencoder = pose_autoencoder # start with 3\n",
    "                cost_hidden_dim = config[\"cost_hidden_dim\"]\n",
    "                cost_output_dim = config[\"cost_output_dim\"]\n",
    "                self.feature_dims = feature_dims\n",
    "                self.phase_dim = feature_dims[\"phase_vec\"]\n",
    "                # self.trajectory_dim = feature_dims[\"tPos\"]\n",
    "                # self.cost_dim = trajectory_dim + feature_dims[\"posCost\"]\n",
    "                self.cost_encoder = MLP(dimensions=[cost_dim, cost_hidden_dim, cost_hidden_dim, cost_output_dim],\n",
    "                                        name=\"CostEncoder\", load=True, single_module=-1)\n",
    "\n",
    "               # phase_input_dimension = input_slicers[0]\n",
    "                moe_input_dim = pose_autoencoder.dimensions[-1] #+ feature_dims[\"targetPosition\"]\n",
    "                moe_output_dim = pose_autoencoder.dimensions[-1] #+ self.phase_dim # + self.trajectory_dim\n",
    "                self.generationModel =  MoE(config=config, dimensions=[moe_input_dim, moe_output_dim], phase_input_dim= feature_dims[\"targetPosition\"],\n",
    "                                            name=\"MixtureOfExperts\")\n",
    "\n",
    "                self.in_slices = [0] + list(accumulate(add, input_slicers))\n",
    "                self.out_slices = [0] + list(accumulate(add, output_slicers))\n",
    "                # self.phase_dim = phase_dim\n",
    "\n",
    "                self.config=config\n",
    "                self.batch_size = config[\"batch_size\"]\n",
    "                self.learning_rate = config[\"lr\"]\n",
    "                self.loss_fn = config[\"loss_fn\"]\n",
    "                self.window_size = config[\"window_size\"]\n",
    "                self.autoregress_chunk_size = config[\"autoregress_chunk_size\"]\n",
    "                self.autoregress_prob = config[\"autoregress_prob\"]\n",
    "                self.autoregress_inc = config[\"autoregress_inc\"]\n",
    "                self.best_val_loss = np.inf\n",
    "                self.phase_smooth_factor = 0.9\n",
    "\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.name = name\n",
    "            self.epochs = 0\n",
    "            self.automatic_optimization = False\n",
    "            self.left_id = 14*3\n",
    "            self.right_id = 20*3\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            x_tensors = [x[:, d0:d1] for d0, d1 in zip(self.in_slices[:-1], self.in_slices[1:])]\n",
    "            pose_h, pose_label = self.pose_autoencoder.encode(x_tensors[1])\n",
    "            phase = x_tensors[0][:, :self.phase_dim]\n",
    "            targets = x_tensors[0][:, self.phase_dim:]\n",
    "            jointPos =torch.cat([x_tensors[1][:, self.left_id:self.left_id+3],x_tensors[1][:, self.right_id:self.right_id+3]], dim=1)\n",
    "\n",
    "            # embedding = torch.cat([pose_h, (x_tensors[2])], dim=1)\n",
    "            # embedding = torch.cat([pose_h)], dim=1)\n",
    "            # embedding = torch.cat([pose_h, x_tensors[2]], dim=1)\n",
    "\n",
    "            # posCost = self.computeCost(targets, x_tensors[-1][:, :self.trajectory_dim])\n",
    "            # out = self.generationModel(embedding, torch.cat([phase, x_tensors[-1][:, self.trajectory_dim:]], dim=1))\n",
    "            out = self.generationModel(pose_h, targets-jointPos)\n",
    "\n",
    "            # out_tensors = [out[:, d0:d1] for d0, d1 in zip(self.out_slices[:-1], self.out_slices[1:])] # phase_update, new_pose\n",
    "\n",
    "            # phase = self.update_phase(phase, out_tensors[0]) # phase_0, phase_1, phase_update\n",
    "\n",
    "            # new_pose = self.pose_autoencoder.decode(out_tensors[1], pose_label)\n",
    "            new_pose = self.pose_autoencoder.decode(out, pose_label)\n",
    "\n",
    "            # out_tensors[-1][:, 9:12] = new_pose[:, self.left_id:self.left_id+3]\n",
    "            #\n",
    "            # out_tensors[-1][:, 27:30] = new_pose[:, self.right_id:self.right_id+3]\n",
    "\n",
    "\n",
    "            posCost = self.computeCost(targets, torch.cat([new_pose[:, self.left_id:self.left_id+3],new_pose[:, self.right_id:self.right_id+3]], dim=1))\n",
    "\n",
    "            # print(phase.size(), targets.size(), new_pose.size(), pose_label.size(), out_tensors[-1].size(), posCost.size(), rotCost.size())\n",
    "            # return [phase, targets, new_pose, pose_label, out_tensors[-1], posCost, rotCost]\n",
    "            return [phase, targets, new_pose, pose_label, posCost]\n",
    "            # return [phase, targets, new_pose, pose_label, out_tensors[-1]]\n",
    "\n",
    "        def computeCost(self, targets, trajectory):\n",
    "            targetPos = targets\n",
    "            # targetRot = targets[:, self.feature_dims[\"targetPosition\"]:]\n",
    "            posT = trajectory\n",
    "            # rotT = trajectory[:, self.feature_dims[\"tPos\"]:]\n",
    "\n",
    "            # targetPos = targetPos.reshape((-1, 2, 3))\n",
    "            # posT = posT.reshape((-1, 2, 3))\n",
    "            # targetRot = targetRot.reshape((-1, 12, 3,3))\n",
    "            # rotT = rotT.reshape((-1, 12, 3, 3))\n",
    "\n",
    "            posCost = ((targetPos - posT)**2).reshape((-1, self.feature_dims[\"posCost\"]))\n",
    "            # colLength = torch.sqrt(torch.clip(torch.sum(rotT**2, axis=2), 0))\n",
    "            # rotT = rotT / colLength[:, :, :, None]\n",
    "\n",
    "            # rotT = torch.transpose(rotT, dim0=2, dim1=3)\n",
    "            # trace =torch.diagonal(targetRot @ rotT, offset=0, dim1=2, dim2=3).sum(dim=2)\n",
    "            # rotCost = torch.abs(torch.arccos((torch.clamp( (trace - 1) / 2.0, -1, 1))))\n",
    "            # torch.nan_to_num_(rotCost, 0)\n",
    "            # rotCost = rotCost.reshape((-1, self.feature_dims[\"rotCost\"]))\n",
    "\n",
    "            return posCost\n",
    "\n",
    "\n",
    "\n",
    "        def step(self, x, y, validation=False):\n",
    "            if not validation:\n",
    "               opt = self.optimizers()\n",
    "            x = x.squeeze(dim=2)\n",
    "            y = y.squeeze(dim=2)\n",
    "\n",
    "            n = x.size()[1]\n",
    "            tot_loss = 0\n",
    "            tot_posLoss = 0\n",
    "            tot_rotLoss = 0\n",
    "            x_c = x[:,0,:]\n",
    "\n",
    "            if self.autoregress_prob < 1:\n",
    "                autoregress_bools = torch.randn(n) < self.autoregress_prob\n",
    "                for i in range(1, n):\n",
    "                    y_c = y[:,i-1,:]\n",
    "                    # y_c.requires_grad_(True)\n",
    "\n",
    "                    out= self(x_c)\n",
    "                    recon = out[2]\n",
    "                    loss = self.loss_fn(recon, y_c)\n",
    "                    posLoss = torch.mean(out[-1])\n",
    "                    # rotLoss = torch.mean(out[-1])\n",
    "                    # rotLoss = 0\n",
    "                    #\n",
    "                    tot_loss += loss.detach()\n",
    "                    tot_posLoss += posLoss.detach() * float(i)/float(n)\n",
    "                    # tot_rotLoss += rotLoss.detach()\n",
    "                    tot_rotLoss += 0\n",
    "\n",
    "                    # elif not recon.requires_grad:\n",
    "                    #     raise ValueError(\"recon no grad, i : \", i, \" \\n\", recon, y_c)\n",
    "                    # elif not loss.requires_grad:\n",
    "                    #     raise ValueError(\"loss no grad\")\n",
    "                    if not validation:\n",
    "                        opt.zero_grad()\n",
    "                        # self.optimizer.zero_grad()\n",
    "                        self.manual_backward(loss)\n",
    "                        # self.optimizer.step()\n",
    "                        opt.step()\n",
    "\n",
    "                    if self.autoregress_prob > 0 and autoregress_bools[i]:\n",
    "                        x_c = torch.cat(out, dim=1).detach()\n",
    "                    else:\n",
    "                        x_c = x[:,i,:]\n",
    "\n",
    "                tot_loss /= float(i+1)\n",
    "                # tot_posLoss /= float(i+1)\n",
    "                tot_rotLoss /= float(i+1)\n",
    "            else:\n",
    "                for i in range(1, n):\n",
    "                    y_c = y[:,i-1,:]\n",
    "\n",
    "                    out= self(x_c)\n",
    "                    # recon = torch.cat([out[0], out[2], out[4]], dim=1)\n",
    "                    recon = out\n",
    "                    loss = self.loss_fn(recon, y_c)\n",
    "                    # posLoss = torch.mean(out[-2])\n",
    "                    posLoss = torch.mean(out[-1])\n",
    "                    # rotLoss = torch.mean(out[-1])\n",
    "                    # rotLoss = 0\n",
    "\n",
    "                    tot_loss += loss.detach()\n",
    "                    tot_posLoss = posLoss.detach()* float(i)/float(n)\n",
    "                    # tot_rotLoss += rotLoss.detach()\n",
    "                    tot_rotLoss += 0\n",
    "                    # self.optimizer.zero_grad()\n",
    "                    # (loss + posLoss + rotLoss).backward()\n",
    "                    if not validation:\n",
    "                        opt.zero_grad()\n",
    "                        # self.optimizer.zero_grad()\n",
    "                        self.manual_backward(loss)\n",
    "                        # self.optimizer.step()\n",
    "                        opt.step()\n",
    "\n",
    "                    x_c = torch.cat(out, dim=1).detach()\n",
    "\n",
    "                tot_loss /= float(i+1)\n",
    "                # tot_posLoss /= float(i+1)\n",
    "                tot_rotLoss /= float(i+1)\n",
    "\n",
    "            return tot_loss, tot_posLoss, tot_rotLoss\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            loss, posLoss, rotLoss = self.step(x,y, False)\n",
    "\n",
    "            self.log(\"ptl/train_loss\", loss, prog_bar=True)\n",
    "            self.log(\"ptl/train_posLoss\", posLoss)\n",
    "            self.log(\"ptl/train_rotLoss\", rotLoss)\n",
    "            # return loss#+posLoss+rotLoss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "\n",
    "            loss, posLoss, rotLoss = self.step(x,y, True)\n",
    "            self.log(\"ptl/val_loss\", loss, prog_bar=True)\n",
    "            self.log(\"ptl/val_posLoss\", posLoss, prog_bar=True)\n",
    "            self.log(\"ptl/val_rotLoss\", rotLoss, prog_bar=True)\n",
    "            return {\"val_loss\":loss}\n",
    "\n",
    "        def validation_epoch_end(self, outputs):\n",
    "            # if self.epochs > 0 and self.epochs % 20==0:\n",
    "            if self.epochs == 20:\n",
    "            #     self.autoregress_prob = min(1, self.autoregress_prob+self.autoregress_inc)\n",
    "                self.autoregress_prob = .5\n",
    "            #     self.autoregress_chunk_size = min(120, self.autoregress_chunk_size+self.autoregress_inc)\n",
    "            # if self.epochs > 0 and self.epochs % 20==0:\n",
    "            # self.scheduler.step()\n",
    "            self.epochs += 1\n",
    "\n",
    "\n",
    "            avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            self.log(\"avg_val_loss\", avg_loss)\n",
    "            if avg_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_loss\n",
    "                self.save_checkpoint()\n",
    "\n",
    "        def save_checkpoint(self, checkpoint_dir=MODEL_PATH):\n",
    "            path = os.path.join(checkpoint_dir, self.name)\n",
    "            loss = self.best_val_loss.cpu().numpy()\n",
    "\n",
    "            pose_autoencoder_path = self.pose_autoencoder.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "            cost_encoder_path = self.cost_encoder.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "            generationModel_path = self.generationModel.save_checkpoint(best_val_loss=loss, checkpoint_dir=path)\n",
    "\n",
    "            model = {\"name\":self.name,\n",
    "                     \"pose_autoencoder_path\":pose_autoencoder_path,\n",
    "                     \"cost_encoder_path\": cost_encoder_path,\n",
    "                     \"motionGenerationModelPath\":generationModel_path,\n",
    "                     \"in_slices\":self.in_slices,\n",
    "                     \"out_slices\":self.out_slices,\n",
    "                     }\n",
    "\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            with bz2.BZ2File(os.path.join(path,\n",
    "                                          str(loss)+\".pbz2\"), \"w\") as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "        @staticmethod\n",
    "        def load_checkpoint(filename, pose_ae_model, cost_encoder_model, generation_model):\n",
    "            with bz2.BZ2File(filename, \"rb\") as f:\n",
    "                obj = pickle.load(f)\n",
    "\n",
    "            pose_autoencoder = pose_ae_model.load_checkpoint(obj[\"pose_autoencoder_path\"])\n",
    "            cost_encoder = cost_encoder_model.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "            generationModel = generation_model.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "            model = MotionGenerationModel(name=obj[\"name\"])\n",
    "            model.pose_autoencoder = pose_autoencoder\n",
    "            model.cost_encoder = cost_encoder\n",
    "            model.generationModel = generationModel\n",
    "            model.in_slices = obj[\"in_slices\"]\n",
    "            model.out_slices = obj[\"out_slices\"]\n",
    "\n",
    "            return model\n",
    "\n",
    "        def update_phase(self, p1, p2):\n",
    "            return self.phase_smooth_factor * p2 + (1-self.phase_smooth_factor)*p1\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "            # optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=2.5e-3)\n",
    "            # optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "            # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=.1)\n",
    "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=.7)\n",
    "            # scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-7, max_lr=1e-3)\n",
    "            # self.scheduler = scheduler\n",
    "            self.optimizer = optimizer\n",
    "            return optimizer\n",
    "\n",
    "        def train_dataloader(self):\n",
    "\n",
    "            return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def val_dataloader(self):\n",
    "\n",
    "            return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        def test_dataloader(self):\n",
    "            return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    data_path = [\n",
    "                 \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two.pbz2\",\n",
    "                # \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One.pbz2\",\n",
    "                #  \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One-large.pbz2\",\n",
    "                #  \"/home/nuoc/Documents/MEX/data/ONE_R2-default-One-small.pbz2\",\n",
    "                #  \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two-small.pbz2\",\n",
    "                #  \"/home/nuoc/Documents/MEX/data/TWO_R2-default-Two-large.pbz2\",\n",
    "                 \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two.pbz2\",\n",
    "                 # \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two-large.pbz2\",\n",
    "                 # \"/home/nuoc/Documents/MEX/data/TWO_ROT_R2-default-Two-small.pbz2\",\n",
    "                 ]\n",
    "\n",
    "\n",
    "\n",
    "    # pose_features = [\"pos\", \"rotMat\", \"velocity\", \"isLeft\", \"chainPos\", \"geoDistanceNormalised\"]\n",
    "    # cost_features = [\"tPos\", \"tRot\", \"posCost\", \"rotCost\"]\n",
    "    # phase_features = [\"phase_vec\", \"targetPosition\", \"targetRotation\"]\n",
    "\n",
    "    pose_features = [\"pos\", \"rotMat\", \"velocity\", \"isLeft\", \"chainPos\", \"geoDistanceNormalised\"]\n",
    "    cost_features = [\"posCost\"]\n",
    "    phase_features = [\"phase_vec\", \"targetPosition\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    def load(file_path):\n",
    "        with bz2.BZ2File(file_path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "            return obj\n",
    "\n",
    "    data = [load(path) for path in data_path]\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "\n",
    "    from scipy.signal import savgol_filter as sav_filter\n",
    "\n",
    "    window_size = 3\n",
    "    frame_window = 15\n",
    "    sampling_step = frame_window / window_size\n",
    "\n",
    "    data_tensors = []\n",
    "    data_tensorsP = []\n",
    "    data_tensorsRot = []\n",
    "\n",
    "    data_dims = []\n",
    "    feature_list = []\n",
    "    feature_data = [{}, {}]\n",
    "    features = phase_features + pose_features + cost_features\n",
    "    for f in features:\n",
    "        feature_data[0][f] = []\n",
    "        feature_data[1][f] = []\n",
    "\n",
    "    first_row = True\n",
    "    first_time = True\n",
    "    key_joints = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    for Data in data:\n",
    "        for clip in Data:\n",
    "            d = pickle.loads(clip)\n",
    "            sequence = []\n",
    "            sequenceP = []\n",
    "            sequenceRot = []\n",
    "            n_frames = len(d[\"frames\"])\n",
    "            if first_time:\n",
    "                key_joints = [i for i in range(len(d[\"frames\"][0])) if d[\"frames\"][0][i][\"key\"]]\n",
    "                first_time = False\n",
    "\n",
    "            idx = [int(n_frames/2)] * len(key_joints)\n",
    "            for i, jo in enumerate(key_joints):\n",
    "                for f_id, frame in enumerate(d[\"frames\"]):\n",
    "                    if frame[jo][\"contact\"]:\n",
    "                        idx[i] = f_id\n",
    "                        break\n",
    "\n",
    "            max_id = max(idx)\n",
    "            max_id2 = max_id + 3\n",
    "            min_id = min(idx)\n",
    "            min_id2 = min(idx) - 3\n",
    "            start1 = 0\n",
    "            end1 = max_id2 if max_id2 <= n_frames else max_id\n",
    "            end2 = min_id2 if min_id2 >= 0 else min_id\n",
    "            start2 = n_frames-1\n",
    "            clip_idx = [(start1, end1), (start2, end2)]\n",
    "            # clip_idx = [(0, 120)]\n",
    "\n",
    "\n",
    "            for i, (start, end) in enumerate(clip_idx):\n",
    "                frames = d[\"frames\"]\n",
    "                if start < end:\n",
    "                    idx = np.arange(start+1, end-1)\n",
    "                    if len(idx) < 28: rep = True\n",
    "                    else: rep = False\n",
    "                    intervals = [sorted(np.random.choice(idx, 28, replace=rep).tolist()) for i in range(3)]\n",
    "                    # print(i, intervals, sorted(np.random.choice(idx, 28, replace=False).tolist()))\n",
    "                else:\n",
    "                    idx = np.arange(end+1, start-1)\n",
    "                    if len(idx) < 28: rep = True\n",
    "                    else: rep = False\n",
    "                    intervals = [sorted(np.random.choice(idx, 28, replace=rep).tolist(),reverse=True) for i in range(3)]\n",
    "                # intervals = [np.arange(0,120,2), np.arange(1,120,2)]\n",
    "                # print(intervals[0])\n",
    "                for interval in intervals:\n",
    "                    if start < end:\n",
    "                        interval = [idx[0]-1] + interval + [idx[-1]+1]\n",
    "                    else:\n",
    "                        interval = [idx[-1]+1] + interval + [idx[0]-1]\n",
    "                    for f in interval:\n",
    "                        row_vec = []\n",
    "                        positions = []\n",
    "                        rotations = []\n",
    "                # if i == 1: continue\n",
    "                    # if i == 0:\n",
    "                    #     n = end-start\n",
    "                    # else:\n",
    "                    #     n = start\n",
    "                    #\n",
    "                    # if i == 0:\n",
    "                    #     f_idx = np.arange(f-frame_window, f+frame_window, sampling_step, dtype=int)\n",
    "                    # else:\n",
    "                    #     f_idx = np.arange(f+frame_window, f-frame_window, -sampling_step, dtype=int)\n",
    "                    #\n",
    "                    # f_idx[f_idx < 0] = 0\n",
    "                    # f_idx[f_idx >= n] = n\n",
    "                    # f_idx = f_idx.tolist()\n",
    "                        f_idx = [f]\n",
    "                        for feature in phase_features:\n",
    "                            if feature == \"phase_vec\":\n",
    "                                # sin = np.asarray([frames[idx][jj][\"phase_vec\"] for jj in key_joints for idx in f_idx])\n",
    "                                sin = np.asarray([frames[idx][jj][\"sin_normalised_contact\"] for jj in key_joints for idx in f_idx])\n",
    "                                # vel = np.concatenate([frames[idx][jj][\"velocity\"] for jj in key_joints for idx in f_idx])\n",
    "                                # vel = np.reshape(vel, (3,-1))\n",
    "                                # vel = np.sqrt(np.sum(vel**2, axis=0))\n",
    "                                cos = np.cos(np.arcsin(np.asarray([frames[idx][jj][\"sin_normalised_contact\"] for jj in key_joints for idx in f_idx])))\n",
    "                                # cos = cos * vel\n",
    "                                row_vec.append(np.concatenate([np.asarray([sin[i], cos[i]]) for i in range(len(sin))]))\n",
    "                            elif feature == \"targetRotation\" or feature == \"targetPosition\":\n",
    "                                row_vec.append(np.concatenate([frames[idx][jj][feature].ravel() for jj in key_joints for idx in f_idx]))\n",
    "                            elif feature == \"contact\":\n",
    "                                row_vec.append(np.asarray([frames[idx][jj][\"contact\"] for jj in key_joints for idx in f_idx]))\n",
    "                            else:\n",
    "                                row_vec.append(np.concatenate([frames[f][jj][feature] for jj in key_joints]))\n",
    "\n",
    "                            # feature_data[i][feature].append(row_vec)\n",
    "                            if first_row:\n",
    "                                data_dims.append(row_vec[-1].shape)\n",
    "                                feature_list.append(feature)\n",
    "\n",
    "                        for feature in pose_features:\n",
    "                            if feature==\"rotMat\":\n",
    "                                joRot = np.concatenate([jo[\"rotMat\"].ravel() for jo in frames[f]])\n",
    "\n",
    "                                rotations = joRot\n",
    "\n",
    "                                joRot = joRot.reshape((-1, 3, 3))\n",
    "                                joRot_s = np.sqrt(np.sum(joRot**2, axis=1))\n",
    "                                joRot = joRot / joRot_s[:, :, None]\n",
    "\n",
    "                                if np.sum(np.isnan(joRot)) > 0:\n",
    "                                    raise ValueError(\"RotMat has nan\")\n",
    "                                row_vec.append(joRot.ravel())\n",
    "\n",
    "                            elif feature == \"isLeft\" or feature == \"chainPos\" or feature == \"geoDistanceNormalised\":\n",
    "                                row_vec.append(np.concatenate([[jo[feature]] for jo in frames[f]]))\n",
    "                            elif feature == \"pos\":\n",
    "                                row_vec.append(np.concatenate([jo[feature] for jo in frames[f]]))\n",
    "                                positions = np.concatenate([jo[feature] for jo in frames[f]])\n",
    "                            else:\n",
    "                                row_vec.append(np.concatenate([jo[feature] for jo in frames[f]]))\n",
    "\n",
    "                            # feature_data[i][feature].append(row_vec)\n",
    "                            if first_row:\n",
    "                                data_dims.append(row_vec[-1].shape)\n",
    "                                feature_list.append(feature)\n",
    "                        for feature in cost_features:\n",
    "                            if feature == \"posCost\":\n",
    "                                targetPos = np.concatenate([frames[idx][jj][\"targetPosition\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                                targetPos = targetPos.reshape((len(key_joints), len(f_idx), 3))\n",
    "                                joPos = np.concatenate([frames[idx][jj][\"pos\"] for jj in key_joints for idx in f_idx])\n",
    "                                joPos = joPos.reshape((len(key_joints), len(f_idx), 3))\n",
    "                                posCost = ((targetPos - joPos)**2).ravel()\n",
    "                                # posCost = np.concatenate([frames[idx][jj][\"posCost\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                                row_vec.append(posCost)\n",
    "                            elif feature == \"rotCost\":\n",
    "                                targetRot = np.concatenate([frames[idx][jj][\"targetRotation\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                                targetRot = targetRot.reshape((len(key_joints), len(f_idx), 3, 3))\n",
    "\n",
    "                                joRot = np.concatenate([frames[idx][jj][\"rotMat\"].ravel() for jj in key_joints for idx in f_idx])\n",
    "                                joRot = joRot.reshape((len(key_joints), len(f_idx), 3, 3))\n",
    "                                joRot_s = np.sqrt(np.sum(joRot**2, axis=2))\n",
    "                                joRot = joRot / joRot_s[:, :, :, None]\n",
    "\n",
    "                                joRotT = np.transpose(joRot, (0, 1, 3, 2))\n",
    "                                rotDiff = np.arccos(np.clip((np.trace(targetRot @ joRotT, axis1=2, axis2=3) - 1) / 2.0, -1, 1).ravel())\n",
    "\n",
    "                                if np.sum(np.isnan(rotDiff)) > 0:\n",
    "                                    raise ValueError(\"RotCost has nan\")\n",
    "                                row_vec.append(rotDiff)\n",
    "                            elif feature == \"tPos\" or feature == \"tRot\":\n",
    "                                feature = \"pos\" if feature == \"tPos\" else \"rotMat\"\n",
    "                                row_vec.append(np.concatenate([frames[idx][jj][feature].ravel() for jj in key_joints for idx in f_idx]))\n",
    "                            else:\n",
    "                                row_vec.append(np.concatenate([frames[f][jj][feature] for jj in key_joints]))\n",
    "\n",
    "                            # feature_data[i][feature].append(row_vec)\n",
    "                            if first_row:\n",
    "                                data_dims.append(row_vec[-1].shape)\n",
    "                                feature_list.append(feature)\n",
    "\n",
    "                        if first_row: first_row = False\n",
    "                    # if i == 1:\n",
    "                    #     row_vec = np.flip(np.concatenate(row_vec))\n",
    "                    #     sequence.append(row_vec)\n",
    "                    # else:\n",
    "                        sequence.append(np.concatenate(row_vec))\n",
    "                        sequenceP.append(positions)\n",
    "                        sequenceRot.append(rotations)\n",
    "                    data_tensors.append(np.vstack(sequence))\n",
    "                    data_tensorsP.append(np.vstack(sequenceP))\n",
    "                    data_tensorsRot.append(np.vstack(sequenceRot))\n",
    "                    sequence = []\n",
    "                    sequenceP = []\n",
    "                    sequenceRot = []\n",
    "            # break\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    def loss_fn(x, y):\n",
    "        return nn.functional.mse_loss(x,y)\n",
    "    def loss_fn2(x, y):\n",
    "        return nn.functional.smooth_l1_loss(x,y)\n",
    "    def normalise(x):\n",
    "        std = torch.std(x, dim=0)\n",
    "        std[std==0] = 1\n",
    "        return (x-torch.mean(x, dim=0)) / std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    x_tensors = torch.stack([normalise(torch.from_numpy(clip[:-1])).float() for clip in data_tensors])\n",
    "    y_tensors = torch.stack([torch.from_numpy(clip[1:]).float() for clip in data_tensors])\n",
    "    y_tensors2 = torch.stack([normalise(torch.from_numpy(clip[1:])).float() for clip in data_tensors])\n",
    "    positions = torch.stack([torch.from_numpy(pos[1:]).float() for pos in data_tensorsP])\n",
    "    rotations = torch.stack([torch.from_numpy(rot[1:]).float() for rot in data_tensorsRot])\n",
    "    # print(len(data_tensorsRot), len([0]), len(data_tensorsRot[1]),  len(data_tensorsRot[2]))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    extra_feature_len = 21 * 3\n",
    "    n_phase_features = len(phase_features)\n",
    "    n_pose_features = len(pose_features)\n",
    "    phase_dim = np.sum(data_dims[0:n_phase_features])\n",
    "    pp_dim = data_dims[0][0]\n",
    "    pose_dim = np.sum(data_dims[n_phase_features:n_phase_features+n_pose_features])\n",
    "    cost_dim = np.sum(data_dims[n_phase_features+n_pose_features:])\n",
    "\n",
    "    table = [feature_list, data_dims]\n",
    "    print(tabulate(table))\n",
    "    print(\"phase dim: \",phase_dim)\n",
    "    print(\"pose dim: \", pose_dim)\n",
    "    print(\"cost dim: \", cost_dim)\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    dataset = TensorDataset(torch.Tensor(x_tensors), torch.Tensor(y_tensors2))\n",
    "    N = len(x_tensors)\n",
    "\n",
    "    train_ratio = int(.8*N)\n",
    "    val_ratio = int((N-train_ratio) / 2.0)\n",
    "    test_ratio = N - train_ratio - val_ratio\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(2021))\n",
    "    print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # with bz2.BZ2File(\"data_sets_9_30hz_phaes+pos+cost_single_frame_fullset.pbz2\", \"rb\") as f:\n",
    "    #     obj = pickle.load(f)\n",
    "\n",
    "    # train_set = obj[\"data_sets\"][0]\n",
    "    # val_set = obj[\"data_sets\"][1]\n",
    "    # test_set = obj[\"data_sets\"][2]\n",
    "    # table = obj[\"table\"]\n",
    "    feature_dims = {}\n",
    "    for feat, dim in zip(table[0], table[1]):\n",
    "        if feat in feature_dims:\n",
    "            if feat == \"pos\": feat = \"tPos\"\n",
    "            elif feat == \"rotMat\": feat = \"tRot\"\n",
    "        feature_dims[feat] = dim[0]\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    config = {\n",
    "        \"k_experts\": 4,\n",
    "        \"gate_size\": 32,\n",
    "        \"keep_prob\": 0.3,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"cost_hidden_dim\" : 16,\n",
    "        \"cost_output_dim\" : 16,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 1e-5,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"window_size\":1,\n",
    "        \"autoregress_prob\" : 0,\n",
    "        \"autoregress_inc\" : 0,\n",
    "        \"autoregress_chunk_size\": 120\n",
    "    }\n",
    "\n",
    "    model_name = \"Test_23_phase+pose+cost_single\"\n",
    "    epochs = 120\n",
    "\n",
    "    #%%\n",
    "\n",
    "    phase_dim = sum([feature_dims[feat] for feat in phase_features])\n",
    "    pose_dim = sum([feature_dims[feat] for feat in pose_features])\n",
    "    cost_dim = sum([feature_dims[feat] for feat in cost_features])\n",
    "    # trajectory_dim = feature_dims[\"tPos\"]\n",
    "\n",
    "    pose_autoencoder = MLP_withLabel.load_checkpoint(\"/home/nuoc/Documents/MEX/models/MLP4_withLabel_best/M3/0.00324857.512.pbz2\")\n",
    "    pose_encoder_out_dim = pose_autoencoder.dimensions[-1]\n",
    "\n",
    "    input_slices=[phase_dim, pose_dim, cost_dim]\n",
    "    output_slices=[feature_dims[\"phase_vec\"], pose_encoder_out_dim]\n",
    "    print(input_slices)\n",
    "    print(output_slices)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    def reformLabel(dataset, slices, phase_dim, extra_feature_len):\n",
    "        set_y = list(dataset[:][1])\n",
    "        slices = [0] + list(accumulate(add, slices))\n",
    "        for i, y in enumerate(set_y):\n",
    "            y = y.squeeze(1)\n",
    "            y_tensors = [y[:, d0:d1] for d0, d1 in zip(slices[:-1], slices[1:])]\n",
    "            phase = y_tensors[0][:, :phase_dim]\n",
    "            pose = y_tensors[1][:, :-extra_feature_len]\n",
    "            trajectory = y_tensors[2]\n",
    "            set_y[i] = torch.cat([pose], dim=1).unsqueeze(dim=1)\n",
    "        return [(x, y) for x, y in zip(dataset[:][0], set_y)]\n",
    "\n",
    "    train_set2 = reformLabel(train_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n",
    "    val_set2 = reformLabel(val_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n",
    "    test_set2 = reformLabel(test_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3)\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    model = MotionGenerationModel(config=config, pose_autoencoder=pose_autoencoder, cost_input_dimension=cost_dim,\n",
    "                              input_slicers=input_slices, output_slicers=output_slices,feature_dims=feature_dims,\n",
    "                              train_set=train_set2, val_set=val_set2, name=model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    import pytorch_lightning as pl\n",
    "    # prob\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"avg_val_loss\", save_top_k=3)\n",
    "    earlystopping = EarlyStopping(monitor=\"avg_val_loss\", patience=10)\n",
    "    logger=TensorBoardLogger(save_dir=\"logs/\", name=model_name, version=\"0.0\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=\"/home/nuoc/Documents/MEX/src/motion_generation/checkpoints\",\n",
    "        gpus=1, precision=16,\n",
    "        # callbacks=[earlystopping],\n",
    "        min_epochs=50,\n",
    "        max_epochs=epochs,\n",
    "        stochastic_weight_avg=True\n",
    "    )\n",
    "\n",
    "    # train_set = datasets[0][0]\n",
    "    # val_set = datasets[0][1]\n",
    "    train_loader = DataLoader(train_set2, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "    val_loader = DataLoader(val_set2, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "    trainer.fit(model,train_loader, val_loader)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # for i in range(10):\n",
    "        # model.scheduler.step()\n",
    "    trainer.fit(model,train_loader, val_loader)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    def load(path):\n",
    "        with bz2.BZ2File(path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "            return obj\n",
    "    mo = load(\"/home/nuoc/Documents/MEX/src/motion_generation/Test_12_phase+pose+cost_single_full/0.006421638.pbz2\")\n",
    "    ae = load(mo[\"pose_autoencoder_path\"])\n",
    "    cost = load(mo[\"cost_encoder_path\"])\n",
    "    gene = load(mo[\"motionGenerationModelPath\"])\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # model.pose_autoencoder.encoder.load_state_dict(ae[\"encoder\"])\n",
    "    # model.pose_autoencoder.decoder.load_state_dict(ae[\"decoder\"])\n",
    "    # model.cost_encoder.encoder.load_state_dict(cost[\"encoder\"])\n",
    "    # model.generationModel.gate.load_state_dict(gene[\"gate\"])\n",
    "    # model.generationModel.load_state_dict(gene[\"generationNetwork\"])\n",
    "    # model.name = \"\"\n",
    "    # pose_autoencoder = pose_ae_model.load_checkpoint(mo[\"pose_autoencoder_path\"])\n",
    "    # cost_encoder = cost_encoder_model.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "    # generationModel = generation_model.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "    # model.name = \"Test_13_phase+pose+cost_single_full\"\n",
    "    model.save_checkpoint(\"\")\n",
    "    # model = MotionGenerationModel.load_checkpoint(\n",
    "    #     filename=\",\n",
    "    # pose_ae_model=MLP_withLabel, cost_encoder_model=MLP, generation_model=MoE)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # model.learning_rate = 1e-5\n",
    "    # test_set2 = reformLabel(test_set, slices=input_slices, phase_dim=feature_dims[\"phase_vec\"], extra_feature_len=21*3, trajectory_dim=trajectory_dim)\n",
    "    model.autoregress_prob = 0\n",
    "    test_loader = DataLoader(test_set2, batch_size=config[\"batch_size\"], pin_memory=True, num_workers=6)\n",
    "    model.cpu()\n",
    "    with torch.no_grad():\n",
    "        loss, posLoss = 0, 0\n",
    "        for x,y in test_loader:\n",
    "            x = x.to(\"cpu\")\n",
    "            y = y.to(\"cpu\")\n",
    "            l, pll, rl = model.step(x, y, validation=True)\n",
    "            loss += l\n",
    "            posLoss += pll\n",
    "        loss /= float(len(test_loader))\n",
    "        posLoss /= float(len(test_loader))\n",
    "        print(loss.item())\n",
    "        print(posLoss.item())\n",
    "\n",
    "    #%%\n",
    "\n",
    "    print(positions.size(), rotations.size())\n",
    "\n",
    "    #%%\n",
    "\n",
    "    model.autoregress_prob = 1\n",
    "    # test_loader = DataLoader(test_set, batch_size=1, pin_memory=True, num_workers=6)\n",
    "    model.cpu()\n",
    "    n = 10\n",
    "    idx = np.random.randint(0, len(x_tensors), n)\n",
    "    originalp = []\n",
    "    originalr = []\n",
    "    generated = []\n",
    "    pose_idx_upper = 10 + feature_dims[\"pos\"] #+ feature_dims[\"rotMat\"]\n",
    "    print(pose_idx_upper)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # loss, posLoss = 0, 0\n",
    "        # for i, (x,y) in enumerate(test_loader):\n",
    "        for i in range(1):\n",
    "            # if i > n:\n",
    "            #     break\n",
    "            # o_frames = torch.cat((positions[idx,:, : ], rotations[idx, :, :]),dim=1)\n",
    "            # original = torch.cat((positions[idx,:, : ], rotations[idx, :, :]),dim=2)\n",
    "            originalp = positions[idx, :, :]\n",
    "            originalr = rotations[idx, :, :]\n",
    "            x = x_tensors[idx]\n",
    "            size = x.size()[1]\n",
    "            g_frames = []\n",
    "            x_c = x[:, 0, :]\n",
    "            for i in range(0, size):\n",
    "                out = model(x_c)\n",
    "                x_c = torch.cat(out, dim=1)\n",
    "                g_frames.append(x_c.unsqueeze(1))\n",
    "            # original.append(o_frames)\n",
    "            generated.append(torch.cat(g_frames, dim=1))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    print(originalp.size())\n",
    "    print(originalr.size())\n",
    "    print(generated[0].size())\n",
    "\n",
    "    #%%\n",
    "\n",
    "    # original2p = torch.Tensor(originalp)\n",
    "    generatedp = generated[0][:, :, 10:73]\n",
    "    generatedt = generated[0][:, :, 4:10]\n",
    "    generatedr = generated[0][:, :, 73:262]\n",
    "    # original2 = positions[40:50].reshape(-1, 29, 21, 3)\n",
    "    # generated2 = generated[0][40:50, :, 10:73].reshape(-1, 29, 21, 3)\n",
    "    # print(original2.shape)\n",
    "    # print(generated2.shape)\n",
    "\n",
    "\n",
    "\n",
    "    #%%\n",
    "\n",
    "    n = originalp.shape[1]\n",
    "    original2p = originalp.reshape((-1, n, 21, 3))\n",
    "    original2r = originalr.reshape((-1, n, 21, 9))\n",
    "    generated2p = generatedp.reshape((-1, n, 21, 3))\n",
    "    generated2r = generatedr.reshape((-1, n, 21, 9))\n",
    "    # print(original2.shape, generated2.shape)\n",
    "    # print(loss_fn2(original2p, generated2p))\n",
    "    print(loss_fn(original2p, generated2p))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    template = js.load(open(\"/run/media/nuoc/70BAFF0DBAFECE9A/Users/Neroro/AppData/LocalLow/DefaultCompany/Procedural Animation/TWO_R2-default-Two/0.json\", \"r\"))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    def insert_pos(clips, rotations, target, name):\n",
    "        for c in range(len(clips)):\n",
    "            template[\"frames\"][0][\"targetsPositions\"][0][\"x\"] = target[c,0,0].item()\n",
    "            template[\"frames\"][0][\"targetsPositions\"][0][\"y\"] = target[c,0,1].item()\n",
    "            template[\"frames\"][0][\"targetsPositions\"][0][\"z\"] = target[c,0,2].item()\n",
    "            template[\"frames\"][0][\"targetsPositions\"][1][\"x\"] = target[c,0,3].item()\n",
    "            template[\"frames\"][0][\"targetsPositions\"][1][\"y\"] = target[c,0,4].item()\n",
    "            template[\"frames\"][0][\"targetsPositions\"][1][\"z\"] = target[c,0,5].item()\n",
    "\n",
    "            for f in range(n):\n",
    "                for j in range(21):\n",
    "                    template[\"frames\"][f][\"joints\"][j][\"position\"][\"x\"] = clips[c,f,j,0].item()\n",
    "                    template[\"frames\"][f][\"joints\"][j][\"position\"][\"y\"] = clips[c,f,j,1].item()\n",
    "                    template[\"frames\"][f][\"joints\"][j][\"position\"][\"z\"] = clips[c,f,j,2].item()\n",
    "                    k = 0\n",
    "                    for col in [\"c0\", \"c1\", \"c2\"]:\n",
    "                        for cell in [\"x\", \"y\", \"z\"]:\n",
    "                            template[\"frames\"][f][\"joints\"][j][\"rotMat\"][col][cell] = rotations[c,f,j,k].item()\n",
    "                            k+=1\n",
    "            with open(\"{}_{}.json\".format(name, c), \"w\") as f:\n",
    "                js.dump(template, f)\n",
    "\n",
    "    insert_pos(original2p, original2r, generatedt, \"/home/nuoc/Documents/MEX/src/motion_generation/replay_clips_no_phase/original6\")\n",
    "    insert_pos(generated2p, generated2r,generatedt, \"/home/nuoc/Documents/MEX/src/motion_generation/replay_clips_no_phase/generated6\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}