{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dimensions:list, act_fn:str, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dimensions = dimensions          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.act= act_fn                     #   func\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "        self.batch_size = batch_size        #   int\n",
    "\n",
    "        self.model = []\n",
    "\n",
    "        assert(len(dimensions) >= 2)\n",
    "        assert(batch_size > 0)\n",
    "        assert(act_fn == \"elu\" or act_fn == \"relu\")\n",
    "        assert(keep_prob < 1)\n",
    "        for e in dimensions: assert(type(e) == int)\n",
    "\n",
    "        self.build()\n",
    "        self.model.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), self.activation(self.act)))\n",
    "                layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "\n",
    "        self.model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(fn_name):\n",
    "        if fn_name == \"elu\":\n",
    "            return nn.ELU()\n",
    "        elif fn_name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        else:\n",
    "            return nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(.01)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, num_layers:int = 1, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.num_layers = num_layers\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "\n",
    "        self.model = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=keep_prob)\n",
    "        hidden_state = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        cell_state = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        h_t, h_n = self.model(x, self.hidden)\n",
    "        self.hidden = h_n\n",
    "        return h_t\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, num_layers:int = 1, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.num_layers = num_layers\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "\n",
    "        self.model = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=keep_prob)\n",
    "        hidden_state = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        cell_state = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        h_t, h_n = self.model(x, self.hidden)\n",
    "        self.hidden = h_n\n",
    "        return h_t\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module):\n",
    "        super(DEC_AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 63)\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "all_data = []\n",
    "for d in data:\n",
    "    d = pickle.loads(d)\n",
    "    pos = []\n",
    "    for f in d[\"frames\"]:\n",
    "        p = [jo[\"pos\"] for jo in f]\n",
    "        pos.append(p)\n",
    "    all_data.append(pos)\n",
    "\n",
    "input_data = np.array([np.concatenate([p for p in j]) for pos in all_data for j in pos])\n",
    "print(input_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train:  1007 , Validation:  216 , Test:  217\n"
     ]
    }
   ],
   "source": [
    "data_ratio = (.7, .15, .15) # training, validation, testing\n",
    "SEED = 2021\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(input_data).float()\n",
    "y_tensor = torch.from_numpy(input_data).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "N = len(dataset)\n",
    "\n",
    "train_ratio = int(data_ratio[0]*N)\n",
    "val_ratio = int(data_ratio[1] * N)\n",
    "test_ratio = int(N-train_ratio-val_ratio)\n",
    "print(\"Train: \", train_ratio, \", Validation: \", val_ratio, \", Test: \", test_ratio)\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEC_AE(\n",
      "  (encoder): DEC(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (linear): Linear(in_features=63, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=36, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (assignment): ClusterAssignment()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (linear): Linear(in_features=36, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=63, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_dim = input_data.shape[1]\n",
    "output_dim = input_data.shape[1]\n",
    "latent_dim = 36         # 12 * 3\n",
    "encoder_layer_sizes = [input_dim, 256, 256, latent_dim]\n",
    "decoder_layer_sizes = [latent_dim, 256, 256, output_dim]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "act_fn = \"elu\"\n",
    "keep_prob = .2\n",
    "\n",
    "# model, loss and scheduler\n",
    "ae = StackedDenoisingAutoEncoder(encoder_layer_sizes, activation=nn.ELU(), final_activation=nn.ELU())\n",
    "model = DEC_AE(DEC(36, 36, ae.encoder), ae.decoder)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1884\n",
      "Epoch [2/100], Loss: 0.1721\n",
      "Epoch [3/100], Loss: 0.1721\n",
      "Epoch [4/100], Loss: 0.1721\n",
      "Epoch [5/100], Loss: 0.1721\n",
      "Epoch [6/100], Loss: 0.1721\n",
      "Epoch [7/100], Loss: 0.1721\n",
      "Early stopping at Epoch:  6\n",
      "last training loss: 0.172100\n",
      "achieved best validation loss: 0.1820 after at Epoch 0\n",
      "Test loss: 0.1849\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "i = 0\n",
    "n_epochs_no_improve = 5\n",
    "\n",
    "train_loader_len = float(len(train_loader))\n",
    "val_loader_len = float(len(val_loader))\n",
    "test_loader_len = float(len(test_loader))\n",
    "\n",
    "last_avg_training_loss = 0\n",
    "min_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_after_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # training\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        training_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    last_avg_training_loss = training_loss / train_loader_len\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, last_avg_training_loss))\n",
    "\n",
    "    # early stopping\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            pred_val = model(inputs)\n",
    "            loss_val = criterion(pred_val, labels)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= val_loader_len\n",
    "        if min_loss > val_loss:\n",
    "            min_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_after_epoch = epoch\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve > n_epochs_no_improve:\n",
    "                print(\"Early stopping at Epoch: \", epoch)\n",
    "                print(\"last training loss: {:2f}\".format(last_avg_training_loss))\n",
    "                print(\"achieved best validation loss: {:.4f} after at Epoch {}\".format(min_loss, best_model_after_epoch))\n",
    "                break\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        pred_test = model(inputs)\n",
    "        loss_test = criterion(pred_test, labels)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= test_loader_len\n",
    "    print(\"Test loss: {:.4f}\".format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}