{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import shutil\n",
    "import tempfile\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from cytoolz import sliding_window\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses simple Multi-layered perceptron (MLP) for both encoder and decoder.\n",
    "\n",
    "$\n",
    "enc = dec = mlp(X, \\theta), \\quad \\theta = W, b\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Test torch lightning + ray tune\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None, loss_fn=None,\n",
    "                 dataset=None, train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            dimensions.append(self.k)\n",
    "            self.dimensions = dimensions\n",
    "            self.loss_fn = loss_fn\n",
    "            self.keep_prob = keep_prob          #   %\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "            self.dataset = dataset\n",
    "\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.test_set = test_set\n",
    "            self.best_val_loss = np.inf\n",
    "\n",
    "            self.build()\n",
    "            if self.train_set is None:\n",
    "                self.setup_data([.7, .15, .15])\n",
    "\n",
    "            self.encoder.apply(self.init_params)\n",
    "            self.decoder.apply(self.init_params)\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "\n",
    "        for i, size in enumerate(layer_sizes):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "        self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        layers = []\n",
    "        for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "        self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir=\"../../models\"):\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                 \"encoder\":self.encoder.state_dict(),\n",
    "                 \"decoder\":self.decoder.state_dict()}\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        with bz2.BZ2File(os.path.join(path,\n",
    "                                      self.name+str(self.best_val_loss.cpu().numpy())+\".\"+str(self.k)+\".pbz2\"), \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filename, checkpoint_dir=\"../../models\"):\n",
    "        # return torch.load(os.path.join(checkpoint_dir,filename))\n",
    "        with bz2.BZ2File(os.path.join(checkpoint_dir, filename), \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        model = MLP(name=obj[\"name\"], dimensions=obj[\"dimensions\"], load=True)\n",
    "        model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        return model\n",
    "        # self.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        # self.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        # self.best_val_loss = obj[\"val_loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def setup_data(self, split_ratio, N):\n",
    "        self.n_train_samples= int(split_ratio[0]*N)\n",
    "        self.n_val_samples= int(split_ratio[1] * N)\n",
    "        self.n_test_samples= int(N-self.n_train_samples-self.n_val_samples)\n",
    "        self.train_set, self.val_set, self.test_set = random_split(self.dataset,\n",
    "                                                                   [self.n_train_samples,\n",
    "                                                                    self.n_val_samples,\n",
    "                                                                    self.n_test_samples])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_tune(config, dimensions:list,  loss_fn=None,\n",
    "                 dataset=None, train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, num_epochs=300, num_cpus=24, num_gpus=1, model_name=\"model\"):\n",
    "\n",
    "    model = MLP(config, dimensions, loss_fn, dataset, train_set, val_set, test_set, keep_prob, name=model_name)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(save_dir=\"logs/\", name=model_name, version=\"0.0\"),\n",
    "        progress_bar_refresh_rate=20,\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"loss\":\"avg_val_loss\",}, on=\"validation_end\"),\n",
    "            EarlyStopping(monitor=\"avg_val_loss\")\n",
    "        ],\n",
    "        precision=16,\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "def normalise(x:torch.Tensor):\n",
    "    std = torch.std(x)\n",
    "    std[std==0] = 1\n",
    "    return (x - torch.mean(x)) / std\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train(datasets:list, featureList:list, config:dict,\n",
    "          train_ratio:float=0.8, val_ratio:float=0.2, test_size:int=100,\n",
    "          EPOCHS:int=300, hidden_dim:int=256, loss_fn=torch.nn.functional.mse_loss, keep_prob=0.2,\n",
    "          n_gpu=1, n_samples=20, model_name=\"model\",\n",
    "          SEED:int=2021):\n",
    "    # process data\n",
    "    data = [func.processData(d, featureList) for d in datasets]\n",
    "    input_data = [np.vstack(d) for d in data]\n",
    "    x_tensors = [normalise(torch.from_numpy(x).float()) for x in input_data]\n",
    "    y_tensors = [torch.from_numpy(x).float() for x in input_data]\n",
    "\n",
    "    # prepare datasets\n",
    "    test_sets = [(x_tensor[-test_size:], y_tensor[-test_size:]) for x_tensor, y_tensor in zip(x_tensors, y_tensors)]\n",
    "    x_training = torch.vstack([x_tensor[:-test_size] for x_tensor in x_tensors])\n",
    "    y_training = torch.vstack([y_tensor[:-test_size] for y_tensor in y_tensors])\n",
    "    dataset = TensorDataset(x_training, y_training)\n",
    "    N = len(x_training)\n",
    "\n",
    "    train_ratio = int(train_ratio*N)\n",
    "    val_ratio = int(val_ratio*N)\n",
    "    print(\"Train: \", train_ratio, \", Validation: \", val_ratio)\n",
    "    train_set, val_set = random_split(dataset, [train_ratio, val_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    # define hyperparameters\n",
    "    input_dim = x_training.size()[1]\n",
    "    output_dim = input_dim\n",
    "\n",
    "    dimensions = [input_dim, hidden_dim]\n",
    "\n",
    "    scheduler = ASHAScheduler(max_t = EPOCHS, grace_period=1, reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"k\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"],\n",
    "        max_error_rows=5,\n",
    "        max_progress_rows=5,\n",
    "        max_report_frequency=10)\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_tune,\n",
    "            dimensions = dimensions,\n",
    "            loss_fn = loss_fn,\n",
    "            train_set = train_set, val_set = val_set, test_set=test_sets,\n",
    "            keep_prob = keep_prob,\n",
    "            num_epochs = EPOCHS,\n",
    "            num_gpus=n_gpu,\n",
    "            model_name=model_name\n",
    "        ),\n",
    "        resources_per_trial= {\"cpu\":1, \"gpu\":n_gpu},\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=n_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"MLP-MLP\",\n",
    "        verbose=False,\n",
    "        checkpoint_freq=0,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"loss\",\n",
    "        checkpoint_at_end=True\n",
    "    )\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    print(\"Best achieved loss was: \", analysis.best_result)\n",
    "    return test_sets\n",
    "\n",
    "def clean_checkpoints(num_keep=3, path=\"../../models\"):\n",
    "    for dir, dname, files in os.walk(path):\n",
    "        saved_checkpoints = []\n",
    "        for fname in files:\n",
    "            fname = fname.split(\".\")\n",
    "            saved_checkpoints.append(fname)\n",
    "        print(\"Num checkpoints in {}: {}\".format(dir, len(saved_checkpoints)))\n",
    "\n",
    "        saved_checkpoints.sort(key = lambda x: x[1])\n",
    "        for filename in saved_checkpoints[3:]:\n",
    "            os.remove(os.path.join(dir,\".\".join(filename)))\n",
    "\n",
    "def test(model:torch.nn.Module, test_sets:list, loss_fn, set_names=list,\n",
    "         save=True, path=\"../../results\", model_name=\"model\"):\n",
    "    # Intra test performance\n",
    "    df = {}\n",
    "    for i,t1 in enumerate(test_sets):\n",
    "        for j, t2 in enumerate(test_sets):\n",
    "            x = t1[0]\n",
    "            y = t2[1]\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(x, y)\n",
    "            df[\"{}-{}\".format(set_names[i], set_names[j])] = [loss.cpu().numpy()]\n",
    "            print(\"Test encoding {} to {}, MSE={:.2f}\".format(set_names[i], set_names[j], loss))\n",
    "    filepath = os.path.join(path, model_name)\n",
    "    if not os.path.exists(filepath): os.mkdir(filepath)\n",
    "    pd.DataFrame(df).to_csv(os.path.join(filepath, \"tests.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing pos, rotMat, velocity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Prepare train data\n",
    "data_path = \"../../data/\"\n",
    "\n",
    "# load data\n",
    "data_1 = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n",
    "data_3 = func.load(data_path+\"LOCO_R2-default-locomotion-large.pbz2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-30 21:44:21,066\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-03-30 21:44:25,230\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-03-30 21:44:29,297\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-03-30 21:44:32,867\tINFO ray_trial_executor.py:197 -- Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.\n",
      "2021-03-30 21:44:33,260\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-03-30 21:44:34,614\tWARNING function_runner.py:540 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0 | encoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 1 | decoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0.837     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0 | encoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 1 | decoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0.837     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0 | encoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 1 | decoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0.837     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0 | encoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 1 | decoder | Sequential | 104 K \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 209 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m 0.837     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0 | encoder | Sequential | 88.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 1 | decoder | Sequential | 88.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0.706     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0 | encoder | Sequential | 88.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 1 | decoder | Sequential | 88.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0.706     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0 | encoder | Sequential | 88.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 1 | decoder | Sequential | 88.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0.706     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0 | encoder | Sequential | 88.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 1 | decoder | Sequential | 88.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 176 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m 0.706     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0 | encoder | Sequential | 90.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 1 | decoder | Sequential | 90.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0.722     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0 | encoder | Sequential | 90.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 1 | decoder | Sequential | 90.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0.722     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0 | encoder | Sequential | 90.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 1 | decoder | Sequential | 90.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0.722     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0 | encoder | Sequential | 90.1 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 1 | decoder | Sequential | 90.4 K\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 180 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m 0.722     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0 | encoder | Sequential | 136 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1.096     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0 | encoder | Sequential | 136 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1.096     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0 | encoder | Sequential | 136 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1.096     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0 | encoder | Sequential | 136 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 273 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m 1.096     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0 | encoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 1 | decoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0.991     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0 | encoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 1 | decoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0.991     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0 | encoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 1 | decoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0.991     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0 | encoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 1 | decoder | Sequential | 123 K \n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 247 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m 0.991     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0.862     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0.862     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0.862     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 215 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m 0.862     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0 | encoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1 | decoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1.153     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0 | encoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1 | decoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1.153     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0 | encoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1 | decoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1.153     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0 | encoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1 | decoder | Sequential | 144 K \n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 288 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m 1.153     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0 | encoder | Sequential | 82.2 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 1 | decoder | Sequential | 82.5 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0.659     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0 | encoder | Sequential | 82.2 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 1 | decoder | Sequential | 82.5 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0.659     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0 | encoder | Sequential | 82.2 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 1 | decoder | Sequential | 82.5 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0.659     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0 | encoder | Sequential | 82.2 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 1 | decoder | Sequential | 82.5 K\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 164 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m 0.659     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1.147     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1.147     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1.147     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m 1.147     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0 | encoder | Sequential | 98.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 1 | decoder | Sequential | 98.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0.790     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0 | encoder | Sequential | 98.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 1 | decoder | Sequential | 98.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0.790     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0 | encoder | Sequential | 98.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 1 | decoder | Sequential | 98.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0.790     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0 | encoder | Sequential | 98.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 1 | decoder | Sequential | 98.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 197 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m 0.790     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1 | decoder | Sequential | 138 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1.104     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1 | decoder | Sequential | 138 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1.104     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1 | decoder | Sequential | 138 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1.104     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1 | decoder | Sequential | 138 K \n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m 1.104     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0 | encoder | Sequential | 93.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 1 | decoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0.751     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0 | encoder | Sequential | 93.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 1 | decoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0.751     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0 | encoder | Sequential | 93.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 1 | decoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0.751     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0 | encoder | Sequential | 93.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 1 | decoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 187 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m 0.751     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0 | encoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 1 | decoder | Sequential | 94.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0.753     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0 | encoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 1 | decoder | Sequential | 94.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0.753     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0 | encoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 1 | decoder | Sequential | 94.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0.753     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0 | encoder | Sequential | 94.0 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 1 | decoder | Sequential | 94.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 188 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m 0.753     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0.860     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0.860     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0.860     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0 | encoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 1 | decoder | Sequential | 107 K \n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 214 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m 0.860     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0 | encoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 1 | decoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0.811     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0 | encoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 1 | decoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0.811     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0 | encoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 1 | decoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0.811     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0 | encoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 1 | decoder | Sequential | 101 K \n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 202 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m 0.811     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1.145     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1.145     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1.145     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0 | encoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1 | decoder | Sequential | 143 K \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 286 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m 1.145     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0 | encoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 1 | decoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0.971     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0 | encoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 1 | decoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0.971     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0 | encoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 1 | decoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0.971     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0 | encoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 1 | decoder | Sequential | 121 K \n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 242 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m 0.971     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0 | encoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1 | decoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1.141     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0 | encoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1 | decoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1.141     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0 | encoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1 | decoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1.141     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0 | encoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1 | decoder | Sequential | 142 K \n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 285 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m 1.141     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0 | encoder | Sequential | 90.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 1 | decoder | Sequential | 90.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0.726     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0 | encoder | Sequential | 90.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 1 | decoder | Sequential | 90.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0.726     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0 | encoder | Sequential | 90.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 1 | decoder | Sequential | 90.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0.726     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0 | encoder | Sequential | 90.7 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 1 | decoder | Sequential | 90.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 181 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m 0.726     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0 | encoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1 | decoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1.161     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0 | encoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1 | decoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1.161     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0 | encoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1 | decoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1.161     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0 | encoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1 | decoder | Sequential | 145 K \n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 290 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m 1.161     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0 | encoder | Sequential | 87.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 1 | decoder | Sequential | 87.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0.702     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0 | encoder | Sequential | 87.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 1 | decoder | Sequential | 87.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0.702     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0 | encoder | Sequential | 87.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 1 | decoder | Sequential | 87.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0.702     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0 | encoder | Sequential | 87.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 1 | decoder | Sequential | 87.9 K\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 175 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m 0.702     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0 | encoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 1 | decoder | Sequential | 85.6 K\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m 0.683     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1.102     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1.102     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1.102     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0 | encoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1 | decoder | Sequential | 137 K \n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 275 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m 1.102     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0 | encoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1 | decoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1.053     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0 | encoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1 | decoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1.053     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0 | encoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1 | decoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1.053     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0 | encoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1 | decoder | Sequential | 131 K \n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 263 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m 1.053     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0 | encoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1 | decoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1.079     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0 | encoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1 | decoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1.079     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0 | encoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1 | decoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1.079     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0 | encoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1 | decoder | Sequential | 134 K \n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 269 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m 1.079     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0 | encoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 1 | decoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0.899     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0 | encoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 1 | decoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0.899     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0 | encoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 1 | decoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0.899     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0 | encoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 1 | decoder | Sequential | 112 K \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 224 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m 0.899     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0 | encoder | Sequential | 127 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1 | decoder | Sequential | 128 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1.024     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0 | encoder | Sequential | 127 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1 | decoder | Sequential | 128 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1.024     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0 | encoder | Sequential | 127 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1 | decoder | Sequential | 128 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1.024     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0 | encoder | Sequential | 127 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1 | decoder | Sequential | 128 K \n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 255 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m 1.024     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0 | encoder | Sequential | 85.0 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 1 | decoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0.681     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0 | encoder | Sequential | 85.0 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 1 | decoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0.681     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0 | encoder | Sequential | 85.0 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 1 | decoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0.681     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0 | encoder | Sequential | 85.0 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 1 | decoder | Sequential | 85.3 K\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 170 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m 0.681     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m TPU available: None, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m Using native 16bit precision.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0 | encoder | Sequential | 98.1 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 1 | decoder | Sequential | 98.4 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0.786     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0 | encoder | Sequential | 98.1 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 1 | decoder | Sequential | 98.4 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0.786     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0 | encoder | Sequential | 98.1 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 1 | decoder | Sequential | 98.4 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0.786     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   | Name    | Type       | Params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0 | encoder | Sequential | 98.1 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 1 | decoder | Sequential | 98.4 K\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m ---------------------------------------\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 196 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m 0.786     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m /home/nuoc/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=1903226)\u001B[0m   warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  3216 , Validation:  804\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 379.81it/s, loss=0.751, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 379.81it/s, loss=0.751, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 379.81it/s, loss=0.751, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 379.81it/s, loss=0.751, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 417.32it/s, loss=0.478, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 417.32it/s, loss=0.478, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 417.32it/s, loss=0.478, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 417.32it/s, loss=0.478, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 430.31it/s, loss=0.328, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 430.31it/s, loss=0.328, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 430.31it/s, loss=0.328, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 430.31it/s, loss=0.328, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 437.41it/s, loss=0.272, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 437.41it/s, loss=0.272, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 437.41it/s, loss=0.272, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 437.41it/s, loss=0.272, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 437.69it/s, loss=0.221, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 437.69it/s, loss=0.221, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 437.69it/s, loss=0.221, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 437.69it/s, loss=0.221, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 438.73it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.859]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 438.73it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.859]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 438.73it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.859]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 438.73it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.859]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 419.79it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 419.79it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 419.79it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 419.79it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.0907]           \n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 439.08it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 439.08it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 439.08it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 439.08it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 447.57it/s, loss=0.16, v_num=0.0, ptl/val_loss=0.0907] \n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 447.57it/s, loss=0.16, v_num=0.0, ptl/val_loss=0.0907] \n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 447.57it/s, loss=0.16, v_num=0.0, ptl/val_loss=0.0907] \n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 447.57it/s, loss=0.16, v_num=0.0, ptl/val_loss=0.0907] \n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 450.91it/s, loss=0.138, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 450.91it/s, loss=0.138, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 450.91it/s, loss=0.138, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 450.91it/s, loss=0.138, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 451.45it/s, loss=0.129, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 451.45it/s, loss=0.129, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 451.45it/s, loss=0.129, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 451.45it/s, loss=0.129, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.68it/s, loss=0.109, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.68it/s, loss=0.109, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.68it/s, loss=0.109, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.68it/s, loss=0.109, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 436.17it/s, loss=0.117, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 436.17it/s, loss=0.117, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 436.17it/s, loss=0.117, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 436.17it/s, loss=0.117, v_num=0.0, ptl/val_loss=0.0907]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 417.13it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 417.13it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 417.13it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 417.13it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]           \n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 433.82it/s, loss=0.107, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 433.82it/s, loss=0.107, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 433.82it/s, loss=0.107, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 433.82it/s, loss=0.107, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 442.64it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 442.64it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 442.64it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 442.64it/s, loss=0.103, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 447.33it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 447.33it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 447.33it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 447.33it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 449.36it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 449.36it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 449.36it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 449.36it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 447.94it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 447.94it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 447.94it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0342]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 447.94it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0342]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 425.67it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 425.67it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 425.67it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 425.67it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0751, v_num=0.0, ptl/val_loss=0.0203]           \n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 434.08it/s, loss=0.0805, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 434.08it/s, loss=0.0805, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 434.08it/s, loss=0.0805, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 434.08it/s, loss=0.0805, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 440.20it/s, loss=0.0772, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 440.20it/s, loss=0.0772, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 440.20it/s, loss=0.0772, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 440.20it/s, loss=0.0772, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 439.55it/s, loss=0.0685, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 439.55it/s, loss=0.0685, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 439.55it/s, loss=0.0685, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 439.55it/s, loss=0.0685, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 438.13it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0203] \n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 438.13it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0203] \n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 438.13it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0203] \n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 438.13it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0203] \n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 431.70it/s, loss=0.0674, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 431.70it/s, loss=0.0674, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 431.70it/s, loss=0.0674, v_num=0.0, ptl/val_loss=0.0203]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 431.70it/s, loss=0.0674, v_num=0.0, ptl/val_loss=0.0203]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 413.52it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 413.52it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 413.52it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 413.52it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0603, v_num=0.0, ptl/val_loss=0.0147]           \n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 399.52it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 399.52it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 399.52it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 399.52it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.31it/s, loss=0.0616, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.31it/s, loss=0.0616, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.31it/s, loss=0.0616, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.31it/s, loss=0.0616, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  36%|███▌      | 60/168 [00:00<00:00, 422.79it/s, loss=0.0565, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  36%|███▌      | 60/168 [00:00<00:00, 422.79it/s, loss=0.0565, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  36%|███▌      | 60/168 [00:00<00:00, 422.79it/s, loss=0.0565, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  36%|███▌      | 60/168 [00:00<00:00, 422.79it/s, loss=0.0565, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 427.43it/s, loss=0.0567, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 427.43it/s, loss=0.0567, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 427.43it/s, loss=0.0567, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 427.43it/s, loss=0.0567, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 420.56it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 420.56it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 420.56it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 420.56it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 413.66it/s, loss=0.0556, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 413.66it/s, loss=0.0556, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 413.66it/s, loss=0.0556, v_num=0.0, ptl/val_loss=0.0147]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 413.66it/s, loss=0.0556, v_num=0.0, ptl/val_loss=0.0147]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 386.62it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 386.62it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 386.62it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 386.62it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0517, v_num=0.0, ptl/val_loss=0.0129]           \n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 439.34it/s, loss=0.0535, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 439.34it/s, loss=0.0535, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 439.34it/s, loss=0.0535, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 439.34it/s, loss=0.0535, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.90it/s, loss=0.0531, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.90it/s, loss=0.0531, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.90it/s, loss=0.0531, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.90it/s, loss=0.0531, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.25it/s, loss=0.0485, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.25it/s, loss=0.0485, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.25it/s, loss=0.0485, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 416.25it/s, loss=0.0485, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 421.75it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 421.75it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 421.75it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 421.75it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.96it/s, loss=0.0439, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.96it/s, loss=0.0439, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.96it/s, loss=0.0439, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.96it/s, loss=0.0439, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 416.72it/s, loss=0.0472, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 416.72it/s, loss=0.0472, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 416.72it/s, loss=0.0472, v_num=0.0, ptl/val_loss=0.0129]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 416.72it/s, loss=0.0472, v_num=0.0, ptl/val_loss=0.0129]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 399.42it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 399.42it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 399.42it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 399.42it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0105]           \n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 403.72it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 403.72it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 403.72it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 403.72it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 419.78it/s, loss=0.0448, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 419.78it/s, loss=0.0448, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 419.78it/s, loss=0.0448, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 419.78it/s, loss=0.0448, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  36%|███▌      | 60/168 [00:00<00:00, 422.42it/s, loss=0.0425, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  36%|███▌      | 60/168 [00:00<00:00, 422.42it/s, loss=0.0425, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  36%|███▌      | 60/168 [00:00<00:00, 422.42it/s, loss=0.0425, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  36%|███▌      | 60/168 [00:00<00:00, 422.42it/s, loss=0.0425, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 425.95it/s, loss=0.0428, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 425.95it/s, loss=0.0428, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 425.95it/s, loss=0.0428, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 425.95it/s, loss=0.0428, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.41it/s, loss=0.0392, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.41it/s, loss=0.0392, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.41it/s, loss=0.0392, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.41it/s, loss=0.0392, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.17it/s, loss=0.0427, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.17it/s, loss=0.0427, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.17it/s, loss=0.0427, v_num=0.0, ptl/val_loss=0.0105]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 425.17it/s, loss=0.0427, v_num=0.0, ptl/val_loss=0.0105]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.61it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.61it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.61it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.61it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00987]           \n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 398.68it/s, loss=0.0414, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 398.68it/s, loss=0.0414, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 398.68it/s, loss=0.0414, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 398.68it/s, loss=0.0414, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 402.89it/s, loss=0.0399, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 402.89it/s, loss=0.0399, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 402.89it/s, loss=0.0399, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 402.89it/s, loss=0.0399, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  36%|███▌      | 60/168 [00:00<00:00, 409.08it/s, loss=0.0366, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  36%|███▌      | 60/168 [00:00<00:00, 409.08it/s, loss=0.0366, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  36%|███▌      | 60/168 [00:00<00:00, 409.08it/s, loss=0.0366, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  36%|███▌      | 60/168 [00:00<00:00, 409.08it/s, loss=0.0366, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 413.83it/s, loss=0.0371, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 413.83it/s, loss=0.0371, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 413.83it/s, loss=0.0371, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 413.83it/s, loss=0.0371, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 412.77it/s, loss=0.034, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 412.77it/s, loss=0.034, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 412.77it/s, loss=0.034, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 412.77it/s, loss=0.034, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 411.51it/s, loss=0.0386, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 411.51it/s, loss=0.0386, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 411.51it/s, loss=0.0386, v_num=0.0, ptl/val_loss=0.00987]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 411.51it/s, loss=0.0386, v_num=0.0, ptl/val_loss=0.00987]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 391.56it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]           \n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 391.56it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]           \n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 391.56it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]           \n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 391.56it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00766]           \n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 400.12it/s, loss=0.0376, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 400.12it/s, loss=0.0376, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 400.12it/s, loss=0.0376, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 400.12it/s, loss=0.0376, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 422.18it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 422.18it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 422.18it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 422.18it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 428.47it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 428.47it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 428.47it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 428.47it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 426.90it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 426.90it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 426.90it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 426.90it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 430.03it/s, loss=0.0337, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 430.03it/s, loss=0.0337, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 430.03it/s, loss=0.0337, v_num=0.0, ptl/val_loss=0.00766]\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 430.03it/s, loss=0.0337, v_num=0.0, ptl/val_loss=0.00766]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 411.62it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068] \n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 411.62it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068] \n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 411.62it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068] \n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 411.62it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068] \n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0068]           \n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 451.79it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 451.79it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 451.79it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 451.79it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 445.50it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 445.50it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 445.50it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 445.50it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 447.20it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 447.20it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 447.20it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 447.20it/s, loss=0.0306, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 446.24it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 446.24it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 446.24it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 446.24it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 444.77it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 444.77it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 444.77it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0068]\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 444.77it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0068]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 424.14it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 443.61it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 424.14it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 443.61it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 424.14it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 443.61it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 424.14it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 443.61it/s, loss=0.0304, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 440.25it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 440.25it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 440.25it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 440.25it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 442.61it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 442.61it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 442.61it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 442.61it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 443.25it/s, loss=0.0283, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 443.25it/s, loss=0.0283, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 443.25it/s, loss=0.0283, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 443.25it/s, loss=0.0283, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 444.96it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 444.96it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 444.96it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 444.96it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 441.99it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 441.99it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 441.99it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00623]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 441.99it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00623]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 420.63it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "                                                  \u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 420.63it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "                                                  \u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 420.63it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "                                                  \u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 420.63it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "                                                  \u001B[A\n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]           \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]           \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]           \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]           \n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 447.41it/s, loss=0.029, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 447.41it/s, loss=0.029, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 447.41it/s, loss=0.029, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 447.41it/s, loss=0.029, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 443.63it/s, loss=0.0286, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 443.63it/s, loss=0.0286, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 443.63it/s, loss=0.0286, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 443.63it/s, loss=0.0286, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 449.12it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 449.12it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 449.12it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 449.12it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 450.22it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00563] \n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 450.22it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00563] \n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 450.22it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00563] \n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 450.22it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00563] \n",
      "Epoch 11:  60%|█████▉    | 100/168 [00:00<00:00, 445.10it/s, loss=0.0248, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  60%|█████▉    | 100/168 [00:00<00:00, 445.10it/s, loss=0.0248, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  60%|█████▉    | 100/168 [00:00<00:00, 445.10it/s, loss=0.0248, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  60%|█████▉    | 100/168 [00:00<00:00, 445.10it/s, loss=0.0248, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 444.87it/s, loss=0.0273, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 444.87it/s, loss=0.0273, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 444.87it/s, loss=0.0273, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 444.87it/s, loss=0.0273, v_num=0.0, ptl/val_loss=0.00563]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 511.09it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]           \n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 511.09it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]           \n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 511.09it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]           \n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 511.09it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]           \n",
      "Epoch 12:  12%|█▏        | 20/168 [00:00<00:00, 459.10it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  12%|█▏        | 20/168 [00:00<00:00, 459.10it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  12%|█▏        | 20/168 [00:00<00:00, 459.10it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  12%|█▏        | 20/168 [00:00<00:00, 459.10it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  24%|██▍       | 40/168 [00:00<00:00, 460.29it/s, loss=0.0263, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  24%|██▍       | 40/168 [00:00<00:00, 460.29it/s, loss=0.0263, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  24%|██▍       | 40/168 [00:00<00:00, 460.29it/s, loss=0.0263, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  24%|██▍       | 40/168 [00:00<00:00, 460.29it/s, loss=0.0263, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  48%|████▊     | 80/168 [00:00<00:00, 452.99it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  48%|████▊     | 80/168 [00:00<00:00, 452.99it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  48%|████▊     | 80/168 [00:00<00:00, 452.99it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  48%|████▊     | 80/168 [00:00<00:00, 452.99it/s, loss=0.0245, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  60%|█████▉    | 100/168 [00:00<00:00, 452.59it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  60%|█████▉    | 100/168 [00:00<00:00, 452.59it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  60%|█████▉    | 100/168 [00:00<00:00, 452.59it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  60%|█████▉    | 100/168 [00:00<00:00, 452.59it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  71%|███████▏  | 120/168 [00:00<00:00, 450.37it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  71%|███████▏  | 120/168 [00:00<00:00, 450.37it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  71%|███████▏  | 120/168 [00:00<00:00, 450.37it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00631]\n",
      "Epoch 12:  71%|███████▏  | 120/168 [00:00<00:00, 450.37it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00631]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12: 100%|██████████| 168/168 [00:00<00:00, 422.53it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]           \n",
      "Epoch 12: 100%|██████████| 168/168 [00:00<00:00, 422.53it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]           \n",
      "Epoch 12: 100%|██████████| 168/168 [00:00<00:00, 422.53it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]           \n",
      "Epoch 12: 100%|██████████| 168/168 [00:00<00:00, 422.53it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]           \n",
      "Epoch 13:  12%|█▏        | 20/168 [00:00<00:00, 442.06it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  12%|█▏        | 20/168 [00:00<00:00, 442.06it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  12%|█▏        | 20/168 [00:00<00:00, 442.06it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  12%|█▏        | 20/168 [00:00<00:00, 442.06it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  24%|██▍       | 40/168 [00:00<00:00, 443.55it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  24%|██▍       | 40/168 [00:00<00:00, 443.55it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  24%|██▍       | 40/168 [00:00<00:00, 443.55it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  24%|██▍       | 40/168 [00:00<00:00, 443.55it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  36%|███▌      | 60/168 [00:00<00:00, 447.06it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  36%|███▌      | 60/168 [00:00<00:00, 447.06it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  36%|███▌      | 60/168 [00:00<00:00, 447.06it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  36%|███▌      | 60/168 [00:00<00:00, 447.06it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  48%|████▊     | 80/168 [00:00<00:00, 445.93it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  48%|████▊     | 80/168 [00:00<00:00, 445.93it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  48%|████▊     | 80/168 [00:00<00:00, 445.93it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  48%|████▊     | 80/168 [00:00<00:00, 445.93it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  60%|█████▉    | 100/168 [00:00<00:00, 448.61it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  60%|█████▉    | 100/168 [00:00<00:00, 448.61it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  60%|█████▉    | 100/168 [00:00<00:00, 448.61it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  60%|█████▉    | 100/168 [00:00<00:00, 448.61it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  71%|███████▏  | 120/168 [00:00<00:00, 449.49it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  71%|███████▏  | 120/168 [00:00<00:00, 449.49it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  71%|███████▏  | 120/168 [00:00<00:00, 449.49it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00472]\n",
      "Epoch 13:  71%|███████▏  | 120/168 [00:00<00:00, 449.49it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00472]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13: 100%|██████████| 168/168 [00:00<00:00, 427.20it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]           \n",
      "Epoch 13: 100%|██████████| 168/168 [00:00<00:00, 427.20it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]           \n",
      "Epoch 13: 100%|██████████| 168/168 [00:00<00:00, 427.20it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]           \n",
      "Epoch 13: 100%|██████████| 168/168 [00:00<00:00, 427.20it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0212, v_num=0.0, ptl/val_loss=0.00433]           \n",
      "Epoch 14:  24%|██▍       | 40/168 [00:00<00:00, 446.09it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  24%|██▍       | 40/168 [00:00<00:00, 446.09it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  24%|██▍       | 40/168 [00:00<00:00, 446.09it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  24%|██▍       | 40/168 [00:00<00:00, 446.09it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  36%|███▌      | 60/168 [00:00<00:00, 443.56it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  36%|███▌      | 60/168 [00:00<00:00, 443.56it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  36%|███▌      | 60/168 [00:00<00:00, 443.56it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  36%|███▌      | 60/168 [00:00<00:00, 443.56it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  48%|████▊     | 80/168 [00:00<00:00, 439.61it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  48%|████▊     | 80/168 [00:00<00:00, 439.61it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  48%|████▊     | 80/168 [00:00<00:00, 439.61it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  48%|████▊     | 80/168 [00:00<00:00, 439.61it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  60%|█████▉    | 100/168 [00:00<00:00, 439.17it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  60%|█████▉    | 100/168 [00:00<00:00, 439.17it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  60%|█████▉    | 100/168 [00:00<00:00, 439.17it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  60%|█████▉    | 100/168 [00:00<00:00, 439.17it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00433]\n",
      "Epoch 14:  71%|███████▏  | 120/168 [00:00<00:00, 440.66it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.00433] \n",
      "Epoch 14:  71%|███████▏  | 120/168 [00:00<00:00, 440.66it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.00433] \n",
      "Epoch 14:  71%|███████▏  | 120/168 [00:00<00:00, 440.66it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.00433] \n",
      "Epoch 14:  71%|███████▏  | 120/168 [00:00<00:00, 440.66it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.00433] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 168/168 [00:00<00:00, 416.18it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]           \n",
      "Epoch 14: 100%|██████████| 168/168 [00:00<00:00, 416.18it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]           \n",
      "Epoch 14: 100%|██████████| 168/168 [00:00<00:00, 416.18it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]           \n",
      "Epoch 14: 100%|██████████| 168/168 [00:00<00:00, 416.18it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0208, v_num=0.0, ptl/val_loss=0.00403]           \n",
      "Epoch 15:  24%|██▍       | 40/168 [00:00<00:00, 445.91it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  24%|██▍       | 40/168 [00:00<00:00, 445.91it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  24%|██▍       | 40/168 [00:00<00:00, 445.91it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  24%|██▍       | 40/168 [00:00<00:00, 445.91it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  36%|███▌      | 60/168 [00:00<00:00, 432.25it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  36%|███▌      | 60/168 [00:00<00:00, 432.25it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  36%|███▌      | 60/168 [00:00<00:00, 432.25it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  36%|███▌      | 60/168 [00:00<00:00, 432.25it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  48%|████▊     | 80/168 [00:00<00:00, 431.10it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  48%|████▊     | 80/168 [00:00<00:00, 431.10it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  48%|████▊     | 80/168 [00:00<00:00, 431.10it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  48%|████▊     | 80/168 [00:00<00:00, 431.10it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  60%|█████▉    | 100/168 [00:00<00:00, 433.41it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  60%|█████▉    | 100/168 [00:00<00:00, 433.41it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  60%|█████▉    | 100/168 [00:00<00:00, 433.41it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  60%|█████▉    | 100/168 [00:00<00:00, 433.41it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  71%|███████▏  | 120/168 [00:00<00:00, 435.66it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  71%|███████▏  | 120/168 [00:00<00:00, 435.66it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  71%|███████▏  | 120/168 [00:00<00:00, 435.66it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00403]\n",
      "Epoch 15:  71%|███████▏  | 120/168 [00:00<00:00, 435.66it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00403]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15: 100%|██████████| 168/168 [00:00<00:00, 413.84it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]           \n",
      "Epoch 15: 100%|██████████| 168/168 [00:00<00:00, 413.84it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]           \n",
      "Epoch 15: 100%|██████████| 168/168 [00:00<00:00, 413.84it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]           \n",
      "Epoch 15: 100%|██████████| 168/168 [00:00<00:00, 413.84it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0191, v_num=0.0, ptl/val_loss=0.00391]           \n",
      "Epoch 16:  24%|██▍       | 40/168 [00:00<00:00, 438.97it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  24%|██▍       | 40/168 [00:00<00:00, 438.97it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  24%|██▍       | 40/168 [00:00<00:00, 438.97it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  24%|██▍       | 40/168 [00:00<00:00, 438.97it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  36%|███▌      | 60/168 [00:00<00:00, 440.27it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  36%|███▌      | 60/168 [00:00<00:00, 440.27it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  36%|███▌      | 60/168 [00:00<00:00, 440.27it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  36%|███▌      | 60/168 [00:00<00:00, 440.27it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  48%|████▊     | 80/168 [00:00<00:00, 440.52it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  48%|████▊     | 80/168 [00:00<00:00, 440.52it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  48%|████▊     | 80/168 [00:00<00:00, 440.52it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  48%|████▊     | 80/168 [00:00<00:00, 440.52it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  60%|█████▉    | 100/168 [00:00<00:00, 442.69it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  60%|█████▉    | 100/168 [00:00<00:00, 442.69it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  60%|█████▉    | 100/168 [00:00<00:00, 442.69it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  60%|█████▉    | 100/168 [00:00<00:00, 442.69it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  71%|███████▏  | 120/168 [00:00<00:00, 443.09it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  71%|███████▏  | 120/168 [00:00<00:00, 443.09it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  71%|███████▏  | 120/168 [00:00<00:00, 443.09it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00391]\n",
      "Epoch 16:  71%|███████▏  | 120/168 [00:00<00:00, 443.09it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00391]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 16: 100%|██████████| 168/168 [00:00<00:00, 420.72it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  12%|█▏        | 20/168 [00:00<00:00, 452.81it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00359] \n",
      "Epoch 16: 100%|██████████| 168/168 [00:00<00:00, 420.72it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  12%|█▏        | 20/168 [00:00<00:00, 452.81it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00359] \n",
      "Epoch 16: 100%|██████████| 168/168 [00:00<00:00, 420.72it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  12%|█▏        | 20/168 [00:00<00:00, 452.81it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00359] \n",
      "Epoch 16: 100%|██████████| 168/168 [00:00<00:00, 420.72it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  12%|█▏        | 20/168 [00:00<00:00, 452.81it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00359] \n",
      "Epoch 17:  24%|██▍       | 40/168 [00:00<00:00, 451.32it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  24%|██▍       | 40/168 [00:00<00:00, 451.32it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  24%|██▍       | 40/168 [00:00<00:00, 451.32it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  24%|██▍       | 40/168 [00:00<00:00, 451.32it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  36%|███▌      | 60/168 [00:00<00:00, 419.92it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  36%|███▌      | 60/168 [00:00<00:00, 419.92it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  36%|███▌      | 60/168 [00:00<00:00, 419.92it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  36%|███▌      | 60/168 [00:00<00:00, 419.92it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  48%|████▊     | 80/168 [00:00<00:00, 418.24it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  48%|████▊     | 80/168 [00:00<00:00, 418.24it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  48%|████▊     | 80/168 [00:00<00:00, 418.24it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  48%|████▊     | 80/168 [00:00<00:00, 418.24it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  60%|█████▉    | 100/168 [00:00<00:00, 425.60it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  60%|█████▉    | 100/168 [00:00<00:00, 425.60it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  60%|█████▉    | 100/168 [00:00<00:00, 425.60it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  60%|█████▉    | 100/168 [00:00<00:00, 425.60it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  71%|███████▏  | 120/168 [00:00<00:00, 431.11it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  71%|███████▏  | 120/168 [00:00<00:00, 431.11it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  71%|███████▏  | 120/168 [00:00<00:00, 431.11it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00359]\n",
      "Epoch 17:  71%|███████▏  | 120/168 [00:00<00:00, 431.11it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00359]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 168/168 [00:00<00:00, 412.41it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]           \n",
      "Epoch 17: 100%|██████████| 168/168 [00:00<00:00, 412.41it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]           \n",
      "Epoch 17: 100%|██████████| 168/168 [00:00<00:00, 412.41it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]           \n",
      "Epoch 17: 100%|██████████| 168/168 [00:00<00:00, 412.41it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00334]           \n",
      "Epoch 18:  24%|██▍       | 40/168 [00:00<00:00, 456.63it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  24%|██▍       | 40/168 [00:00<00:00, 456.63it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  24%|██▍       | 40/168 [00:00<00:00, 456.63it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  24%|██▍       | 40/168 [00:00<00:00, 456.63it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  36%|███▌      | 60/168 [00:00<00:00, 459.58it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  36%|███▌      | 60/168 [00:00<00:00, 459.58it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  36%|███▌      | 60/168 [00:00<00:00, 459.58it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  36%|███▌      | 60/168 [00:00<00:00, 459.58it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  48%|████▊     | 80/168 [00:00<00:00, 458.63it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  48%|████▊     | 80/168 [00:00<00:00, 458.63it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  48%|████▊     | 80/168 [00:00<00:00, 458.63it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  48%|████▊     | 80/168 [00:00<00:00, 458.63it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  60%|█████▉    | 100/168 [00:00<00:00, 456.90it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  60%|█████▉    | 100/168 [00:00<00:00, 456.90it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  60%|█████▉    | 100/168 [00:00<00:00, 456.90it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  60%|█████▉    | 100/168 [00:00<00:00, 456.90it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Epoch 18:  71%|███████▏  | 120/168 [00:00<00:00, 458.14it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  71%|███████▏  | 120/168 [00:00<00:00, 458.14it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  71%|███████▏  | 120/168 [00:00<00:00, 458.14it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  71%|███████▏  | 120/168 [00:00<00:00, 458.14it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00334]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 168/168 [00:00<00:00, 523.48it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]           \n",
      "Epoch 18: 100%|██████████| 168/168 [00:00<00:00, 523.48it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]           \n",
      "Epoch 18: 100%|██████████| 168/168 [00:00<00:00, 523.48it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]           \n",
      "Epoch 18: 100%|██████████| 168/168 [00:00<00:00, 523.48it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00363]           \n",
      "Epoch 19:  12%|█▏        | 20/168 [00:00<00:00, 442.89it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  12%|█▏        | 20/168 [00:00<00:00, 442.89it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  12%|█▏        | 20/168 [00:00<00:00, 442.89it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  12%|█▏        | 20/168 [00:00<00:00, 442.89it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  24%|██▍       | 40/168 [00:00<00:00, 444.44it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  24%|██▍       | 40/168 [00:00<00:00, 444.44it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  24%|██▍       | 40/168 [00:00<00:00, 444.44it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  24%|██▍       | 40/168 [00:00<00:00, 444.44it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  36%|███▌      | 60/168 [00:00<00:00, 446.48it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  36%|███▌      | 60/168 [00:00<00:00, 446.48it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  36%|███▌      | 60/168 [00:00<00:00, 446.48it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  36%|███▌      | 60/168 [00:00<00:00, 446.48it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  48%|████▊     | 80/168 [00:00<00:00, 450.46it/s, loss=0.0166, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  48%|████▊     | 80/168 [00:00<00:00, 450.46it/s, loss=0.0166, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  48%|████▊     | 80/168 [00:00<00:00, 450.46it/s, loss=0.0166, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  48%|████▊     | 80/168 [00:00<00:00, 450.46it/s, loss=0.0166, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  60%|█████▉    | 100/168 [00:00<00:00, 450.76it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  60%|█████▉    | 100/168 [00:00<00:00, 450.76it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  60%|█████▉    | 100/168 [00:00<00:00, 450.76it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  60%|█████▉    | 100/168 [00:00<00:00, 450.76it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  71%|███████▏  | 120/168 [00:00<00:00, 447.33it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  71%|███████▏  | 120/168 [00:00<00:00, 447.33it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  71%|███████▏  | 120/168 [00:00<00:00, 447.33it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00363]\n",
      "Epoch 19:  71%|███████▏  | 120/168 [00:00<00:00, 447.33it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00363]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 168/168 [00:00<00:00, 513.84it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 168/168 [00:00<00:00, 513.84it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 168/168 [00:00<00:00, 513.84it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 168/168 [00:00<00:00, 513.84it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 19:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]           \n",
      "Epoch 20:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  12%|█▏        | 20/168 [00:00<00:00, 458.49it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  12%|█▏        | 20/168 [00:00<00:00, 458.49it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  12%|█▏        | 20/168 [00:00<00:00, 458.49it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  12%|█▏        | 20/168 [00:00<00:00, 458.49it/s, loss=0.0172, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  24%|██▍       | 40/168 [00:00<00:00, 455.58it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  24%|██▍       | 40/168 [00:00<00:00, 455.58it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  24%|██▍       | 40/168 [00:00<00:00, 455.58it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  24%|██▍       | 40/168 [00:00<00:00, 455.58it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  36%|███▌      | 60/168 [00:00<00:00, 449.38it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  36%|███▌      | 60/168 [00:00<00:00, 449.38it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  36%|███▌      | 60/168 [00:00<00:00, 449.38it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  36%|███▌      | 60/168 [00:00<00:00, 449.38it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  48%|████▊     | 80/168 [00:00<00:00, 452.11it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  48%|████▊     | 80/168 [00:00<00:00, 452.11it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  48%|████▊     | 80/168 [00:00<00:00, 452.11it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  48%|████▊     | 80/168 [00:00<00:00, 452.11it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  60%|█████▉    | 100/168 [00:00<00:00, 454.33it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  60%|█████▉    | 100/168 [00:00<00:00, 454.33it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  60%|█████▉    | 100/168 [00:00<00:00, 454.33it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  60%|█████▉    | 100/168 [00:00<00:00, 454.33it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Epoch 20:  71%|███████▏  | 120/168 [00:00<00:00, 454.96it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  71%|███████▏  | 120/168 [00:00<00:00, 454.96it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  71%|███████▏  | 120/168 [00:00<00:00, 454.96it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  71%|███████▏  | 120/168 [00:00<00:00, 454.96it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00381]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20: 100%|██████████| 168/168 [00:00<00:00, 428.62it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029] \n",
      "Epoch 21:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029]           \n",
      "Epoch 20: 100%|██████████| 168/168 [00:00<00:00, 428.62it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029] \n",
      "Epoch 21:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029]           \n",
      "Epoch 20: 100%|██████████| 168/168 [00:00<00:00, 428.62it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029] \n",
      "Epoch 21:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029]           \n",
      "Epoch 20: 100%|██████████| 168/168 [00:00<00:00, 428.62it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029] \n",
      "Epoch 21:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.0029]           \n",
      "Epoch 21:  12%|█▏        | 20/168 [00:00<00:00, 457.72it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  12%|█▏        | 20/168 [00:00<00:00, 457.72it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  12%|█▏        | 20/168 [00:00<00:00, 457.72it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  12%|█▏        | 20/168 [00:00<00:00, 457.72it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  24%|██▍       | 40/168 [00:00<00:00, 455.79it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  24%|██▍       | 40/168 [00:00<00:00, 455.79it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  24%|██▍       | 40/168 [00:00<00:00, 455.79it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  24%|██▍       | 40/168 [00:00<00:00, 455.79it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  36%|███▌      | 60/168 [00:00<00:00, 457.48it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  36%|███▌      | 60/168 [00:00<00:00, 457.48it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  36%|███▌      | 60/168 [00:00<00:00, 457.48it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  36%|███▌      | 60/168 [00:00<00:00, 457.48it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  48%|████▊     | 80/168 [00:00<00:00, 454.25it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  48%|████▊     | 80/168 [00:00<00:00, 454.25it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  48%|████▊     | 80/168 [00:00<00:00, 454.25it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  48%|████▊     | 80/168 [00:00<00:00, 454.25it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  71%|███████▏  | 120/168 [00:00<00:00, 454.59it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  71%|███████▏  | 120/168 [00:00<00:00, 454.59it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  71%|███████▏  | 120/168 [00:00<00:00, 454.59it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.0029]\n",
      "Epoch 21:  71%|███████▏  | 120/168 [00:00<00:00, 454.59it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.0029]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 168/168 [00:00<00:00, 428.82it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]           \n",
      "Epoch 21: 100%|██████████| 168/168 [00:00<00:00, 428.82it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]           \n",
      "Epoch 21: 100%|██████████| 168/168 [00:00<00:00, 428.82it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]           \n",
      "Epoch 21: 100%|██████████| 168/168 [00:00<00:00, 428.82it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00254]           \n",
      "Epoch 22:  12%|█▏        | 20/168 [00:00<00:00, 450.34it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  12%|█▏        | 20/168 [00:00<00:00, 450.34it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  12%|█▏        | 20/168 [00:00<00:00, 450.34it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  12%|█▏        | 20/168 [00:00<00:00, 450.34it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  24%|██▍       | 40/168 [00:00<00:00, 452.31it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  24%|██▍       | 40/168 [00:00<00:00, 452.31it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  24%|██▍       | 40/168 [00:00<00:00, 452.31it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  24%|██▍       | 40/168 [00:00<00:00, 452.31it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  48%|████▊     | 80/168 [00:00<00:00, 448.32it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  48%|████▊     | 80/168 [00:00<00:00, 448.32it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  48%|████▊     | 80/168 [00:00<00:00, 448.32it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  48%|████▊     | 80/168 [00:00<00:00, 448.32it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  60%|█████▉    | 100/168 [00:00<00:00, 452.81it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  60%|█████▉    | 100/168 [00:00<00:00, 452.81it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  60%|█████▉    | 100/168 [00:00<00:00, 452.81it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  60%|█████▉    | 100/168 [00:00<00:00, 452.81it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  71%|███████▏  | 120/168 [00:00<00:00, 454.31it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  71%|███████▏  | 120/168 [00:00<00:00, 454.31it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  71%|███████▏  | 120/168 [00:00<00:00, 454.31it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00254]\n",
      "Epoch 22:  71%|███████▏  | 120/168 [00:00<00:00, 454.31it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00254]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 22: 100%|██████████| 168/168 [00:00<00:00, 428.52it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]           \n",
      "Epoch 22: 100%|██████████| 168/168 [00:00<00:00, 428.52it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]           \n",
      "Epoch 22: 100%|██████████| 168/168 [00:00<00:00, 428.52it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]           \n",
      "Epoch 22: 100%|██████████| 168/168 [00:00<00:00, 428.52it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00234]           \n",
      "Epoch 23:  24%|██▍       | 40/168 [00:00<00:00, 463.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  24%|██▍       | 40/168 [00:00<00:00, 463.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  24%|██▍       | 40/168 [00:00<00:00, 463.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  24%|██▍       | 40/168 [00:00<00:00, 463.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  36%|███▌      | 60/168 [00:00<00:00, 467.11it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  36%|███▌      | 60/168 [00:00<00:00, 467.11it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  36%|███▌      | 60/168 [00:00<00:00, 467.11it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  36%|███▌      | 60/168 [00:00<00:00, 467.11it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  48%|████▊     | 80/168 [00:00<00:00, 462.52it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  48%|████▊     | 80/168 [00:00<00:00, 462.52it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  48%|████▊     | 80/168 [00:00<00:00, 462.52it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  48%|████▊     | 80/168 [00:00<00:00, 462.52it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  60%|█████▉    | 100/168 [00:00<00:00, 463.06it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  60%|█████▉    | 100/168 [00:00<00:00, 463.06it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  60%|█████▉    | 100/168 [00:00<00:00, 463.06it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  60%|█████▉    | 100/168 [00:00<00:00, 463.06it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 23:  71%|███████▏  | 120/168 [00:00<00:00, 463.06it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 23:  71%|███████▏  | 120/168 [00:00<00:00, 463.06it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 23:  71%|███████▏  | 120/168 [00:00<00:00, 463.06it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 23:  71%|███████▏  | 120/168 [00:00<00:00, 463.06it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00234] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 168/168 [00:00<00:00, 529.41it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 168/168 [00:00<00:00, 529.41it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 168/168 [00:00<00:00, 529.41it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 168/168 [00:00<00:00, 529.41it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00275]           \n",
      "Epoch 24:  12%|█▏        | 20/168 [00:00<00:00, 461.27it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  12%|█▏        | 20/168 [00:00<00:00, 461.27it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  12%|█▏        | 20/168 [00:00<00:00, 461.27it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  12%|█▏        | 20/168 [00:00<00:00, 461.27it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  24%|██▍       | 40/168 [00:00<00:00, 454.54it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  24%|██▍       | 40/168 [00:00<00:00, 454.54it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  24%|██▍       | 40/168 [00:00<00:00, 454.54it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  24%|██▍       | 40/168 [00:00<00:00, 454.54it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  48%|████▊     | 80/168 [00:00<00:00, 459.32it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  48%|████▊     | 80/168 [00:00<00:00, 459.32it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  48%|████▊     | 80/168 [00:00<00:00, 459.32it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  48%|████▊     | 80/168 [00:00<00:00, 459.32it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  60%|█████▉    | 100/168 [00:00<00:00, 458.68it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  60%|█████▉    | 100/168 [00:00<00:00, 458.68it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  60%|█████▉    | 100/168 [00:00<00:00, 458.68it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  60%|█████▉    | 100/168 [00:00<00:00, 458.68it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  71%|███████▏  | 120/168 [00:00<00:00, 454.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  71%|███████▏  | 120/168 [00:00<00:00, 454.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  71%|███████▏  | 120/168 [00:00<00:00, 454.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00275]\n",
      "Epoch 24:  71%|███████▏  | 120/168 [00:00<00:00, 454.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00275]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 168/168 [00:00<00:00, 516.50it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 168/168 [00:00<00:00, 516.50it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 168/168 [00:00<00:00, 516.50it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 168/168 [00:00<00:00, 516.50it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "                                                  \u001B[A\n",
      "Epoch 25:  12%|█▏        | 20/168 [00:00<00:00, 444.44it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00253] \n",
      "Epoch 25:  12%|█▏        | 20/168 [00:00<00:00, 444.44it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00253] \n",
      "Epoch 25:  12%|█▏        | 20/168 [00:00<00:00, 444.44it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00253] \n",
      "Epoch 25:  12%|█▏        | 20/168 [00:00<00:00, 444.44it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00253] \n",
      "Epoch 25:  24%|██▍       | 40/168 [00:00<00:00, 453.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  24%|██▍       | 40/168 [00:00<00:00, 453.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  24%|██▍       | 40/168 [00:00<00:00, 453.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  24%|██▍       | 40/168 [00:00<00:00, 453.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  36%|███▌      | 60/168 [00:00<00:00, 451.02it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  36%|███▌      | 60/168 [00:00<00:00, 451.02it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  36%|███▌      | 60/168 [00:00<00:00, 451.02it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  36%|███▌      | 60/168 [00:00<00:00, 451.02it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  48%|████▊     | 80/168 [00:00<00:00, 451.59it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  48%|████▊     | 80/168 [00:00<00:00, 451.59it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  48%|████▊     | 80/168 [00:00<00:00, 451.59it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  48%|████▊     | 80/168 [00:00<00:00, 451.59it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  60%|█████▉    | 100/168 [00:00<00:00, 454.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  60%|█████▉    | 100/168 [00:00<00:00, 454.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  60%|█████▉    | 100/168 [00:00<00:00, 454.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  60%|█████▉    | 100/168 [00:00<00:00, 454.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Epoch 25:  71%|███████▏  | 120/168 [00:00<00:00, 453.08it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  71%|███████▏  | 120/168 [00:00<00:00, 453.08it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  71%|███████▏  | 120/168 [00:00<00:00, 453.08it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  71%|███████▏  | 120/168 [00:00<00:00, 453.08it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00253]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25: 100%|██████████| 168/168 [00:00<00:00, 427.48it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]           \n",
      "Epoch 25: 100%|██████████| 168/168 [00:00<00:00, 427.48it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]           \n",
      "Epoch 25: 100%|██████████| 168/168 [00:00<00:00, 427.48it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]           \n",
      "Epoch 25: 100%|██████████| 168/168 [00:00<00:00, 427.48it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00221]           \n",
      "Epoch 26:  12%|█▏        | 20/168 [00:00<00:00, 421.15it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  12%|█▏        | 20/168 [00:00<00:00, 421.15it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  12%|█▏        | 20/168 [00:00<00:00, 421.15it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  12%|█▏        | 20/168 [00:00<00:00, 421.15it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  24%|██▍       | 40/168 [00:00<00:00, 435.67it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  24%|██▍       | 40/168 [00:00<00:00, 435.67it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  24%|██▍       | 40/168 [00:00<00:00, 435.67it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  24%|██▍       | 40/168 [00:00<00:00, 435.67it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  36%|███▌      | 60/168 [00:00<00:00, 437.86it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  36%|███▌      | 60/168 [00:00<00:00, 437.86it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  36%|███▌      | 60/168 [00:00<00:00, 437.86it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  36%|███▌      | 60/168 [00:00<00:00, 437.86it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  48%|████▊     | 80/168 [00:00<00:00, 433.85it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  48%|████▊     | 80/168 [00:00<00:00, 433.85it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  48%|████▊     | 80/168 [00:00<00:00, 433.85it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  48%|████▊     | 80/168 [00:00<00:00, 433.85it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  60%|█████▉    | 100/168 [00:00<00:00, 435.54it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  60%|█████▉    | 100/168 [00:00<00:00, 435.54it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  60%|█████▉    | 100/168 [00:00<00:00, 435.54it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  60%|█████▉    | 100/168 [00:00<00:00, 435.54it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Epoch 26:  71%|███████▏  | 120/168 [00:00<00:00, 436.63it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  71%|███████▏  | 120/168 [00:00<00:00, 436.63it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  71%|███████▏  | 120/168 [00:00<00:00, 436.63it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  71%|███████▏  | 120/168 [00:00<00:00, 436.63it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00221]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26: 100%|██████████| 168/168 [00:00<00:00, 502.69it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]           \n",
      "Epoch 26: 100%|██████████| 168/168 [00:00<00:00, 502.69it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]           \n",
      "Epoch 26: 100%|██████████| 168/168 [00:00<00:00, 502.69it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]           \n",
      "Epoch 26: 100%|██████████| 168/168 [00:00<00:00, 502.69it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00231]           \n",
      "Epoch 27:  12%|█▏        | 20/168 [00:00<00:00, 422.63it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  12%|█▏        | 20/168 [00:00<00:00, 422.63it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  12%|█▏        | 20/168 [00:00<00:00, 422.63it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  12%|█▏        | 20/168 [00:00<00:00, 422.63it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  24%|██▍       | 40/168 [00:00<00:00, 431.77it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  24%|██▍       | 40/168 [00:00<00:00, 431.77it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  24%|██▍       | 40/168 [00:00<00:00, 431.77it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  24%|██▍       | 40/168 [00:00<00:00, 431.77it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  48%|████▊     | 80/168 [00:00<00:00, 440.72it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  48%|████▊     | 80/168 [00:00<00:00, 440.72it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  48%|████▊     | 80/168 [00:00<00:00, 440.72it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  48%|████▊     | 80/168 [00:00<00:00, 440.72it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  60%|█████▉    | 100/168 [00:00<00:00, 438.08it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  60%|█████▉    | 100/168 [00:00<00:00, 438.08it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  60%|█████▉    | 100/168 [00:00<00:00, 438.08it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  60%|█████▉    | 100/168 [00:00<00:00, 438.08it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  71%|███████▏  | 120/168 [00:00<00:00, 438.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  71%|███████▏  | 120/168 [00:00<00:00, 438.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  71%|███████▏  | 120/168 [00:00<00:00, 438.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "Epoch 27:  71%|███████▏  | 120/168 [00:00<00:00, 438.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00231]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 168/168 [00:00<00:00, 501.93it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 168/168 [00:00<00:00, 501.93it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 168/168 [00:00<00:00, 501.93it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 168/168 [00:00<00:00, 501.93it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "                                                  \u001B[A\n",
      "Epoch 28:  12%|█▏        | 20/168 [00:00<00:00, 442.10it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00234]  \n",
      "Epoch 28:  12%|█▏        | 20/168 [00:00<00:00, 442.10it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00234]  \n",
      "Epoch 28:  12%|█▏        | 20/168 [00:00<00:00, 442.10it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00234]  \n",
      "Epoch 28:  12%|█▏        | 20/168 [00:00<00:00, 442.10it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00234]  \n",
      "Epoch 28:  24%|██▍       | 40/168 [00:00<00:00, 438.80it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  24%|██▍       | 40/168 [00:00<00:00, 438.80it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  24%|██▍       | 40/168 [00:00<00:00, 438.80it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  24%|██▍       | 40/168 [00:00<00:00, 438.80it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  36%|███▌      | 60/168 [00:00<00:00, 436.29it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 28:  36%|███▌      | 60/168 [00:00<00:00, 436.29it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 28:  36%|███▌      | 60/168 [00:00<00:00, 436.29it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 28:  36%|███▌      | 60/168 [00:00<00:00, 436.29it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00234] \n",
      "Epoch 28:  48%|████▊     | 80/168 [00:00<00:00, 384.91it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  48%|████▊     | 80/168 [00:00<00:00, 384.91it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  48%|████▊     | 80/168 [00:00<00:00, 384.91it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  48%|████▊     | 80/168 [00:00<00:00, 384.91it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  60%|█████▉    | 100/168 [00:00<00:00, 396.13it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  60%|█████▉    | 100/168 [00:00<00:00, 396.13it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  60%|█████▉    | 100/168 [00:00<00:00, 396.13it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  60%|█████▉    | 100/168 [00:00<00:00, 396.13it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  71%|███████▏  | 120/168 [00:00<00:00, 402.34it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  71%|███████▏  | 120/168 [00:00<00:00, 402.34it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  71%|███████▏  | 120/168 [00:00<00:00, 402.34it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00234]\n",
      "Epoch 28:  71%|███████▏  | 120/168 [00:00<00:00, 402.34it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00234]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28: 100%|██████████| 168/168 [00:00<00:00, 393.44it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]           \n",
      "Epoch 28: 100%|██████████| 168/168 [00:00<00:00, 393.44it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]           \n",
      "Epoch 28: 100%|██████████| 168/168 [00:00<00:00, 393.44it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]           \n",
      "Epoch 28: 100%|██████████| 168/168 [00:00<00:00, 393.44it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00207]           \n",
      "Epoch 29:  12%|█▏        | 20/168 [00:00<00:00, 429.51it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  12%|█▏        | 20/168 [00:00<00:00, 429.51it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  12%|█▏        | 20/168 [00:00<00:00, 429.51it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  12%|█▏        | 20/168 [00:00<00:00, 429.51it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  24%|██▍       | 40/168 [00:00<00:00, 431.06it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  24%|██▍       | 40/168 [00:00<00:00, 431.06it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  24%|██▍       | 40/168 [00:00<00:00, 431.06it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  24%|██▍       | 40/168 [00:00<00:00, 431.06it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  36%|███▌      | 60/168 [00:00<00:00, 437.34it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  36%|███▌      | 60/168 [00:00<00:00, 437.34it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  36%|███▌      | 60/168 [00:00<00:00, 437.34it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  36%|███▌      | 60/168 [00:00<00:00, 437.34it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  60%|█████▉    | 100/168 [00:00<00:00, 442.16it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  60%|█████▉    | 100/168 [00:00<00:00, 442.16it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  60%|█████▉    | 100/168 [00:00<00:00, 442.16it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  60%|█████▉    | 100/168 [00:00<00:00, 442.16it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00207]\n",
      "Epoch 29:  71%|███████▏  | 120/168 [00:00<00:00, 435.42it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  71%|███████▏  | 120/168 [00:00<00:00, 435.42it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  71%|███████▏  | 120/168 [00:00<00:00, 435.42it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "Epoch 29:  71%|███████▏  | 120/168 [00:00<00:00, 435.42it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00207] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 29: 100%|██████████| 168/168 [00:00<00:00, 501.05it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]           \n",
      "Epoch 29: 100%|██████████| 168/168 [00:00<00:00, 501.05it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]           \n",
      "Epoch 29: 100%|██████████| 168/168 [00:00<00:00, 501.05it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]           \n",
      "Epoch 29: 100%|██████████| 168/168 [00:00<00:00, 501.05it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]           \n",
      "Epoch 30:  12%|█▏        | 20/168 [00:00<00:00, 455.67it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  12%|█▏        | 20/168 [00:00<00:00, 455.67it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  12%|█▏        | 20/168 [00:00<00:00, 455.67it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  12%|█▏        | 20/168 [00:00<00:00, 455.67it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  24%|██▍       | 40/168 [00:00<00:00, 454.13it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  24%|██▍       | 40/168 [00:00<00:00, 454.13it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  24%|██▍       | 40/168 [00:00<00:00, 454.13it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  24%|██▍       | 40/168 [00:00<00:00, 454.13it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  36%|███▌      | 60/168 [00:00<00:00, 440.36it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  36%|███▌      | 60/168 [00:00<00:00, 440.36it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  36%|███▌      | 60/168 [00:00<00:00, 440.36it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  36%|███▌      | 60/168 [00:00<00:00, 440.36it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  48%|████▊     | 80/168 [00:00<00:00, 444.32it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  48%|████▊     | 80/168 [00:00<00:00, 444.32it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  48%|████▊     | 80/168 [00:00<00:00, 444.32it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  48%|████▊     | 80/168 [00:00<00:00, 444.32it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  60%|█████▉    | 100/168 [00:00<00:00, 445.35it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  60%|█████▉    | 100/168 [00:00<00:00, 445.35it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  60%|█████▉    | 100/168 [00:00<00:00, 445.35it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  60%|█████▉    | 100/168 [00:00<00:00, 445.35it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  71%|███████▏  | 120/168 [00:00<00:00, 448.16it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  71%|███████▏  | 120/168 [00:00<00:00, 448.16it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  71%|███████▏  | 120/168 [00:00<00:00, 448.16it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00283]\n",
      "Epoch 30:  71%|███████▏  | 120/168 [00:00<00:00, 448.16it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00283]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 30: 100%|██████████| 168/168 [00:00<00:00, 423.22it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]           \n",
      "Epoch 30: 100%|██████████| 168/168 [00:00<00:00, 423.22it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]           \n",
      "Epoch 30: 100%|██████████| 168/168 [00:00<00:00, 423.22it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]           \n",
      "Epoch 30: 100%|██████████| 168/168 [00:00<00:00, 423.22it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00172]           \n",
      "Epoch 31:  12%|█▏        | 20/168 [00:00<00:00, 451.25it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  12%|█▏        | 20/168 [00:00<00:00, 451.25it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  12%|█▏        | 20/168 [00:00<00:00, 451.25it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  12%|█▏        | 20/168 [00:00<00:00, 451.25it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  24%|██▍       | 40/168 [00:00<00:00, 454.95it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  24%|██▍       | 40/168 [00:00<00:00, 454.95it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  24%|██▍       | 40/168 [00:00<00:00, 454.95it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  24%|██▍       | 40/168 [00:00<00:00, 454.95it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  36%|███▌      | 60/168 [00:00<00:00, 449.43it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  36%|███▌      | 60/168 [00:00<00:00, 449.43it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  36%|███▌      | 60/168 [00:00<00:00, 449.43it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  36%|███▌      | 60/168 [00:00<00:00, 449.43it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  48%|████▊     | 80/168 [00:00<00:00, 443.93it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  48%|████▊     | 80/168 [00:00<00:00, 443.93it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  48%|████▊     | 80/168 [00:00<00:00, 443.93it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  48%|████▊     | 80/168 [00:00<00:00, 443.93it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  60%|█████▉    | 100/168 [00:00<00:00, 447.07it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  60%|█████▉    | 100/168 [00:00<00:00, 447.07it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  60%|█████▉    | 100/168 [00:00<00:00, 447.07it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  60%|█████▉    | 100/168 [00:00<00:00, 447.07it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Epoch 31:  71%|███████▏  | 120/168 [00:00<00:00, 449.06it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 31:  71%|███████▏  | 120/168 [00:00<00:00, 449.06it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 31:  71%|███████▏  | 120/168 [00:00<00:00, 449.06it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 31:  71%|███████▏  | 120/168 [00:00<00:00, 449.06it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00172]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 31: 100%|██████████| 168/168 [00:00<00:00, 515.64it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]           \n",
      "Epoch 31: 100%|██████████| 168/168 [00:00<00:00, 515.64it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]           \n",
      "Epoch 31: 100%|██████████| 168/168 [00:00<00:00, 515.64it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]           \n",
      "Epoch 31: 100%|██████████| 168/168 [00:00<00:00, 515.64it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00211]           \n",
      "Epoch 32:  12%|█▏        | 20/168 [00:00<00:00, 428.29it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  12%|█▏        | 20/168 [00:00<00:00, 428.29it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  12%|█▏        | 20/168 [00:00<00:00, 428.29it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  12%|█▏        | 20/168 [00:00<00:00, 428.29it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  24%|██▍       | 40/168 [00:00<00:00, 446.71it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  24%|██▍       | 40/168 [00:00<00:00, 446.71it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  24%|██▍       | 40/168 [00:00<00:00, 446.71it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  24%|██▍       | 40/168 [00:00<00:00, 446.71it/s, loss=0.0112, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  48%|████▊     | 80/168 [00:00<00:00, 456.65it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  48%|████▊     | 80/168 [00:00<00:00, 456.65it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  48%|████▊     | 80/168 [00:00<00:00, 456.65it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  48%|████▊     | 80/168 [00:00<00:00, 456.65it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  60%|█████▉    | 100/168 [00:00<00:00, 452.56it/s, loss=0.00978, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  60%|█████▉    | 100/168 [00:00<00:00, 452.56it/s, loss=0.00978, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  60%|█████▉    | 100/168 [00:00<00:00, 452.56it/s, loss=0.00978, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  60%|█████▉    | 100/168 [00:00<00:00, 452.56it/s, loss=0.00978, v_num=0.0, ptl/val_loss=0.00211]\n",
      "Epoch 32:  71%|███████▏  | 120/168 [00:00<00:00, 452.13it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211] \n",
      "Epoch 32:  71%|███████▏  | 120/168 [00:00<00:00, 452.13it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211] \n",
      "Epoch 32:  71%|███████▏  | 120/168 [00:00<00:00, 452.13it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211] \n",
      "Epoch 32:  71%|███████▏  | 120/168 [00:00<00:00, 452.13it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00211] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 32: 100%|██████████| 168/168 [00:00<00:00, 427.67it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 32: 100%|██████████| 168/168 [00:00<00:00, 427.67it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 32: 100%|██████████| 168/168 [00:00<00:00, 427.67it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 32: 100%|██████████| 168/168 [00:00<00:00, 427.67it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 33:  24%|██▍       | 40/168 [00:00<00:00, 441.19it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  24%|██▍       | 40/168 [00:00<00:00, 441.19it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  24%|██▍       | 40/168 [00:00<00:00, 441.19it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  24%|██▍       | 40/168 [00:00<00:00, 441.19it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  36%|███▌      | 60/168 [00:00<00:00, 448.84it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  36%|███▌      | 60/168 [00:00<00:00, 448.84it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  36%|███▌      | 60/168 [00:00<00:00, 448.84it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  36%|███▌      | 60/168 [00:00<00:00, 448.84it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  48%|████▊     | 80/168 [00:00<00:00, 452.91it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  48%|████▊     | 80/168 [00:00<00:00, 452.91it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  48%|████▊     | 80/168 [00:00<00:00, 452.91it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  48%|████▊     | 80/168 [00:00<00:00, 452.91it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  60%|█████▉    | 100/168 [00:00<00:00, 455.13it/s, loss=0.00909, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  60%|█████▉    | 100/168 [00:00<00:00, 455.13it/s, loss=0.00909, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  60%|█████▉    | 100/168 [00:00<00:00, 455.13it/s, loss=0.00909, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  60%|█████▉    | 100/168 [00:00<00:00, 455.13it/s, loss=0.00909, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 33:  71%|███████▏  | 120/168 [00:00<00:00, 454.83it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 33:  71%|███████▏  | 120/168 [00:00<00:00, 454.83it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 33:  71%|███████▏  | 120/168 [00:00<00:00, 454.83it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 33:  71%|███████▏  | 120/168 [00:00<00:00, 454.83it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00164] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|██████████| 168/168 [00:00<00:00, 519.03it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|██████████| 168/168 [00:00<00:00, 519.03it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|██████████| 168/168 [00:00<00:00, 519.03it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 33: 100%|██████████| 168/168 [00:00<00:00, 519.03it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00992, v_num=0.0, ptl/val_loss=0.0021]           \n",
      "Epoch 34:  12%|█▏        | 20/168 [00:00<00:00, 455.22it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  12%|█▏        | 20/168 [00:00<00:00, 455.22it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  12%|█▏        | 20/168 [00:00<00:00, 455.22it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  12%|█▏        | 20/168 [00:00<00:00, 455.22it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  24%|██▍       | 40/168 [00:00<00:00, 463.81it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  24%|██▍       | 40/168 [00:00<00:00, 463.81it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  24%|██▍       | 40/168 [00:00<00:00, 463.81it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  24%|██▍       | 40/168 [00:00<00:00, 463.81it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  36%|███▌      | 60/168 [00:00<00:00, 458.20it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  36%|███▌      | 60/168 [00:00<00:00, 458.20it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  36%|███▌      | 60/168 [00:00<00:00, 458.20it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  36%|███▌      | 60/168 [00:00<00:00, 458.20it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  60%|█████▉    | 100/168 [00:00<00:00, 461.16it/s, loss=0.00953, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  60%|█████▉    | 100/168 [00:00<00:00, 461.16it/s, loss=0.00953, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  60%|█████▉    | 100/168 [00:00<00:00, 461.16it/s, loss=0.00953, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  60%|█████▉    | 100/168 [00:00<00:00, 461.16it/s, loss=0.00953, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  71%|███████▏  | 120/168 [00:00<00:00, 461.48it/s, loss=0.00976, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  71%|███████▏  | 120/168 [00:00<00:00, 461.48it/s, loss=0.00976, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  71%|███████▏  | 120/168 [00:00<00:00, 461.48it/s, loss=0.00976, v_num=0.0, ptl/val_loss=0.0021]\n",
      "Epoch 34:  71%|███████▏  | 120/168 [00:00<00:00, 461.48it/s, loss=0.00976, v_num=0.0, ptl/val_loss=0.0021]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|██████████| 168/168 [00:00<00:00, 525.32it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|██████████| 168/168 [00:00<00:00, 525.32it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|██████████| 168/168 [00:00<00:00, 525.32it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 34: 100%|██████████| 168/168 [00:00<00:00, 525.32it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.0017]           \n",
      "Epoch 35:  12%|█▏        | 20/168 [00:00<00:00, 438.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  12%|█▏        | 20/168 [00:00<00:00, 438.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  12%|█▏        | 20/168 [00:00<00:00, 438.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  12%|█▏        | 20/168 [00:00<00:00, 438.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  24%|██▍       | 40/168 [00:00<00:00, 443.22it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  24%|██▍       | 40/168 [00:00<00:00, 443.22it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  24%|██▍       | 40/168 [00:00<00:00, 443.22it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  24%|██▍       | 40/168 [00:00<00:00, 443.22it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  36%|███▌      | 60/168 [00:00<00:00, 449.01it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  36%|███▌      | 60/168 [00:00<00:00, 449.01it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  36%|███▌      | 60/168 [00:00<00:00, 449.01it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  36%|███▌      | 60/168 [00:00<00:00, 449.01it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  48%|████▊     | 80/168 [00:00<00:00, 451.08it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  48%|████▊     | 80/168 [00:00<00:00, 451.08it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  48%|████▊     | 80/168 [00:00<00:00, 451.08it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  48%|████▊     | 80/168 [00:00<00:00, 451.08it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  60%|█████▉    | 100/168 [00:00<00:00, 451.46it/s, loss=0.00926, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  60%|█████▉    | 100/168 [00:00<00:00, 451.46it/s, loss=0.00926, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  60%|█████▉    | 100/168 [00:00<00:00, 451.46it/s, loss=0.00926, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  60%|█████▉    | 100/168 [00:00<00:00, 451.46it/s, loss=0.00926, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Epoch 35:  71%|███████▏  | 120/168 [00:00<00:00, 451.38it/s, loss=0.00995, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 35:  71%|███████▏  | 120/168 [00:00<00:00, 451.38it/s, loss=0.00995, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 35:  71%|███████▏  | 120/168 [00:00<00:00, 451.38it/s, loss=0.00995, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 35:  71%|███████▏  | 120/168 [00:00<00:00, 451.38it/s, loss=0.00995, v_num=0.0, ptl/val_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 35: 100%|██████████| 168/168 [00:00<00:00, 427.99it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 35: 100%|██████████| 168/168 [00:00<00:00, 427.99it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 35: 100%|██████████| 168/168 [00:00<00:00, 427.99it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 35: 100%|██████████| 168/168 [00:00<00:00, 427.99it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00927, v_num=0.0, ptl/val_loss=0.00164]           \n",
      "Epoch 36:  12%|█▏        | 20/168 [00:00<00:00, 450.43it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  12%|█▏        | 20/168 [00:00<00:00, 450.43it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  12%|█▏        | 20/168 [00:00<00:00, 450.43it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  12%|█▏        | 20/168 [00:00<00:00, 450.43it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  24%|██▍       | 40/168 [00:00<00:00, 450.83it/s, loss=0.00998, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  24%|██▍       | 40/168 [00:00<00:00, 450.83it/s, loss=0.00998, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  24%|██▍       | 40/168 [00:00<00:00, 450.83it/s, loss=0.00998, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  24%|██▍       | 40/168 [00:00<00:00, 450.83it/s, loss=0.00998, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  36%|███▌      | 60/168 [00:00<00:00, 455.47it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  36%|███▌      | 60/168 [00:00<00:00, 455.47it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  36%|███▌      | 60/168 [00:00<00:00, 455.47it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  36%|███▌      | 60/168 [00:00<00:00, 455.47it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  60%|█████▉    | 100/168 [00:00<00:00, 458.59it/s, loss=0.00863, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  60%|█████▉    | 100/168 [00:00<00:00, 458.59it/s, loss=0.00863, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  60%|█████▉    | 100/168 [00:00<00:00, 458.59it/s, loss=0.00863, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  60%|█████▉    | 100/168 [00:00<00:00, 458.59it/s, loss=0.00863, v_num=0.0, ptl/val_loss=0.00164]\n",
      "Epoch 36:  71%|███████▏  | 120/168 [00:00<00:00, 458.93it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 36:  71%|███████▏  | 120/168 [00:00<00:00, 458.93it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 36:  71%|███████▏  | 120/168 [00:00<00:00, 458.93it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164] \n",
      "Epoch 36:  71%|███████▏  | 120/168 [00:00<00:00, 458.93it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00164] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 36: 100%|██████████| 168/168 [00:00<00:00, 430.38it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]           \n",
      "Epoch 36: 100%|██████████| 168/168 [00:00<00:00, 430.38it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]           \n",
      "Epoch 36: 100%|██████████| 168/168 [00:00<00:00, 430.38it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]           \n",
      "Epoch 36: 100%|██████████| 168/168 [00:00<00:00, 430.38it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00923, v_num=0.0, ptl/val_loss=0.00147]           \n",
      "Epoch 37:  12%|█▏        | 20/168 [00:00<00:00, 457.73it/s, loss=0.00997, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  12%|█▏        | 20/168 [00:00<00:00, 457.73it/s, loss=0.00997, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  12%|█▏        | 20/168 [00:00<00:00, 457.73it/s, loss=0.00997, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  12%|█▏        | 20/168 [00:00<00:00, 457.73it/s, loss=0.00997, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  36%|███▌      | 60/168 [00:00<00:00, 454.91it/s, loss=0.00938, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  36%|███▌      | 60/168 [00:00<00:00, 454.91it/s, loss=0.00938, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  36%|███▌      | 60/168 [00:00<00:00, 454.91it/s, loss=0.00938, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  36%|███▌      | 60/168 [00:00<00:00, 454.91it/s, loss=0.00938, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  48%|████▊     | 80/168 [00:00<00:00, 457.80it/s, loss=0.0092, v_num=0.0, ptl/val_loss=0.00147] \n",
      "Epoch 37:  48%|████▊     | 80/168 [00:00<00:00, 457.80it/s, loss=0.0092, v_num=0.0, ptl/val_loss=0.00147] \n",
      "Epoch 37:  48%|████▊     | 80/168 [00:00<00:00, 457.80it/s, loss=0.0092, v_num=0.0, ptl/val_loss=0.00147] \n",
      "Epoch 37:  48%|████▊     | 80/168 [00:00<00:00, 457.80it/s, loss=0.0092, v_num=0.0, ptl/val_loss=0.00147] \n",
      "Epoch 37:  60%|█████▉    | 100/168 [00:00<00:00, 455.06it/s, loss=0.00849, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  60%|█████▉    | 100/168 [00:00<00:00, 455.06it/s, loss=0.00849, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  60%|█████▉    | 100/168 [00:00<00:00, 455.06it/s, loss=0.00849, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  60%|█████▉    | 100/168 [00:00<00:00, 455.06it/s, loss=0.00849, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  71%|███████▏  | 120/168 [00:00<00:00, 455.02it/s, loss=0.00971, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  71%|███████▏  | 120/168 [00:00<00:00, 455.02it/s, loss=0.00971, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  71%|███████▏  | 120/168 [00:00<00:00, 455.02it/s, loss=0.00971, v_num=0.0, ptl/val_loss=0.00147]\n",
      "Epoch 37:  71%|███████▏  | 120/168 [00:00<00:00, 455.02it/s, loss=0.00971, v_num=0.0, ptl/val_loss=0.00147]\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|██████████| 168/168 [00:00<00:00, 519.35it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|██████████| 168/168 [00:00<00:00, 519.35it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|██████████| 168/168 [00:00<00:00, 519.35it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]           \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 37: 100%|██████████| 168/168 [00:00<00:00, 519.35it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.00925, v_num=0.0, ptl/val_loss=0.00186]           \n",
      "Epoch 38:  12%|█▏        | 20/168 [00:00<00:00, 451.81it/s, loss=0.00965, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  12%|█▏        | 20/168 [00:00<00:00, 451.81it/s, loss=0.00965, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  12%|█▏        | 20/168 [00:00<00:00, 451.81it/s, loss=0.00965, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  12%|█▏        | 20/168 [00:00<00:00, 451.81it/s, loss=0.00965, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  24%|██▍       | 40/168 [00:00<00:00, 455.80it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  24%|██▍       | 40/168 [00:00<00:00, 455.80it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  24%|██▍       | 40/168 [00:00<00:00, 455.80it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  24%|██▍       | 40/168 [00:00<00:00, 455.80it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  36%|███▌      | 60/168 [00:00<00:00, 458.73it/s, loss=0.00882, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  36%|███▌      | 60/168 [00:00<00:00, 458.73it/s, loss=0.00882, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  36%|███▌      | 60/168 [00:00<00:00, 458.73it/s, loss=0.00882, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  36%|███▌      | 60/168 [00:00<00:00, 458.73it/s, loss=0.00882, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  60%|█████▉    | 100/168 [00:00<00:00, 455.96it/s, loss=0.00857, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  60%|█████▉    | 100/168 [00:00<00:00, 455.96it/s, loss=0.00857, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  60%|█████▉    | 100/168 [00:00<00:00, 455.96it/s, loss=0.00857, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  60%|█████▉    | 100/168 [00:00<00:00, 455.96it/s, loss=0.00857, v_num=0.0, ptl/val_loss=0.00186]\n",
      "Epoch 38:  71%|███████▏  | 120/168 [00:00<00:00, 457.12it/s, loss=0.0096, v_num=0.0, ptl/val_loss=0.00186] \n",
      "Epoch 38:  71%|███████▏  | 120/168 [00:00<00:00, 457.12it/s, loss=0.0096, v_num=0.0, ptl/val_loss=0.00186] \n",
      "Epoch 38:  71%|███████▏  | 120/168 [00:00<00:00, 457.12it/s, loss=0.0096, v_num=0.0, ptl/val_loss=0.00186] \n",
      "Epoch 38:  71%|███████▏  | 120/168 [00:00<00:00, 457.12it/s, loss=0.0096, v_num=0.0, ptl/val_loss=0.00186] \n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|██████████| 168/168 [00:00<00:00, 522.35it/s, loss=0.00842, v_num=0.0, ptl/val_loss=0.00155]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|██████████| 168/168 [00:00<00:00, 522.35it/s, loss=0.00842, v_num=0.0, ptl/val_loss=0.00155]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|██████████| 168/168 [00:00<00:00, 522.35it/s, loss=0.00842, v_num=0.0, ptl/val_loss=0.00155]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892880)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 38: 100%|██████████| 168/168 [00:00<00:00, 522.35it/s, loss=0.00842, v_num=0.0, ptl/val_loss=0.00155]\n",
      "                                                  \u001B[A\n",
      "Epoch 39:  12%|█▏        | 20/168 [00:00<00:00, 459.61it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00155] \n",
      "Epoch 39:  12%|█▏        | 20/168 [00:00<00:00, 459.61it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00155] \n",
      "Epoch 39:  12%|█▏        | 20/168 [00:00<00:00, 459.61it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00155] \n",
      "Epoch 39:  12%|█▏        | 20/168 [00:00<00:00, 459.61it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00155] \n",
      "Epoch 39:  24%|██▍       | 40/168 [00:00<00:00, 458.22it/s, loss=0.00947, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  24%|██▍       | 40/168 [00:00<00:00, 458.22it/s, loss=0.00947, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  24%|██▍       | 40/168 [00:00<00:00, 458.22it/s, loss=0.00947, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  24%|██▍       | 40/168 [00:00<00:00, 458.22it/s, loss=0.00947, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  36%|███▌      | 60/168 [00:00<00:00, 455.04it/s, loss=0.00884, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  36%|███▌      | 60/168 [00:00<00:00, 455.04it/s, loss=0.00884, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  36%|███▌      | 60/168 [00:00<00:00, 455.04it/s, loss=0.00884, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  36%|███▌      | 60/168 [00:00<00:00, 455.04it/s, loss=0.00884, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  48%|████▊     | 80/168 [00:00<00:00, 454.38it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  48%|████▊     | 80/168 [00:00<00:00, 454.38it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  48%|████▊     | 80/168 [00:00<00:00, 454.38it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  48%|████▊     | 80/168 [00:00<00:00, 454.38it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  60%|█████▉    | 100/168 [00:00<00:00, 455.43it/s, loss=0.00798, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  60%|█████▉    | 100/168 [00:00<00:00, 455.43it/s, loss=0.00798, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  60%|█████▉    | 100/168 [00:00<00:00, 455.43it/s, loss=0.00798, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  60%|█████▉    | 100/168 [00:00<00:00, 455.43it/s, loss=0.00798, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Epoch 39:  71%|███████▏  | 120/168 [00:00<00:00, 454.33it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 39:  71%|███████▏  | 120/168 [00:00<00:00, 454.33it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 39:  71%|███████▏  | 120/168 [00:00<00:00, 454.33it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 39:  71%|███████▏  | 120/168 [00:00<00:00, 454.33it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00155]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 520.09it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 513.04it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 520.09it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 513.04it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 520.09it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 513.04it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 520.09it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Epoch 39: 100%|██████████| 168/168 [00:00<00:00, 513.04it/s, loss=0.00877, v_num=0.0, ptl/val_loss=0.00154]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 416.96it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 416.96it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 416.96it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 416.96it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 435.13it/s, loss=0.711, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 435.13it/s, loss=0.711, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 435.13it/s, loss=0.711, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 435.13it/s, loss=0.711, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 438.89it/s, loss=0.614, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 438.89it/s, loss=0.614, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 438.89it/s, loss=0.614, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 438.89it/s, loss=0.614, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.85it/s, loss=0.647, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.85it/s, loss=0.647, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.85it/s, loss=0.647, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.85it/s, loss=0.647, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.23it/s, loss=0.587, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.23it/s, loss=0.587, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.23it/s, loss=0.587, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.23it/s, loss=0.587, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 445.60it/s, loss=0.555, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 445.60it/s, loss=0.555, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 445.60it/s, loss=0.555, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 445.60it/s, loss=0.555, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.85it/s, loss=0.524, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.85it/s, loss=0.524, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.85it/s, loss=0.524, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.85it/s, loss=0.524, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.26it/s, loss=0.469, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.26it/s, loss=0.469, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.26it/s, loss=0.469, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.26it/s, loss=0.469, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 445.99it/s, loss=0.401, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 445.99it/s, loss=0.401, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 445.99it/s, loss=0.401, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 445.99it/s, loss=0.401, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.27it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.27it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.27it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.27it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.69it/s, loss=0.411, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.69it/s, loss=0.411, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.69it/s, loss=0.411, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.69it/s, loss=0.411, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 443.40it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 443.40it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 443.40it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 443.40it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 514.49it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 514.49it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 514.49it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 514.49it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.05it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.05it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.05it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.05it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.607]\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892884)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.96it/s, loss=0.902, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.96it/s, loss=0.902, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.96it/s, loss=0.902, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.96it/s, loss=0.902, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.16it/s, loss=0.842, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.16it/s, loss=0.842, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.16it/s, loss=0.842, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.16it/s, loss=0.842, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 444.75it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 444.75it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 444.75it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 444.75it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.01it/s, loss=0.846, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.01it/s, loss=0.846, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.01it/s, loss=0.846, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 445.01it/s, loss=0.846, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 443.40it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 443.40it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 443.40it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 443.40it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.21it/s, loss=0.812, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.21it/s, loss=0.812, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.21it/s, loss=0.812, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.21it/s, loss=0.812, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 444.55it/s, loss=0.864, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 444.55it/s, loss=0.864, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 444.55it/s, loss=0.864, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 444.55it/s, loss=0.864, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 447.51it/s, loss=0.784, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 447.51it/s, loss=0.784, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 447.51it/s, loss=0.784, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 447.51it/s, loss=0.784, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.26it/s, loss=0.847, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.26it/s, loss=0.847, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.26it/s, loss=0.847, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.26it/s, loss=0.847, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 447.12it/s, loss=0.857, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 447.12it/s, loss=0.857, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 447.12it/s, loss=0.857, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 447.12it/s, loss=0.857, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.74it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.74it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.74it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.74it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 501.54it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 501.54it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 501.54it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 501.54it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 502.13it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 502.13it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 502.13it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 502.13it/s, loss=0.813, v_num=0.0, ptl/val_loss=0.676]\n",
      "\u001B[2m\u001B[36m(pid=1892885)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 417.52it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 417.52it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 417.52it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 417.52it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 427.53it/s, loss=0.879, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 427.53it/s, loss=0.879, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 427.53it/s, loss=0.879, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 427.53it/s, loss=0.879, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 431.52it/s, loss=0.854, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 431.52it/s, loss=0.854, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 431.52it/s, loss=0.854, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 431.52it/s, loss=0.854, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 435.25it/s, loss=0.834, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 435.25it/s, loss=0.834, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 435.25it/s, loss=0.834, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 435.25it/s, loss=0.834, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.18it/s, loss=0.809, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.18it/s, loss=0.809, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.18it/s, loss=0.809, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.18it/s, loss=0.809, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 437.03it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 437.03it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 437.03it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 437.03it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 417.70it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 417.70it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 417.70it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 417.70it/s, loss=0.848, v_num=0.0, ptl/val_loss=0.767]\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892886)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 433.64it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 433.64it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 433.64it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 433.64it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 442.12it/s, loss=1.08, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 442.12it/s, loss=1.08, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 442.12it/s, loss=1.08, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 442.12it/s, loss=1.08, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.09it/s, loss=0.901, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.09it/s, loss=0.901, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.09it/s, loss=0.901, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.09it/s, loss=0.901, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 448.05it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 448.05it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 448.05it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 448.05it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 446.17it/s, loss=0.732, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 446.17it/s, loss=0.732, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 446.17it/s, loss=0.732, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 446.17it/s, loss=0.732, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.83it/s, loss=0.67, v_num=0.0, ptl/val_loss=0.889] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.83it/s, loss=0.67, v_num=0.0, ptl/val_loss=0.889] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.83it/s, loss=0.67, v_num=0.0, ptl/val_loss=0.889] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.83it/s, loss=0.67, v_num=0.0, ptl/val_loss=0.889] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 446.52it/s, loss=0.691, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 446.52it/s, loss=0.691, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 446.52it/s, loss=0.691, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 446.52it/s, loss=0.691, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 447.65it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 447.65it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 447.65it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 447.65it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.23it/s, loss=0.557, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.23it/s, loss=0.557, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.23it/s, loss=0.557, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.23it/s, loss=0.557, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 445.39it/s, loss=0.583, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 445.39it/s, loss=0.583, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 445.39it/s, loss=0.583, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 445.39it/s, loss=0.583, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.07it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.07it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.07it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 446.07it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.40it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.40it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.40it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.40it/s, loss=0.511, v_num=0.0, ptl/val_loss=0.889]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 450.79it/s, loss=0.513, v_num=0.0, ptl/val_loss=0.352]\n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 450.79it/s, loss=0.513, v_num=0.0, ptl/val_loss=0.352]\n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 450.79it/s, loss=0.513, v_num=0.0, ptl/val_loss=0.352]\n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 450.79it/s, loss=0.513, v_num=0.0, ptl/val_loss=0.352]\n",
      "                                                  \u001B[A\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 442.78it/s, loss=0.489, v_num=0.0, ptl/val_loss=0.352] \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 442.78it/s, loss=0.489, v_num=0.0, ptl/val_loss=0.352] \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 442.78it/s, loss=0.489, v_num=0.0, ptl/val_loss=0.352] \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 442.78it/s, loss=0.489, v_num=0.0, ptl/val_loss=0.352] \n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 412.16it/s, loss=0.538, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 412.16it/s, loss=0.538, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 412.16it/s, loss=0.538, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 412.16it/s, loss=0.538, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 376.07it/s, loss=0.467, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 376.07it/s, loss=0.467, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 376.07it/s, loss=0.467, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 376.07it/s, loss=0.467, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 400.19it/s, loss=0.445, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 400.19it/s, loss=0.445, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 400.19it/s, loss=0.445, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 400.19it/s, loss=0.445, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 408.93it/s, loss=0.408, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 408.93it/s, loss=0.408, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 408.93it/s, loss=0.408, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 408.93it/s, loss=0.408, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 414.02it/s, loss=0.384, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 414.02it/s, loss=0.384, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 414.02it/s, loss=0.384, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 414.02it/s, loss=0.384, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 416.08it/s, loss=0.429, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 416.08it/s, loss=0.429, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 416.08it/s, loss=0.429, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 416.08it/s, loss=0.429, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 421.22it/s, loss=0.389, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 421.22it/s, loss=0.389, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 421.22it/s, loss=0.389, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 421.22it/s, loss=0.389, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 424.99it/s, loss=0.338, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 424.99it/s, loss=0.338, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 424.99it/s, loss=0.338, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 424.99it/s, loss=0.338, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 429.97it/s, loss=0.379, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 429.97it/s, loss=0.379, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 429.97it/s, loss=0.379, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 429.97it/s, loss=0.379, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 431.19it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 431.19it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 431.19it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 431.19it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 466.83it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 466.83it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 466.83it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 466.83it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 435.73it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 435.73it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 435.73it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 435.73it/s, loss=0.336, v_num=0.0, ptl/val_loss=0.352]\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892882)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 420.56it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 420.56it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 420.56it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 420.56it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 432.60it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 432.60it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 432.60it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 432.60it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.31it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.31it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.31it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.31it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 447.79it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 447.79it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 447.79it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 447.79it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.859]\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892876)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 440.30it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 440.30it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 440.30it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 440.30it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 454.16it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 454.16it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 454.16it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 454.16it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.74it/s, loss=1, v_num=0.0, ptl/val_loss=0.842]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.74it/s, loss=1, v_num=0.0, ptl/val_loss=0.842]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.74it/s, loss=1, v_num=0.0, ptl/val_loss=0.842]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.74it/s, loss=1, v_num=0.0, ptl/val_loss=0.842]   \n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.64it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.64it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.64it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.64it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.34it/s, loss=0.955, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.34it/s, loss=0.955, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.34it/s, loss=0.955, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.34it/s, loss=0.955, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 448.55it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 448.55it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 448.55it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 448.55it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 408.15it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 408.15it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 408.15it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 408.15it/s, loss=1.01, v_num=0.0, ptl/val_loss=0.842]\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892878)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 445.00it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 445.00it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 445.00it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 445.00it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 453.76it/s, loss=1.26, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 453.76it/s, loss=1.26, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 453.76it/s, loss=1.26, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 453.76it/s, loss=1.26, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 453.90it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 453.90it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 453.90it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 453.90it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.41it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.41it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.41it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.41it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 459.10it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 459.10it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 459.10it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 459.10it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 459.24it/s, loss=0.976, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 459.24it/s, loss=0.976, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 459.24it/s, loss=0.976, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 459.24it/s, loss=0.976, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 457.67it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 457.67it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 457.67it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 457.67it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 459.66it/s, loss=0.988, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 459.66it/s, loss=0.988, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 459.66it/s, loss=0.988, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 459.66it/s, loss=0.988, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 460.08it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 460.08it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 460.08it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 460.08it/s, loss=0.886, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 458.19it/s, loss=0.983, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 458.19it/s, loss=0.983, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 458.19it/s, loss=0.983, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 458.19it/s, loss=0.983, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 457.60it/s, loss=0.96, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 457.60it/s, loss=0.96, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 457.60it/s, loss=0.96, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 457.60it/s, loss=0.96, v_num=0.0, ptl/val_loss=0.890] \n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 455.53it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 455.53it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 455.53it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 455.53it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 529.36it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 529.36it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 529.36it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 529.36it/s, loss=0.889, v_num=0.0, ptl/val_loss=0.890]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 456.15it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 456.15it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 456.15it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 456.15it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.667]           \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 446.15it/s, loss=0.86, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 446.15it/s, loss=0.86, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 446.15it/s, loss=0.86, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 446.15it/s, loss=0.86, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 448.08it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 448.08it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 448.08it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 448.08it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 442.64it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 442.64it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 442.64it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 442.64it/s, loss=0.896, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 447.64it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 447.64it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 447.64it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 447.64it/s, loss=0.839, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 451.79it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 451.79it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 451.79it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 451.79it/s, loss=0.798, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 452.41it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 452.41it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 452.41it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 452.41it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 450.21it/s, loss=0.827, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 450.21it/s, loss=0.827, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 450.21it/s, loss=0.827, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 450.21it/s, loss=0.827, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 451.16it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 451.16it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 451.16it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 451.16it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 452.75it/s, loss=0.78, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 452.75it/s, loss=0.78, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 452.75it/s, loss=0.78, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 452.75it/s, loss=0.78, v_num=0.0, ptl/val_loss=0.667] \n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 453.53it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 453.53it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 453.53it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 453.53it/s, loss=0.777, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.55it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.55it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.55it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.55it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 491.84it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 491.84it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 491.84it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 491.84it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 453.57it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 453.57it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 453.57it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 453.57it/s, loss=0.721, v_num=0.0, ptl/val_loss=0.667]\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892881)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 414.65it/s, loss=0.671, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 414.65it/s, loss=0.671, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 414.65it/s, loss=0.671, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 414.65it/s, loss=0.671, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 433.62it/s, loss=0.655, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 433.62it/s, loss=0.655, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 433.62it/s, loss=0.655, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 433.62it/s, loss=0.655, v_num=0.0, ptl/val_loss=0.592]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.50it/s, loss=0.646, v_num=0.0, ptl/val_loss=0.592]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.50it/s, loss=0.646, v_num=0.0, ptl/val_loss=0.592]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.50it/s, loss=0.646, v_num=0.0, ptl/val_loss=0.592]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 436.50it/s, loss=0.646, v_num=0.0, ptl/val_loss=0.592]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 490.54it/s, loss=0.65, v_num=0.0, ptl/val_loss=0.605] \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 490.54it/s, loss=0.65, v_num=0.0, ptl/val_loss=0.605] \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 490.54it/s, loss=0.65, v_num=0.0, ptl/val_loss=0.605] \n",
      "                                                  \u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 490.54it/s, loss=0.65, v_num=0.0, ptl/val_loss=0.605] \n",
      "                                                  \u001B[A\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 425.01it/s, loss=0.665, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 425.01it/s, loss=0.665, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 425.01it/s, loss=0.665, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 425.01it/s, loss=0.665, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 425.97it/s, loss=0.649, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 425.97it/s, loss=0.649, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 425.97it/s, loss=0.649, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 425.97it/s, loss=0.649, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 426.16it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 426.16it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 426.16it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 426.16it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 455.20it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 455.20it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 455.20it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 455.20it/s, loss=0.639, v_num=0.0, ptl/val_loss=0.605]\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892879)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 412.19it/s, loss=0.581, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 412.19it/s, loss=0.581, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 412.19it/s, loss=0.581, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 412.19it/s, loss=0.581, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 422.82it/s, loss=0.246, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 422.82it/s, loss=0.246, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 422.82it/s, loss=0.246, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 422.82it/s, loss=0.246, v_num=0.0, ptl/val_loss=0.905]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 428.74it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.905]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 428.74it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.905]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 428.74it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.905]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 428.74it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.905]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 300.90it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 300.90it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 300.90it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 300.90it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0453]          \n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 331.94it/s, loss=0.128, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 331.94it/s, loss=0.128, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 331.94it/s, loss=0.128, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 331.94it/s, loss=0.128, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 369.15it/s, loss=0.1, v_num=0.0, ptl/val_loss=0.0453]  \n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 369.15it/s, loss=0.1, v_num=0.0, ptl/val_loss=0.0453]  \n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 369.15it/s, loss=0.1, v_num=0.0, ptl/val_loss=0.0453]  \n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 369.15it/s, loss=0.1, v_num=0.0, ptl/val_loss=0.0453]  \n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 381.33it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 381.33it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 381.33it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 381.33it/s, loss=0.0868, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 294.19it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]\n",
      "                                                  \u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 294.19it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]\n",
      "                                                  \u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 294.19it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]\n",
      "                                                  \u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 294.19it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]\n",
      "                                                  \u001B[A\n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]          \n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]          \n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]          \n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0836, v_num=0.0, ptl/val_loss=0.0187]          \n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 429.30it/s, loss=0.0686, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 429.30it/s, loss=0.0686, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 429.30it/s, loss=0.0686, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 429.30it/s, loss=0.0686, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 426.48it/s, loss=0.0622, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 426.48it/s, loss=0.0622, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 426.48it/s, loss=0.0622, v_num=0.0, ptl/val_loss=0.0187]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 426.48it/s, loss=0.0622, v_num=0.0, ptl/val_loss=0.0187]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 312.81it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 312.81it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 312.81it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 312.81it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0125]          \n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 418.83it/s, loss=0.0596, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 418.83it/s, loss=0.0596, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 418.83it/s, loss=0.0596, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 418.83it/s, loss=0.0596, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 420.86it/s, loss=0.0539, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 420.86it/s, loss=0.0539, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 420.86it/s, loss=0.0539, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 420.86it/s, loss=0.0539, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 418.51it/s, loss=0.0495, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 418.51it/s, loss=0.0495, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 418.51it/s, loss=0.0495, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 418.51it/s, loss=0.0495, v_num=0.0, ptl/val_loss=0.0125]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 310.97it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 310.97it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 310.97it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 310.97it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0482, v_num=0.0, ptl/val_loss=0.00999]          \n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 429.35it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 429.35it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 429.35it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 429.35it/s, loss=0.0481, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 426.88it/s, loss=0.0438, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 426.88it/s, loss=0.0438, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 426.88it/s, loss=0.0438, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 426.88it/s, loss=0.0438, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 424.78it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 424.78it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 424.78it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.00999]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 424.78it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.00999]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 313.59it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 313.59it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 313.59it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 313.59it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00757]          \n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 406.87it/s, loss=0.0415, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 406.87it/s, loss=0.0415, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 406.87it/s, loss=0.0415, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 406.87it/s, loss=0.0415, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.08it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.08it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.08it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.08it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.08it/s, loss=0.0359, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.08it/s, loss=0.0359, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.08it/s, loss=0.0359, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.08it/s, loss=0.0359, v_num=0.0, ptl/val_loss=0.00757]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 309.74it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 309.74it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 309.74it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 309.74it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0356, v_num=0.0, ptl/val_loss=0.00639]          \n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 411.63it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 411.63it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 411.63it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 411.63it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 417.50it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 417.50it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 417.50it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 417.50it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 417.19it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 417.19it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 417.19it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 417.19it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00639]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 470.50it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 470.50it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 470.50it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 470.50it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00688]          \n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 401.98it/s, loss=0.0323, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 401.98it/s, loss=0.0323, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 401.98it/s, loss=0.0323, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 401.98it/s, loss=0.0323, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 404.39it/s, loss=0.0287, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 404.39it/s, loss=0.0287, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 404.39it/s, loss=0.0287, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 404.39it/s, loss=0.0287, v_num=0.0, ptl/val_loss=0.00688]\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 410.08it/s, loss=0.028, v_num=0.0, ptl/val_loss=0.00688] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 410.08it/s, loss=0.028, v_num=0.0, ptl/val_loss=0.00688] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 410.08it/s, loss=0.028, v_num=0.0, ptl/val_loss=0.00688] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 410.08it/s, loss=0.028, v_num=0.0, ptl/val_loss=0.00688] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 307.27it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]          \n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 307.27it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]          \n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 307.27it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]          \n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 307.27it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00519]          \n",
      "Epoch 8:  24%|██▍       | 20/84 [00:00<00:00, 409.78it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  24%|██▍       | 20/84 [00:00<00:00, 409.78it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  24%|██▍       | 20/84 [00:00<00:00, 409.78it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  24%|██▍       | 20/84 [00:00<00:00, 409.78it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  48%|████▊     | 40/84 [00:00<00:00, 413.64it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  48%|████▊     | 40/84 [00:00<00:00, 413.64it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  48%|████▊     | 40/84 [00:00<00:00, 413.64it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  48%|████▊     | 40/84 [00:00<00:00, 413.64it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Epoch 8:  71%|███████▏  | 60/84 [00:00<00:00, 420.71it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 60/84 [00:00<00:00, 420.71it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 60/84 [00:00<00:00, 420.71it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 60/84 [00:00<00:00, 420.71it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00519]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 84/84 [00:00<00:00, 313.08it/s, loss=0.0257, v_num=0.0, ptl/val_loss=0.00467]\n",
      "                                                  \u001B[A\n",
      "Epoch 8: 100%|██████████| 84/84 [00:00<00:00, 313.08it/s, loss=0.0257, v_num=0.0, ptl/val_loss=0.00467]\n",
      "                                                  \u001B[A\n",
      "Epoch 8: 100%|██████████| 84/84 [00:00<00:00, 313.08it/s, loss=0.0257, v_num=0.0, ptl/val_loss=0.00467]\n",
      "                                                  \u001B[A\n",
      "Epoch 8: 100%|██████████| 84/84 [00:00<00:00, 313.08it/s, loss=0.0257, v_num=0.0, ptl/val_loss=0.00467]\n",
      "                                                  \u001B[A\n",
      "Epoch 9:  24%|██▍       | 20/84 [00:00<00:00, 433.38it/s, loss=0.0258, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  24%|██▍       | 20/84 [00:00<00:00, 433.38it/s, loss=0.0258, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  24%|██▍       | 20/84 [00:00<00:00, 433.38it/s, loss=0.0258, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  24%|██▍       | 20/84 [00:00<00:00, 433.38it/s, loss=0.0258, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  48%|████▊     | 40/84 [00:00<00:00, 431.74it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  48%|████▊     | 40/84 [00:00<00:00, 431.74it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  48%|████▊     | 40/84 [00:00<00:00, 431.74it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  48%|████▊     | 40/84 [00:00<00:00, 431.74it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  71%|███████▏  | 60/84 [00:00<00:00, 428.52it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  71%|███████▏  | 60/84 [00:00<00:00, 428.52it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  71%|███████▏  | 60/84 [00:00<00:00, 428.52it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00467]\n",
      "Epoch 9:  71%|███████▏  | 60/84 [00:00<00:00, 428.52it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00467]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 84/84 [00:00<00:00, 313.00it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]         \n",
      "Epoch 9: 100%|██████████| 84/84 [00:00<00:00, 313.00it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]         \n",
      "Epoch 9: 100%|██████████| 84/84 [00:00<00:00, 313.00it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]         \n",
      "Epoch 9: 100%|██████████| 84/84 [00:00<00:00, 313.00it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.00464]         \n",
      "Epoch 10:  24%|██▍       | 20/84 [00:00<00:00, 422.88it/s, loss=0.0242, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  24%|██▍       | 20/84 [00:00<00:00, 422.88it/s, loss=0.0242, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  24%|██▍       | 20/84 [00:00<00:00, 422.88it/s, loss=0.0242, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  24%|██▍       | 20/84 [00:00<00:00, 422.88it/s, loss=0.0242, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  48%|████▊     | 40/84 [00:00<00:00, 425.57it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  48%|████▊     | 40/84 [00:00<00:00, 425.57it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  48%|████▊     | 40/84 [00:00<00:00, 425.57it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  48%|████▊     | 40/84 [00:00<00:00, 425.57it/s, loss=0.0228, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Epoch 10:  71%|███████▏  | 60/84 [00:00<00:00, 424.12it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  71%|███████▏  | 60/84 [00:00<00:00, 424.12it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  71%|███████▏  | 60/84 [00:00<00:00, 424.12it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  71%|███████▏  | 60/84 [00:00<00:00, 424.12it/s, loss=0.0219, v_num=0.0, ptl/val_loss=0.00464]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 84/84 [00:00<00:00, 313.75it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]          \n",
      "Epoch 10: 100%|██████████| 84/84 [00:00<00:00, 313.75it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]          \n",
      "Epoch 10: 100%|██████████| 84/84 [00:00<00:00, 313.75it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]          \n",
      "Epoch 10: 100%|██████████| 84/84 [00:00<00:00, 313.75it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00329]          \n",
      "Epoch 11:  24%|██▍       | 20/84 [00:00<00:00, 422.12it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  24%|██▍       | 20/84 [00:00<00:00, 422.12it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  24%|██▍       | 20/84 [00:00<00:00, 422.12it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  24%|██▍       | 20/84 [00:00<00:00, 422.12it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  48%|████▊     | 40/84 [00:00<00:00, 426.16it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  48%|████▊     | 40/84 [00:00<00:00, 426.16it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  48%|████▊     | 40/84 [00:00<00:00, 426.16it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  48%|████▊     | 40/84 [00:00<00:00, 426.16it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Epoch 11:  71%|███████▏  | 60/84 [00:00<00:00, 427.25it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 60/84 [00:00<00:00, 427.25it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 60/84 [00:00<00:00, 427.25it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  71%|███████▏  | 60/84 [00:00<00:00, 427.25it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00329]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11: 100%|██████████| 84/84 [00:00<00:00, 477.69it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]          \n",
      "Epoch 11: 100%|██████████| 84/84 [00:00<00:00, 477.69it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]          \n",
      "Epoch 11: 100%|██████████| 84/84 [00:00<00:00, 477.69it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]          \n",
      "Epoch 11: 100%|██████████| 84/84 [00:00<00:00, 477.69it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00345]          \n",
      "Epoch 12:  24%|██▍       | 20/84 [00:00<00:00, 423.58it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  24%|██▍       | 20/84 [00:00<00:00, 423.58it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  24%|██▍       | 20/84 [00:00<00:00, 423.58it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  24%|██▍       | 20/84 [00:00<00:00, 423.58it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  48%|████▊     | 40/84 [00:00<00:00, 426.40it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  48%|████▊     | 40/84 [00:00<00:00, 426.40it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  48%|████▊     | 40/84 [00:00<00:00, 426.40it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  48%|████▊     | 40/84 [00:00<00:00, 426.40it/s, loss=0.0193, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Epoch 12:  71%|███████▏  | 60/84 [00:00<00:00, 426.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  71%|███████▏  | 60/84 [00:00<00:00, 426.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  71%|███████▏  | 60/84 [00:00<00:00, 426.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  71%|███████▏  | 60/84 [00:00<00:00, 426.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00345]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12: 100%|██████████| 84/84 [00:00<00:00, 317.02it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]          \n",
      "Epoch 12: 100%|██████████| 84/84 [00:00<00:00, 317.02it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]          \n",
      "Epoch 12: 100%|██████████| 84/84 [00:00<00:00, 317.02it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]          \n",
      "Epoch 12: 100%|██████████| 84/84 [00:00<00:00, 317.02it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00301]          \n",
      "Epoch 13:  24%|██▍       | 20/84 [00:00<00:00, 423.52it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:  24%|██▍       | 20/84 [00:00<00:00, 423.52it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:  24%|██▍       | 20/84 [00:00<00:00, 423.52it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:  24%|██▍       | 20/84 [00:00<00:00, 423.52it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Epoch 13:  48%|████▊     | 40/84 [00:00<00:00, 425.76it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00301] \n",
      "Epoch 13:  48%|████▊     | 40/84 [00:00<00:00, 425.76it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00301] \n",
      "Epoch 13:  48%|████▊     | 40/84 [00:00<00:00, 425.76it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00301] \n",
      "Epoch 13:  48%|████▊     | 40/84 [00:00<00:00, 425.76it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00301] \n",
      "Epoch 13:  71%|███████▏  | 60/84 [00:00<00:00, 420.27it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  71%|███████▏  | 60/84 [00:00<00:00, 420.27it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  71%|███████▏  | 60/84 [00:00<00:00, 420.27it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  71%|███████▏  | 60/84 [00:00<00:00, 420.27it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00301]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13: 100%|██████████| 84/84 [00:00<00:00, 476.52it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]          \n",
      "Epoch 13: 100%|██████████| 84/84 [00:00<00:00, 476.52it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]          \n",
      "Epoch 13: 100%|██████████| 84/84 [00:00<00:00, 476.52it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]          \n",
      "Epoch 13: 100%|██████████| 84/84 [00:00<00:00, 476.52it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00339]          \n",
      "Epoch 14:  24%|██▍       | 20/84 [00:00<00:00, 440.08it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:  24%|██▍       | 20/84 [00:00<00:00, 440.08it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:  24%|██▍       | 20/84 [00:00<00:00, 440.08it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:  24%|██▍       | 20/84 [00:00<00:00, 440.08it/s, loss=0.0187, v_num=0.0, ptl/val_loss=0.00339]\n",
      "Epoch 14:  71%|███████▏  | 60/84 [00:00<00:00, 424.35it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00339] \n",
      "Epoch 14:  71%|███████▏  | 60/84 [00:00<00:00, 424.35it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00339] \n",
      "Epoch 14:  71%|███████▏  | 60/84 [00:00<00:00, 424.35it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00339] \n",
      "Epoch 14:  71%|███████▏  | 60/84 [00:00<00:00, 424.35it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00339] \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 84/84 [00:00<00:00, 482.31it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 84/84 [00:00<00:00, 482.31it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 84/84 [00:00<00:00, 482.31it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 84/84 [00:00<00:00, 482.31it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376]          \n",
      "Epoch 15:  24%|██▍       | 20/84 [00:00<00:00, 426.90it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  24%|██▍       | 20/84 [00:00<00:00, 426.90it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  24%|██▍       | 20/84 [00:00<00:00, 426.90it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  24%|██▍       | 20/84 [00:00<00:00, 426.90it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  48%|████▊     | 40/84 [00:00<00:00, 429.20it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  48%|████▊     | 40/84 [00:00<00:00, 429.20it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  48%|████▊     | 40/84 [00:00<00:00, 429.20it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  48%|████▊     | 40/84 [00:00<00:00, 429.20it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00376]\n",
      "Epoch 15:  71%|███████▏  | 60/84 [00:00<00:00, 422.77it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  71%|███████▏  | 60/84 [00:00<00:00, 422.77it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  71%|███████▏  | 60/84 [00:00<00:00, 422.77it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  71%|███████▏  | 60/84 [00:00<00:00, 422.77it/s, loss=0.017, v_num=0.0, ptl/val_loss=0.00376] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15: 100%|██████████| 84/84 [00:00<00:00, 311.32it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]\n",
      "                                                  \u001B[A\n",
      "Epoch 15: 100%|██████████| 84/84 [00:00<00:00, 311.32it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]\n",
      "                                                  \u001B[A\n",
      "Epoch 15: 100%|██████████| 84/84 [00:00<00:00, 311.32it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]\n",
      "                                                  \u001B[A\n",
      "Epoch 15: 100%|██████████| 84/84 [00:00<00:00, 311.32it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]\n",
      "                                                  \u001B[A\n",
      "Epoch 16:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]          \n",
      "Epoch 16:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]          \n",
      "Epoch 16:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]          \n",
      "Epoch 16:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00263]          \n",
      "Epoch 16:  48%|████▊     | 40/84 [00:00<00:00, 421.25it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  48%|████▊     | 40/84 [00:00<00:00, 421.25it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  48%|████▊     | 40/84 [00:00<00:00, 421.25it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  48%|████▊     | 40/84 [00:00<00:00, 421.25it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  71%|███████▏  | 60/84 [00:00<00:00, 422.92it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  71%|███████▏  | 60/84 [00:00<00:00, 422.92it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  71%|███████▏  | 60/84 [00:00<00:00, 422.92it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 16:  71%|███████▏  | 60/84 [00:00<00:00, 422.92it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00263]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 16: 100%|██████████| 84/84 [00:00<00:00, 312.32it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]          \n",
      "Epoch 16: 100%|██████████| 84/84 [00:00<00:00, 312.32it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]          \n",
      "Epoch 16: 100%|██████████| 84/84 [00:00<00:00, 312.32it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]          \n",
      "Epoch 16: 100%|██████████| 84/84 [00:00<00:00, 312.32it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00233]          \n",
      "Epoch 17:  24%|██▍       | 20/84 [00:00<00:00, 412.93it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  24%|██▍       | 20/84 [00:00<00:00, 412.93it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  24%|██▍       | 20/84 [00:00<00:00, 412.93it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  24%|██▍       | 20/84 [00:00<00:00, 412.93it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  48%|████▊     | 40/84 [00:00<00:00, 421.09it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  48%|████▊     | 40/84 [00:00<00:00, 421.09it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  48%|████▊     | 40/84 [00:00<00:00, 421.09it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  48%|████▊     | 40/84 [00:00<00:00, 421.09it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Epoch 17:  71%|███████▏  | 60/84 [00:00<00:00, 422.36it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 84/84 [00:00<00:00, 477.10it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00259]\n",
      "                                                  \u001B[A\n",
      "Epoch 17:  71%|███████▏  | 60/84 [00:00<00:00, 422.36it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 84/84 [00:00<00:00, 477.10it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00259]\n",
      "                                                  \u001B[A\n",
      "Epoch 17:  71%|███████▏  | 60/84 [00:00<00:00, 422.36it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 84/84 [00:00<00:00, 477.10it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00259]\n",
      "                                                  \u001B[A\n",
      "Epoch 17:  71%|███████▏  | 60/84 [00:00<00:00, 422.36it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00233]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 84/84 [00:00<00:00, 477.10it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00259]\n",
      "                                                  \u001B[A\n",
      "Epoch 18:  24%|██▍       | 20/84 [00:00<00:00, 424.17it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  24%|██▍       | 20/84 [00:00<00:00, 424.17it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  24%|██▍       | 20/84 [00:00<00:00, 424.17it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  24%|██▍       | 20/84 [00:00<00:00, 424.17it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  48%|████▊     | 40/84 [00:00<00:00, 430.00it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  48%|████▊     | 40/84 [00:00<00:00, 430.00it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  48%|████▊     | 40/84 [00:00<00:00, 430.00it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  48%|████▊     | 40/84 [00:00<00:00, 430.00it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  71%|███████▏  | 60/84 [00:00<00:00, 429.49it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  71%|███████▏  | 60/84 [00:00<00:00, 429.49it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  71%|███████▏  | 60/84 [00:00<00:00, 429.49it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 18:  71%|███████▏  | 60/84 [00:00<00:00, 429.49it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00259]\n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 84/84 [00:00<00:00, 480.41it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 84/84 [00:00<00:00, 480.41it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 84/84 [00:00<00:00, 480.41it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]          \n",
      "\u001B[2m\u001B[36m(pid=1892883)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 84/84 [00:00<00:00, 480.41it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00259]          \n",
      "Epoch 19:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  48%|████▊     | 40/84 [00:00<00:00, 413.97it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  48%|████▊     | 40/84 [00:00<00:00, 413.97it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  48%|████▊     | 40/84 [00:00<00:00, 413.97it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  48%|████▊     | 40/84 [00:00<00:00, 413.97it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Epoch 19:  71%|███████▏  | 60/84 [00:00<00:00, 416.70it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  71%|███████▏  | 60/84 [00:00<00:00, 416.70it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  71%|███████▏  | 60/84 [00:00<00:00, 416.70it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  71%|███████▏  | 60/84 [00:00<00:00, 416.70it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00259]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 469.17it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276] \n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 454.77it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276]\n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 469.17it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276] \n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 454.77it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276]\n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 469.17it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276] \n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 454.77it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276]\n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 469.17it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276] \n",
      "Epoch 19: 100%|██████████| 84/84 [00:00<00:00, 454.77it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00276]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 431.28it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 431.28it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 431.28it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 431.28it/s, loss=1.1, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 439.77it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 439.77it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 439.77it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 439.77it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 440.15it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 440.15it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 440.15it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 440.15it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 439.89it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 439.89it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 439.89it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 439.89it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 443.76it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 443.76it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 443.76it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 443.76it/s, loss=1.03, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 441.03it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 441.03it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 441.03it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 441.03it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 487.82it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 487.82it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 487.82it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 487.82it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.865]\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892877)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 446.91it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 446.91it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 446.91it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 446.91it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.41it/s, loss=1.33, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.41it/s, loss=1.33, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.41it/s, loss=1.33, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.41it/s, loss=1.33, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.55it/s, loss=1.27, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.55it/s, loss=1.27, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.55it/s, loss=1.27, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 456.55it/s, loss=1.27, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 457.53it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 457.53it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 457.53it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 457.53it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 455.04it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 455.04it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 455.04it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 455.04it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 456.51it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 456.51it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 456.51it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 456.51it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 453.56it/s, loss=1.16, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 453.56it/s, loss=1.16, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 453.56it/s, loss=1.16, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 453.56it/s, loss=1.16, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 453.65it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 453.65it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 453.65it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 453.65it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 453.70it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 453.70it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 453.70it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 453.70it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.98it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.98it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.98it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.98it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 524.91it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 524.91it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 524.91it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 524.91it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 449.69it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 449.69it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 449.69it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 449.69it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.906]\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892875)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 430.99it/s, loss=0.981, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 430.99it/s, loss=0.981, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 430.99it/s, loss=0.981, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 430.99it/s, loss=0.981, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 440.72it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 440.72it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 440.72it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 440.72it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 443.37it/s, loss=0.938, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 443.37it/s, loss=0.938, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 443.37it/s, loss=0.938, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 443.37it/s, loss=0.938, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 424.38it/s, loss=0.923, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 424.38it/s, loss=0.923, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 424.38it/s, loss=0.923, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 424.38it/s, loss=0.923, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 427.29it/s, loss=0.878, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 427.29it/s, loss=0.878, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 427.29it/s, loss=0.878, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 427.29it/s, loss=0.878, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 430.85it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 430.85it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 430.85it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 430.85it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 405.75it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 405.75it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 405.75it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 405.75it/s, loss=0.932, v_num=0.0, ptl/val_loss=0.814]\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892874)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 428.66it/s, loss=0.855, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 428.66it/s, loss=0.855, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 428.66it/s, loss=0.855, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 428.66it/s, loss=0.855, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.72it/s, loss=0.837, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.72it/s, loss=0.837, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.72it/s, loss=0.837, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.72it/s, loss=0.837, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.33it/s, loss=0.697, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.33it/s, loss=0.697, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.33it/s, loss=0.697, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.33it/s, loss=0.697, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.55it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.55it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.55it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 447.55it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 449.28it/s, loss=0.635, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 449.28it/s, loss=0.635, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 449.28it/s, loss=0.635, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 449.28it/s, loss=0.635, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 444.23it/s, loss=0.588, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 444.23it/s, loss=0.588, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 444.23it/s, loss=0.588, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 444.23it/s, loss=0.588, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.59it/s, loss=0.531, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.59it/s, loss=0.531, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.59it/s, loss=0.531, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 444.59it/s, loss=0.531, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.12it/s, loss=0.5, v_num=0.0, ptl/val_loss=0.730]  \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.12it/s, loss=0.5, v_num=0.0, ptl/val_loss=0.730]  \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.12it/s, loss=0.5, v_num=0.0, ptl/val_loss=0.730]  \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 445.12it/s, loss=0.5, v_num=0.0, ptl/val_loss=0.730]  \n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.12it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.12it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.12it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 448.12it/s, loss=0.428, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 448.52it/s, loss=0.46, v_num=0.0, ptl/val_loss=0.730] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 448.52it/s, loss=0.46, v_num=0.0, ptl/val_loss=0.730] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 448.52it/s, loss=0.46, v_num=0.0, ptl/val_loss=0.730] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 448.52it/s, loss=0.46, v_num=0.0, ptl/val_loss=0.730] \n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 446.45it/s, loss=0.439, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 446.45it/s, loss=0.439, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 446.45it/s, loss=0.439, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 446.45it/s, loss=0.439, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 444.70it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 444.70it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 444.70it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 444.70it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 499.21it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 499.21it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 499.21it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0:  96%|█████████▌| 320/335 [00:00<00:00, 499.21it/s, loss=0.377, v_num=0.0, ptl/val_loss=0.730]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.39it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.39it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.39it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 462.39it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.275]           \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 422.32it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 422.32it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 422.32it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 422.32it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 432.27it/s, loss=0.403, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 432.27it/s, loss=0.403, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 432.27it/s, loss=0.403, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 432.27it/s, loss=0.403, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 437.57it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275] \n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 437.57it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275] \n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 437.57it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275] \n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 437.57it/s, loss=0.36, v_num=0.0, ptl/val_loss=0.275] \n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 439.53it/s, loss=0.323, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 439.53it/s, loss=0.323, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 439.53it/s, loss=0.323, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 439.53it/s, loss=0.323, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 440.25it/s, loss=0.296, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 440.25it/s, loss=0.296, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 440.25it/s, loss=0.296, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 440.25it/s, loss=0.296, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 438.85it/s, loss=0.284, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 438.85it/s, loss=0.284, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 438.85it/s, loss=0.284, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 438.85it/s, loss=0.284, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 439.38it/s, loss=0.316, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 439.38it/s, loss=0.316, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 439.38it/s, loss=0.316, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 439.38it/s, loss=0.316, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 439.43it/s, loss=0.289, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 439.43it/s, loss=0.289, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 439.43it/s, loss=0.289, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 439.43it/s, loss=0.289, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.64it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.64it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.64it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.64it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.69it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.69it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.69it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.69it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.26it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.26it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.26it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.26it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.12it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.12it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.12it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.12it/s, loss=0.287, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 439.54it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 439.54it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 439.54it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 439.54it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 511.43it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 511.43it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 511.43it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 511.43it/s, loss=0.249, v_num=0.0, ptl/val_loss=0.275]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 463.77it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 463.77it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 463.77it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 463.77it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.252, v_num=0.0, ptl/val_loss=0.167]           \n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 437.85it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 437.85it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 437.85it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 437.85it/s, loss=0.251, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 440.47it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 440.47it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 440.47it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 440.47it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 432.91it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 432.91it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 432.91it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 432.91it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 438.46it/s, loss=0.24, v_num=0.0, ptl/val_loss=0.167] \n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 438.46it/s, loss=0.24, v_num=0.0, ptl/val_loss=0.167] \n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 438.46it/s, loss=0.24, v_num=0.0, ptl/val_loss=0.167] \n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 438.46it/s, loss=0.24, v_num=0.0, ptl/val_loss=0.167] \n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 438.34it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 438.34it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 438.34it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 438.34it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.03it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.03it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.03it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.03it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 440.39it/s, loss=0.237, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 440.39it/s, loss=0.237, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 440.39it/s, loss=0.237, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 440.39it/s, loss=0.237, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 440.33it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 440.33it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 440.33it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 440.33it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.37it/s, loss=0.216, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.37it/s, loss=0.216, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.37it/s, loss=0.216, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.37it/s, loss=0.216, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 439.11it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 439.11it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 439.11it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 439.11it/s, loss=0.215, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.60it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.25it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.60it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.25it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.60it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.25it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.60it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.25it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.167]\n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 461.03it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 461.03it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 461.03it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 461.03it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]           \n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 433.95it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 433.95it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 433.95it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 433.95it/s, loss=0.189, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 439.87it/s, loss=0.224, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 439.87it/s, loss=0.224, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 439.87it/s, loss=0.224, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 439.87it/s, loss=0.224, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 441.02it/s, loss=0.192, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 441.02it/s, loss=0.192, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 441.02it/s, loss=0.192, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 441.02it/s, loss=0.192, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 441.48it/s, loss=0.199, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 441.48it/s, loss=0.199, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 441.48it/s, loss=0.199, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 441.48it/s, loss=0.199, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 442.70it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 442.70it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 442.70it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 442.70it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 442.75it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 442.75it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 442.75it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 442.75it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 444.73it/s, loss=0.163, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 444.73it/s, loss=0.163, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 444.73it/s, loss=0.163, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 444.73it/s, loss=0.163, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 444.10it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 444.10it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 444.10it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 444.10it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 444.12it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 444.12it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 444.12it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 444.12it/s, loss=0.166, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 443.82it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 443.82it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 443.82it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 443.82it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 444.65it/s, loss=0.173, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 444.65it/s, loss=0.173, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 444.65it/s, loss=0.173, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 444.65it/s, loss=0.173, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 444.45it/s, loss=0.172, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 444.45it/s, loss=0.172, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 444.45it/s, loss=0.172, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 444.45it/s, loss=0.172, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 442.28it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 442.28it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 442.28it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 442.28it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.27it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.27it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.27it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.27it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 459.93it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 459.93it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 459.93it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 459.93it/s, loss=0.147, v_num=0.0, ptl/val_loss=0.112]\n",
      "\u001B[2m\u001B[36m(pid=1892867)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 418.63it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 418.63it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 418.63it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 418.63it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 433.56it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.936] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 433.56it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.936] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 433.56it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.936] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 433.56it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.936] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 437.56it/s, loss=1.28, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 437.56it/s, loss=1.28, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 437.56it/s, loss=1.28, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 437.56it/s, loss=1.28, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 437.80it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 437.80it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 437.80it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 437.80it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.05it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.05it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.05it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.05it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 439.80it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 439.80it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 439.80it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 439.80it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 438.64it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 438.64it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 438.64it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 438.64it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 438.95it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 438.95it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 438.95it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 438.95it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 437.70it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 437.70it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 437.70it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 437.70it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 436.45it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 436.45it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 436.45it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 436.45it/s, loss=1.12, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 438.31it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 438.31it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 438.31it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 438.31it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.00it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.00it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.00it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.00it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.88it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.88it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.88it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.88it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.936]\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892873)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 436.32it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 436.32it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 436.32it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 436.32it/s, loss=1.06, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 448.16it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 448.16it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 448.16it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 448.16it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.31it/s, loss=1, v_num=0.0, ptl/val_loss=0.868]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.31it/s, loss=1, v_num=0.0, ptl/val_loss=0.868]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.31it/s, loss=1, v_num=0.0, ptl/val_loss=0.868]   \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.31it/s, loss=1, v_num=0.0, ptl/val_loss=0.868]   \n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.86it/s, loss=0.937, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.86it/s, loss=0.937, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.86it/s, loss=0.937, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.86it/s, loss=0.937, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 450.01it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 450.01it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 450.01it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 450.01it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 413.76it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 413.76it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 413.76it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 413.76it/s, loss=0.972, v_num=0.0, ptl/val_loss=0.868]\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892872)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 431.60it/s, loss=0.828, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 431.60it/s, loss=0.828, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 431.60it/s, loss=0.828, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 431.60it/s, loss=0.828, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.89it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.89it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.89it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 441.89it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.48it/s, loss=0.421, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.48it/s, loss=0.421, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.48it/s, loss=0.421, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 443.48it/s, loss=0.421, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.97it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.97it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.97it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 443.97it/s, loss=0.374, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 442.83it/s, loss=0.309, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 442.83it/s, loss=0.309, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 442.83it/s, loss=0.309, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 442.83it/s, loss=0.309, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.12it/s, loss=0.25, v_num=0.0, ptl/val_loss=0.852] \n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.12it/s, loss=0.25, v_num=0.0, ptl/val_loss=0.852] \n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.12it/s, loss=0.25, v_num=0.0, ptl/val_loss=0.852] \n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 438.12it/s, loss=0.25, v_num=0.0, ptl/val_loss=0.852] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 435.17it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 435.17it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 435.17it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 435.17it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 436.49it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 436.49it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 436.49it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 436.49it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 435.19it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 435.19it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 435.19it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 435.19it/s, loss=0.165, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 435.23it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 435.23it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 435.23it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 435.23it/s, loss=0.185, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 435.98it/s, loss=0.175, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 435.98it/s, loss=0.175, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 435.98it/s, loss=0.175, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 435.98it/s, loss=0.175, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 436.19it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 436.19it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 436.19it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 436.19it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 507.42it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 507.42it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 507.42it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 507.42it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.852]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 441.54it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 441.54it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 441.54it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 441.54it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0541]           \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 427.23it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 427.23it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 427.23it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 427.23it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 429.52it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 429.52it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 429.52it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 429.52it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 430.16it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 430.16it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 430.16it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 430.16it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 432.10it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 432.10it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 432.10it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 432.10it/s, loss=0.135, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.15it/s, loss=0.124, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.15it/s, loss=0.124, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.15it/s, loss=0.124, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.15it/s, loss=0.124, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.54it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.54it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.54it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.54it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 436.43it/s, loss=0.114, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 436.43it/s, loss=0.114, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 436.43it/s, loss=0.114, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 436.43it/s, loss=0.114, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 438.00it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 438.00it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 438.00it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 438.00it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.80it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.80it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.80it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 438.80it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.67it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.67it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.67it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.67it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.0541] \n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.57it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.57it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.57it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.57it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 440.87it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 440.87it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 440.87it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 440.87it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 476.07it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 476.07it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 476.07it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 476.07it/s, loss=0.0876, v_num=0.0, ptl/val_loss=0.0541]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 445.64it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 445.64it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 445.64it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 445.64it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0902, v_num=0.0, ptl/val_loss=0.023]           \n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 411.10it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 411.10it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 411.10it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 411.10it/s, loss=0.0892, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 434.05it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 434.05it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 434.05it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 434.05it/s, loss=0.105, v_num=0.0, ptl/val_loss=0.023] \n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 439.62it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 439.62it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 439.62it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 439.62it/s, loss=0.0888, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.54it/s, loss=0.0909, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.54it/s, loss=0.0909, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.54it/s, loss=0.0909, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.54it/s, loss=0.0909, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.20it/s, loss=0.0839, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.20it/s, loss=0.0839, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.20it/s, loss=0.0839, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 439.20it/s, loss=0.0839, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 437.21it/s, loss=0.0738, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 437.21it/s, loss=0.0738, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 437.21it/s, loss=0.0738, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 437.21it/s, loss=0.0738, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.20it/s, loss=0.0752, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.20it/s, loss=0.0752, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.20it/s, loss=0.0752, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 439.20it/s, loss=0.0752, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 438.55it/s, loss=0.0822, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 438.55it/s, loss=0.0822, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 438.55it/s, loss=0.0822, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 438.55it/s, loss=0.0822, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 437.86it/s, loss=0.0744, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 437.86it/s, loss=0.0744, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 437.86it/s, loss=0.0744, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 437.86it/s, loss=0.0744, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 438.11it/s, loss=0.0649, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 438.11it/s, loss=0.0649, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 438.11it/s, loss=0.0649, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 438.11it/s, loss=0.0649, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.10it/s, loss=0.0759, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.10it/s, loss=0.0759, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.10it/s, loss=0.0759, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 439.10it/s, loss=0.0759, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 438.61it/s, loss=0.0748, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 438.61it/s, loss=0.0748, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 438.61it/s, loss=0.0748, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 438.61it/s, loss=0.0748, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.59it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.59it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.59it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 438.59it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.0657, v_num=0.0, ptl/val_loss=0.023]\n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 443.51it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 443.51it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 443.51it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 443.51it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0651, v_num=0.0, ptl/val_loss=0.0149]           \n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 441.14it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 441.14it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 441.14it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 441.14it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 441.79it/s, loss=0.0786, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 441.79it/s, loss=0.0786, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 441.79it/s, loss=0.0786, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 441.79it/s, loss=0.0786, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 438.91it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 438.91it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 438.91it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 438.91it/s, loss=0.068, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 437.55it/s, loss=0.0662, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 437.55it/s, loss=0.0662, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 437.55it/s, loss=0.0662, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 437.55it/s, loss=0.0662, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 439.53it/s, loss=0.0595, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 439.53it/s, loss=0.0595, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 439.53it/s, loss=0.0595, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 439.53it/s, loss=0.0595, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.13it/s, loss=0.0571, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.13it/s, loss=0.0571, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.13it/s, loss=0.0571, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.13it/s, loss=0.0571, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 440.93it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 440.93it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 440.93it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 440.93it/s, loss=0.0648, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 440.74it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 440.74it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 440.74it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 440.74it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.0149] \n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.59it/s, loss=0.0505, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 440.25it/s, loss=0.0597, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 440.25it/s, loss=0.0597, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 440.25it/s, loss=0.0597, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 440.25it/s, loss=0.0597, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 440.70it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 440.70it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 440.70it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 440.70it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 475.81it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 475.81it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 475.81it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 475.81it/s, loss=0.0522, v_num=0.0, ptl/val_loss=0.0149]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 444.37it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 444.37it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 444.37it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 444.37it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0521, v_num=0.0, ptl/val_loss=0.0112]           \n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 417.69it/s, loss=0.0541, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 417.69it/s, loss=0.0541, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 417.69it/s, loss=0.0541, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 417.69it/s, loss=0.0541, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 431.66it/s, loss=0.0613, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 431.66it/s, loss=0.0613, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 431.66it/s, loss=0.0613, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 431.66it/s, loss=0.0613, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 434.67it/s, loss=0.0558, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 434.67it/s, loss=0.0558, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 434.67it/s, loss=0.0558, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 434.67it/s, loss=0.0558, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 436.14it/s, loss=0.0557, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 436.14it/s, loss=0.0557, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 436.14it/s, loss=0.0557, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 436.14it/s, loss=0.0557, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 437.65it/s, loss=0.0532, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 437.65it/s, loss=0.0532, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 437.65it/s, loss=0.0532, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 437.65it/s, loss=0.0532, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  36%|███▌      | 120/335 [00:00<00:00, 438.97it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0112] \n",
      "Epoch 4:  36%|███▌      | 120/335 [00:00<00:00, 438.97it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0112] \n",
      "Epoch 4:  36%|███▌      | 120/335 [00:00<00:00, 438.97it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0112] \n",
      "Epoch 4:  36%|███▌      | 120/335 [00:00<00:00, 438.97it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0112] \n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 439.61it/s, loss=0.0463, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 439.61it/s, loss=0.0463, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 439.61it/s, loss=0.0463, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 439.61it/s, loss=0.0463, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 439.09it/s, loss=0.0524, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 439.09it/s, loss=0.0524, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 439.09it/s, loss=0.0524, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 439.09it/s, loss=0.0524, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 439.57it/s, loss=0.0477, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 439.57it/s, loss=0.0477, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 439.57it/s, loss=0.0477, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 439.57it/s, loss=0.0477, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.58it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.58it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.58it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.58it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.70it/s, loss=0.0491, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.70it/s, loss=0.0491, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.70it/s, loss=0.0491, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.70it/s, loss=0.0491, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 439.89it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 439.89it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 439.89it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 439.89it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 474.68it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 474.68it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 474.68it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 474.68it/s, loss=0.0435, v_num=0.0, ptl/val_loss=0.0112]\n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 445.47it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.00852]\n",
      "                                                  \u001B[A\n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 445.47it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.00852]\n",
      "                                                  \u001B[A\n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 445.47it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.00852]\n",
      "                                                  \u001B[A\n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 445.47it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.00852]\n",
      "                                                  \u001B[A\n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 451.68it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852]  \n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 451.68it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852]  \n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 451.68it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852]  \n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 451.68it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852]  \n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 446.24it/s, loss=0.0507, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 446.24it/s, loss=0.0507, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 446.24it/s, loss=0.0507, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 446.24it/s, loss=0.0507, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 448.19it/s, loss=0.0456, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 448.19it/s, loss=0.0456, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 448.19it/s, loss=0.0456, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 448.19it/s, loss=0.0456, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  24%|██▍       | 80/335 [00:00<00:00, 445.33it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  24%|██▍       | 80/335 [00:00<00:00, 445.33it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  24%|██▍       | 80/335 [00:00<00:00, 445.33it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  24%|██▍       | 80/335 [00:00<00:00, 445.33it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 445.54it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852] \n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 445.54it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852] \n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 445.54it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852] \n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 445.54it/s, loss=0.044, v_num=0.0, ptl/val_loss=0.00852] \n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 443.39it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 443.39it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 443.39it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 443.39it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 444.06it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 444.06it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 444.06it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 444.06it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.24it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.24it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.24it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.24it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 445.19it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 445.19it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 445.19it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 445.19it/s, loss=0.0411, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 445.75it/s, loss=0.0367, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 445.75it/s, loss=0.0367, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 445.75it/s, loss=0.0367, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 445.75it/s, loss=0.0367, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0418, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0418, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0418, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0418, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 447.05it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 447.05it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 447.05it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 447.05it/s, loss=0.0413, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 447.68it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 447.68it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 447.68it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 447.68it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 482.98it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 482.98it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 482.98it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 482.98it/s, loss=0.0368, v_num=0.0, ptl/val_loss=0.00852]\n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 450.87it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 450.87it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 450.87it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 450.87it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0372, v_num=0.0, ptl/val_loss=0.00754]           \n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 452.60it/s, loss=0.0373, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 452.60it/s, loss=0.0373, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 452.60it/s, loss=0.0373, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 452.60it/s, loss=0.0373, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 454.52it/s, loss=0.0445, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 454.52it/s, loss=0.0445, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 454.52it/s, loss=0.0445, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 454.52it/s, loss=0.0445, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 452.42it/s, loss=0.0382, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 452.42it/s, loss=0.0382, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 452.42it/s, loss=0.0382, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 452.42it/s, loss=0.0382, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 453.89it/s, loss=0.0407, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 453.89it/s, loss=0.0407, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 453.89it/s, loss=0.0407, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 453.89it/s, loss=0.0407, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  36%|███▌      | 120/335 [00:00<00:00, 450.51it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  36%|███▌      | 120/335 [00:00<00:00, 450.51it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  36%|███▌      | 120/335 [00:00<00:00, 450.51it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  36%|███▌      | 120/335 [00:00<00:00, 450.51it/s, loss=0.0352, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.60it/s, loss=0.0343, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.60it/s, loss=0.0343, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.60it/s, loss=0.0343, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.60it/s, loss=0.0343, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 449.93it/s, loss=0.038, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 449.93it/s, loss=0.038, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 449.93it/s, loss=0.038, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 449.93it/s, loss=0.038, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.87it/s, loss=0.0365, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.87it/s, loss=0.0365, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.87it/s, loss=0.0365, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.87it/s, loss=0.0365, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 449.71it/s, loss=0.0318, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 449.71it/s, loss=0.0318, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 449.71it/s, loss=0.0318, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 449.71it/s, loss=0.0318, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.76it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.76it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.76it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.76it/s, loss=0.0381, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 449.59it/s, loss=0.037, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 449.59it/s, loss=0.037, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 449.59it/s, loss=0.037, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 449.59it/s, loss=0.037, v_num=0.0, ptl/val_loss=0.00754] \n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 449.19it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 484.90it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 449.19it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 484.90it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 449.19it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 484.90it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 449.19it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 484.90it/s, loss=0.0326, v_num=0.0, ptl/val_loss=0.00754]\n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 451.91it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 451.91it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 451.91it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 451.91it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.00645]           \n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 449.83it/s, loss=0.0332, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 449.83it/s, loss=0.0332, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 449.83it/s, loss=0.0332, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 449.83it/s, loss=0.0332, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 450.29it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 450.29it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 450.29it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 450.29it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  24%|██▍       | 80/335 [00:00<00:00, 448.31it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  24%|██▍       | 80/335 [00:00<00:00, 448.31it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  24%|██▍       | 80/335 [00:00<00:00, 448.31it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  24%|██▍       | 80/335 [00:00<00:00, 448.31it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 448.35it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 448.35it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 448.35it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 448.35it/s, loss=0.0336, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 449.85it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 449.85it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 449.85it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 449.85it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 450.92it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 450.92it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 450.92it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 450.92it/s, loss=0.0301, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 451.31it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 451.31it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 451.31it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 451.31it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 453.28it/s, loss=0.0277, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 453.28it/s, loss=0.0277, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 453.28it/s, loss=0.0277, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 453.28it/s, loss=0.0277, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 452.85it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 452.85it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 452.85it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 452.85it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 453.16it/s, loss=0.033, v_num=0.0, ptl/val_loss=0.00645] \n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 453.16it/s, loss=0.033, v_num=0.0, ptl/val_loss=0.00645] \n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 453.16it/s, loss=0.033, v_num=0.0, ptl/val_loss=0.00645] \n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 453.16it/s, loss=0.033, v_num=0.0, ptl/val_loss=0.00645] \n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 452.75it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 452.75it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 452.75it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 452.75it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 488.23it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 488.23it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 488.23it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 488.23it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00645]\n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 454.98it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 454.98it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 454.98it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 454.98it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0284, v_num=0.0, ptl/val_loss=0.00576]           \n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 445.21it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 445.21it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 445.21it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 445.21it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  12%|█▏        | 40/335 [00:00<00:00, 456.93it/s, loss=0.0354, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  12%|█▏        | 40/335 [00:00<00:00, 456.93it/s, loss=0.0354, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  12%|█▏        | 40/335 [00:00<00:00, 456.93it/s, loss=0.0354, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  12%|█▏        | 40/335 [00:00<00:00, 456.93it/s, loss=0.0354, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 454.53it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 454.53it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 454.53it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 454.53it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 452.59it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 452.59it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 452.59it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 452.59it/s, loss=0.0329, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 454.01it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 454.01it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 454.01it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 454.01it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 455.35it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 455.35it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 455.35it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 455.35it/s, loss=0.0276, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 453.66it/s, loss=0.0299, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 453.66it/s, loss=0.0299, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 453.66it/s, loss=0.0299, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 453.66it/s, loss=0.0299, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  54%|█████▎    | 180/335 [00:00<00:00, 453.95it/s, loss=0.0294, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  54%|█████▎    | 180/335 [00:00<00:00, 453.95it/s, loss=0.0294, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  54%|█████▎    | 180/335 [00:00<00:00, 453.95it/s, loss=0.0294, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  54%|█████▎    | 180/335 [00:00<00:00, 453.95it/s, loss=0.0294, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 453.86it/s, loss=0.0251, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 453.86it/s, loss=0.0251, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 453.86it/s, loss=0.0251, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 453.86it/s, loss=0.0251, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 453.62it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 453.62it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 453.62it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 453.62it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 452.64it/s, loss=0.0292, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 452.64it/s, loss=0.0292, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 452.64it/s, loss=0.0292, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 452.64it/s, loss=0.0292, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.51it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8:  90%|████████▉ | 300/335 [00:00<00:00, 488.42it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.51it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8:  90%|████████▉ | 300/335 [00:00<00:00, 488.42it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.51it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8:  90%|████████▉ | 300/335 [00:00<00:00, 488.42it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.51it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8:  90%|████████▉ | 300/335 [00:00<00:00, 488.42it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00576]\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 454.62it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]           \n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 454.62it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]           \n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 454.62it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]           \n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 454.62it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00489]           \n",
      "Epoch 9:   6%|▌         | 20/335 [00:00<00:00, 449.29it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   6%|▌         | 20/335 [00:00<00:00, 449.29it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   6%|▌         | 20/335 [00:00<00:00, 449.29it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:   6%|▌         | 20/335 [00:00<00:00, 449.29it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  18%|█▊        | 60/335 [00:00<00:00, 451.35it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  18%|█▊        | 60/335 [00:00<00:00, 451.35it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  18%|█▊        | 60/335 [00:00<00:00, 451.35it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  18%|█▊        | 60/335 [00:00<00:00, 451.35it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  24%|██▍       | 80/335 [00:00<00:00, 453.65it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  24%|██▍       | 80/335 [00:00<00:00, 453.65it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  24%|██▍       | 80/335 [00:00<00:00, 453.65it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  24%|██▍       | 80/335 [00:00<00:00, 453.65it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  30%|██▉       | 100/335 [00:00<00:00, 452.65it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  30%|██▉       | 100/335 [00:00<00:00, 452.65it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  30%|██▉       | 100/335 [00:00<00:00, 452.65it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  30%|██▉       | 100/335 [00:00<00:00, 452.65it/s, loss=0.0295, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  36%|███▌      | 120/335 [00:00<00:00, 452.30it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  36%|███▌      | 120/335 [00:00<00:00, 452.30it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  36%|███▌      | 120/335 [00:00<00:00, 452.30it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  36%|███▌      | 120/335 [00:00<00:00, 452.30it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  42%|████▏     | 140/335 [00:00<00:00, 450.54it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  42%|████▏     | 140/335 [00:00<00:00, 450.54it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  42%|████▏     | 140/335 [00:00<00:00, 450.54it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  42%|████▏     | 140/335 [00:00<00:00, 450.54it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  48%|████▊     | 160/335 [00:00<00:00, 450.48it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  48%|████▊     | 160/335 [00:00<00:00, 450.48it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  48%|████▊     | 160/335 [00:00<00:00, 450.48it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  48%|████▊     | 160/335 [00:00<00:00, 450.48it/s, loss=0.0282, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  54%|█████▎    | 180/335 [00:00<00:00, 451.01it/s, loss=0.0266, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  54%|█████▎    | 180/335 [00:00<00:00, 451.01it/s, loss=0.0266, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  54%|█████▎    | 180/335 [00:00<00:00, 451.01it/s, loss=0.0266, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  54%|█████▎    | 180/335 [00:00<00:00, 451.01it/s, loss=0.0266, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  66%|██████▌   | 220/335 [00:00<00:00, 451.32it/s, loss=0.0259, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  66%|██████▌   | 220/335 [00:00<00:00, 451.32it/s, loss=0.0259, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  66%|██████▌   | 220/335 [00:00<00:00, 451.32it/s, loss=0.0259, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  66%|██████▌   | 220/335 [00:00<00:00, 451.32it/s, loss=0.0259, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  72%|███████▏  | 240/335 [00:00<00:00, 451.42it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  72%|███████▏  | 240/335 [00:00<00:00, 451.42it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  72%|███████▏  | 240/335 [00:00<00:00, 451.42it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  72%|███████▏  | 240/335 [00:00<00:00, 451.42it/s, loss=0.0262, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  78%|███████▊  | 260/335 [00:00<00:00, 451.98it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  78%|███████▊  | 260/335 [00:00<00:00, 451.98it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  78%|███████▊  | 260/335 [00:00<00:00, 451.98it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9:  78%|███████▊  | 260/335 [00:00<00:00, 451.98it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9:  90%|████████▉ | 300/335 [00:00<00:00, 487.27it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9:  90%|████████▉ | 300/335 [00:00<00:00, 487.27it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9:  90%|████████▉ | 300/335 [00:00<00:00, 487.27it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 9:  90%|████████▉ | 300/335 [00:00<00:00, 487.27it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00489]\n",
      "Epoch 9: 100%|██████████| 335/335 [00:00<00:00, 453.65it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468] \n",
      "Epoch 10:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]          \n",
      "Epoch 9: 100%|██████████| 335/335 [00:00<00:00, 453.65it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468] \n",
      "Epoch 10:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]          \n",
      "Epoch 9: 100%|██████████| 335/335 [00:00<00:00, 453.65it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468] \n",
      "Epoch 10:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]          \n",
      "Epoch 9: 100%|██████████| 335/335 [00:00<00:00, 453.65it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468] \n",
      "Epoch 10:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]          \n",
      "Epoch 10:   6%|▌         | 20/335 [00:00<00:00, 440.47it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:   6%|▌         | 20/335 [00:00<00:00, 440.47it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:   6%|▌         | 20/335 [00:00<00:00, 440.47it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:   6%|▌         | 20/335 [00:00<00:00, 440.47it/s, loss=0.024, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  12%|█▏        | 40/335 [00:00<00:00, 448.51it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  12%|█▏        | 40/335 [00:00<00:00, 448.51it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  12%|█▏        | 40/335 [00:00<00:00, 448.51it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  12%|█▏        | 40/335 [00:00<00:00, 448.51it/s, loss=0.0298, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  18%|█▊        | 60/335 [00:00<00:00, 451.85it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  18%|█▊        | 60/335 [00:00<00:00, 451.85it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  18%|█▊        | 60/335 [00:00<00:00, 451.85it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  18%|█▊        | 60/335 [00:00<00:00, 451.85it/s, loss=0.0247, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  24%|██▍       | 80/335 [00:00<00:00, 452.97it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  24%|██▍       | 80/335 [00:00<00:00, 452.97it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  24%|██▍       | 80/335 [00:00<00:00, 452.97it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  24%|██▍       | 80/335 [00:00<00:00, 452.97it/s, loss=0.0265, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  30%|██▉       | 100/335 [00:00<00:00, 452.96it/s, loss=0.0261, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  30%|██▉       | 100/335 [00:00<00:00, 452.96it/s, loss=0.0261, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  30%|██▉       | 100/335 [00:00<00:00, 452.96it/s, loss=0.0261, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  30%|██▉       | 100/335 [00:00<00:00, 452.96it/s, loss=0.0261, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  42%|████▏     | 140/335 [00:00<00:00, 451.59it/s, loss=0.0231, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  42%|████▏     | 140/335 [00:00<00:00, 451.59it/s, loss=0.0231, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  42%|████▏     | 140/335 [00:00<00:00, 451.59it/s, loss=0.0231, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  42%|████▏     | 140/335 [00:00<00:00, 451.59it/s, loss=0.0231, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  48%|████▊     | 160/335 [00:00<00:00, 451.91it/s, loss=0.0256, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  48%|████▊     | 160/335 [00:00<00:00, 451.91it/s, loss=0.0256, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  48%|████▊     | 160/335 [00:00<00:00, 451.91it/s, loss=0.0256, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  48%|████▊     | 160/335 [00:00<00:00, 451.91it/s, loss=0.0256, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  54%|█████▎    | 180/335 [00:00<00:00, 451.31it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  54%|█████▎    | 180/335 [00:00<00:00, 451.31it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  54%|█████▎    | 180/335 [00:00<00:00, 451.31it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  54%|█████▎    | 180/335 [00:00<00:00, 451.31it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  60%|█████▉    | 200/335 [00:00<00:00, 452.24it/s, loss=0.0198, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  60%|█████▉    | 200/335 [00:00<00:00, 452.24it/s, loss=0.0198, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  60%|█████▉    | 200/335 [00:00<00:00, 452.24it/s, loss=0.0198, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  60%|█████▉    | 200/335 [00:00<00:00, 452.24it/s, loss=0.0198, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  66%|██████▌   | 220/335 [00:00<00:00, 453.11it/s, loss=0.0238, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  66%|██████▌   | 220/335 [00:00<00:00, 453.11it/s, loss=0.0238, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  66%|██████▌   | 220/335 [00:00<00:00, 453.11it/s, loss=0.0238, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  66%|██████▌   | 220/335 [00:00<00:00, 453.11it/s, loss=0.0238, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  72%|███████▏  | 240/335 [00:00<00:00, 452.98it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  72%|███████▏  | 240/335 [00:00<00:00, 452.98it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  72%|███████▏  | 240/335 [00:00<00:00, 452.98it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  72%|███████▏  | 240/335 [00:00<00:00, 452.98it/s, loss=0.0246, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  78%|███████▊  | 260/335 [00:00<00:00, 452.70it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10:  90%|████████▉ | 300/335 [00:00<00:00, 488.19it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  78%|███████▊  | 260/335 [00:00<00:00, 452.70it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10:  90%|████████▉ | 300/335 [00:00<00:00, 488.19it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  78%|███████▊  | 260/335 [00:00<00:00, 452.70it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10:  90%|████████▉ | 300/335 [00:00<00:00, 488.19it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10:  78%|███████▊  | 260/335 [00:00<00:00, 452.70it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10:  90%|████████▉ | 300/335 [00:00<00:00, 488.19it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00468]\n",
      "Epoch 10: 100%|██████████| 335/335 [00:00<00:00, 454.24it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]  \n",
      "Epoch 11:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]           \n",
      "Epoch 10: 100%|██████████| 335/335 [00:00<00:00, 454.24it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]  \n",
      "Epoch 11:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]           \n",
      "Epoch 10: 100%|██████████| 335/335 [00:00<00:00, 454.24it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]  \n",
      "Epoch 11:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]           \n",
      "Epoch 10: 100%|██████████| 335/335 [00:00<00:00, 454.24it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]  \n",
      "Epoch 11:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0224, v_num=0.0, ptl/val_loss=0.004]           \n",
      "Epoch 11:   6%|▌         | 20/335 [00:00<00:00, 451.43it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:   6%|▌         | 20/335 [00:00<00:00, 451.43it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:   6%|▌         | 20/335 [00:00<00:00, 451.43it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:   6%|▌         | 20/335 [00:00<00:00, 451.43it/s, loss=0.0229, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  12%|█▏        | 40/335 [00:00<00:00, 459.15it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  12%|█▏        | 40/335 [00:00<00:00, 459.15it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  12%|█▏        | 40/335 [00:00<00:00, 459.15it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  12%|█▏        | 40/335 [00:00<00:00, 459.15it/s, loss=0.026, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  24%|██▍       | 80/335 [00:00<00:00, 462.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  24%|██▍       | 80/335 [00:00<00:00, 462.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  24%|██▍       | 80/335 [00:00<00:00, 462.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  24%|██▍       | 80/335 [00:00<00:00, 462.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  30%|██▉       | 100/335 [00:00<00:00, 460.45it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  30%|██▉       | 100/335 [00:00<00:00, 460.45it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  30%|██▉       | 100/335 [00:00<00:00, 460.45it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  30%|██▉       | 100/335 [00:00<00:00, 460.45it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  36%|███▌      | 120/335 [00:00<00:00, 459.07it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  36%|███▌      | 120/335 [00:00<00:00, 459.07it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  36%|███▌      | 120/335 [00:00<00:00, 459.07it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  36%|███▌      | 120/335 [00:00<00:00, 459.07it/s, loss=0.0218, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  42%|████▏     | 140/335 [00:00<00:00, 459.30it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  42%|████▏     | 140/335 [00:00<00:00, 459.30it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  42%|████▏     | 140/335 [00:00<00:00, 459.30it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  42%|████▏     | 140/335 [00:00<00:00, 459.30it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  48%|████▊     | 160/335 [00:00<00:00, 457.85it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  48%|████▊     | 160/335 [00:00<00:00, 457.85it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  48%|████▊     | 160/335 [00:00<00:00, 457.85it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  48%|████▊     | 160/335 [00:00<00:00, 457.85it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  54%|█████▎    | 180/335 [00:00<00:00, 458.13it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  54%|█████▎    | 180/335 [00:00<00:00, 458.13it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  54%|█████▎    | 180/335 [00:00<00:00, 458.13it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  54%|█████▎    | 180/335 [00:00<00:00, 458.13it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  66%|██████▌   | 220/335 [00:00<00:00, 457.24it/s, loss=0.0222, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  66%|██████▌   | 220/335 [00:00<00:00, 457.24it/s, loss=0.0222, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  66%|██████▌   | 220/335 [00:00<00:00, 457.24it/s, loss=0.0222, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  66%|██████▌   | 220/335 [00:00<00:00, 457.24it/s, loss=0.0222, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  72%|███████▏  | 240/335 [00:00<00:00, 457.21it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  72%|███████▏  | 240/335 [00:00<00:00, 457.21it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  72%|███████▏  | 240/335 [00:00<00:00, 457.21it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  72%|███████▏  | 240/335 [00:00<00:00, 457.21it/s, loss=0.022, v_num=0.0, ptl/val_loss=0.004] \n",
      "Epoch 11:  78%|███████▊  | 260/335 [00:00<00:00, 456.67it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  78%|███████▊  | 260/335 [00:00<00:00, 456.67it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  78%|███████▊  | 260/335 [00:00<00:00, 456.67it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11:  78%|███████▊  | 260/335 [00:00<00:00, 456.67it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11:  90%|████████▉ | 300/335 [00:00<00:00, 492.44it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11:  90%|████████▉ | 300/335 [00:00<00:00, 492.44it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11:  90%|████████▉ | 300/335 [00:00<00:00, 492.44it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 11:  90%|████████▉ | 300/335 [00:00<00:00, 492.44it/s, loss=0.0194, v_num=0.0, ptl/val_loss=0.004]\n",
      "Epoch 11: 100%|██████████| 335/335 [00:00<00:00, 458.71it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]           \n",
      "Epoch 11: 100%|██████████| 335/335 [00:00<00:00, 458.71it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]           \n",
      "Epoch 11: 100%|██████████| 335/335 [00:00<00:00, 458.71it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]           \n",
      "Epoch 11: 100%|██████████| 335/335 [00:00<00:00, 458.71it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00385]           \n",
      "Epoch 12:   6%|▌         | 20/335 [00:00<00:00, 442.57it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   6%|▌         | 20/335 [00:00<00:00, 442.57it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   6%|▌         | 20/335 [00:00<00:00, 442.57it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:   6%|▌         | 20/335 [00:00<00:00, 442.57it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  12%|█▏        | 40/335 [00:00<00:00, 449.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  12%|█▏        | 40/335 [00:00<00:00, 449.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  12%|█▏        | 40/335 [00:00<00:00, 449.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  12%|█▏        | 40/335 [00:00<00:00, 449.24it/s, loss=0.0244, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  18%|█▊        | 60/335 [00:00<00:00, 453.02it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  18%|█▊        | 60/335 [00:00<00:00, 453.02it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  18%|█▊        | 60/335 [00:00<00:00, 453.02it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  18%|█▊        | 60/335 [00:00<00:00, 453.02it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  24%|██▍       | 80/335 [00:00<00:00, 454.50it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  24%|██▍       | 80/335 [00:00<00:00, 454.50it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  24%|██▍       | 80/335 [00:00<00:00, 454.50it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  24%|██▍       | 80/335 [00:00<00:00, 454.50it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  30%|██▉       | 100/335 [00:00<00:00, 455.89it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  30%|██▉       | 100/335 [00:00<00:00, 455.89it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  30%|██▉       | 100/335 [00:00<00:00, 455.89it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  30%|██▉       | 100/335 [00:00<00:00, 455.89it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  42%|████▏     | 140/335 [00:00<00:00, 457.96it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  42%|████▏     | 140/335 [00:00<00:00, 457.96it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  42%|████▏     | 140/335 [00:00<00:00, 457.96it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  42%|████▏     | 140/335 [00:00<00:00, 457.96it/s, loss=0.0192, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  48%|████▊     | 160/335 [00:00<00:00, 457.41it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  48%|████▊     | 160/335 [00:00<00:00, 457.41it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  48%|████▊     | 160/335 [00:00<00:00, 457.41it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  48%|████▊     | 160/335 [00:00<00:00, 457.41it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  54%|█████▎    | 180/335 [00:00<00:00, 457.32it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00385] \n",
      "Epoch 12:  54%|█████▎    | 180/335 [00:00<00:00, 457.32it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00385] \n",
      "Epoch 12:  54%|█████▎    | 180/335 [00:00<00:00, 457.32it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00385] \n",
      "Epoch 12:  54%|█████▎    | 180/335 [00:00<00:00, 457.32it/s, loss=0.021, v_num=0.0, ptl/val_loss=0.00385] \n",
      "Epoch 12:  60%|█████▉    | 200/335 [00:00<00:00, 456.95it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  60%|█████▉    | 200/335 [00:00<00:00, 456.95it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  60%|█████▉    | 200/335 [00:00<00:00, 456.95it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  60%|█████▉    | 200/335 [00:00<00:00, 456.95it/s, loss=0.0178, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  66%|██████▌   | 220/335 [00:00<00:00, 457.16it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  66%|██████▌   | 220/335 [00:00<00:00, 457.16it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  66%|██████▌   | 220/335 [00:00<00:00, 457.16it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  66%|██████▌   | 220/335 [00:00<00:00, 457.16it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  72%|███████▏  | 240/335 [00:00<00:00, 456.29it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  72%|███████▏  | 240/335 [00:00<00:00, 456.29it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  72%|███████▏  | 240/335 [00:00<00:00, 456.29it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  72%|███████▏  | 240/335 [00:00<00:00, 456.29it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  78%|███████▊  | 260/335 [00:00<00:00, 456.53it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12:  90%|████████▉ | 300/335 [00:00<00:00, 491.76it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  78%|███████▊  | 260/335 [00:00<00:00, 456.53it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12:  90%|████████▉ | 300/335 [00:00<00:00, 491.76it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  78%|███████▊  | 260/335 [00:00<00:00, 456.53it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12:  90%|████████▉ | 300/335 [00:00<00:00, 491.76it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12:  78%|███████▊  | 260/335 [00:00<00:00, 456.53it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 12:  90%|████████▉ | 300/335 [00:00<00:00, 491.76it/s, loss=0.0188, v_num=0.0, ptl/val_loss=0.00385]\n",
      "Epoch 12: 100%|██████████| 335/335 [00:00<00:00, 456.89it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353] \n",
      "Epoch 13:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353]           \n",
      "Epoch 12: 100%|██████████| 335/335 [00:00<00:00, 456.89it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353] \n",
      "Epoch 13:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353]           \n",
      "Epoch 12: 100%|██████████| 335/335 [00:00<00:00, 456.89it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353] \n",
      "Epoch 13:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353]           \n",
      "Epoch 12: 100%|██████████| 335/335 [00:00<00:00, 456.89it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353] \n",
      "Epoch 13:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.019, v_num=0.0, ptl/val_loss=0.00353]           \n",
      "Epoch 13:   6%|▌         | 20/335 [00:00<00:00, 446.64it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:   6%|▌         | 20/335 [00:00<00:00, 446.64it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:   6%|▌         | 20/335 [00:00<00:00, 446.64it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:   6%|▌         | 20/335 [00:00<00:00, 446.64it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  18%|█▊        | 60/335 [00:00<00:00, 455.82it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  18%|█▊        | 60/335 [00:00<00:00, 455.82it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  18%|█▊        | 60/335 [00:00<00:00, 455.82it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  18%|█▊        | 60/335 [00:00<00:00, 455.82it/s, loss=0.0205, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  24%|██▍       | 80/335 [00:00<00:00, 454.63it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  24%|██▍       | 80/335 [00:00<00:00, 454.63it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  24%|██▍       | 80/335 [00:00<00:00, 454.63it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  24%|██▍       | 80/335 [00:00<00:00, 454.63it/s, loss=0.0216, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  30%|██▉       | 100/335 [00:00<00:00, 455.66it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  30%|██▉       | 100/335 [00:00<00:00, 455.66it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  30%|██▉       | 100/335 [00:00<00:00, 455.66it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  30%|██▉       | 100/335 [00:00<00:00, 455.66it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  36%|███▌      | 120/335 [00:00<00:00, 456.05it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  36%|███▌      | 120/335 [00:00<00:00, 456.05it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  36%|███▌      | 120/335 [00:00<00:00, 456.05it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  36%|███▌      | 120/335 [00:00<00:00, 456.05it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  42%|████▏     | 140/335 [00:00<00:00, 455.53it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  42%|████▏     | 140/335 [00:00<00:00, 455.53it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  42%|████▏     | 140/335 [00:00<00:00, 455.53it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  42%|████▏     | 140/335 [00:00<00:00, 455.53it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  48%|████▊     | 160/335 [00:00<00:00, 454.75it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  48%|████▊     | 160/335 [00:00<00:00, 454.75it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  48%|████▊     | 160/335 [00:00<00:00, 454.75it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  48%|████▊     | 160/335 [00:00<00:00, 454.75it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  60%|█████▉    | 200/335 [00:00<00:00, 455.11it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  60%|█████▉    | 200/335 [00:00<00:00, 455.11it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  60%|█████▉    | 200/335 [00:00<00:00, 455.11it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  60%|█████▉    | 200/335 [00:00<00:00, 455.11it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  66%|██████▌   | 220/335 [00:00<00:00, 425.89it/s, loss=0.02, v_num=0.0, ptl/val_loss=0.00353]  \n",
      "Epoch 13:  66%|██████▌   | 220/335 [00:00<00:00, 425.89it/s, loss=0.02, v_num=0.0, ptl/val_loss=0.00353]  \n",
      "Epoch 13:  66%|██████▌   | 220/335 [00:00<00:00, 425.89it/s, loss=0.02, v_num=0.0, ptl/val_loss=0.00353]  \n",
      "Epoch 13:  66%|██████▌   | 220/335 [00:00<00:00, 425.89it/s, loss=0.02, v_num=0.0, ptl/val_loss=0.00353]  \n",
      "Epoch 13:  72%|███████▏  | 240/335 [00:00<00:00, 427.96it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  72%|███████▏  | 240/335 [00:00<00:00, 427.96it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  72%|███████▏  | 240/335 [00:00<00:00, 427.96it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  72%|███████▏  | 240/335 [00:00<00:00, 427.96it/s, loss=0.0196, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  78%|███████▊  | 260/335 [00:00<00:00, 429.82it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  78%|███████▊  | 260/335 [00:00<00:00, 429.82it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  78%|███████▊  | 260/335 [00:00<00:00, 429.82it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  78%|███████▊  | 260/335 [00:00<00:00, 429.82it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  90%|████████▉ | 300/335 [00:00<00:00, 464.39it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  90%|████████▉ | 300/335 [00:00<00:00, 464.39it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  90%|████████▉ | 300/335 [00:00<00:00, 464.39it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13:  90%|████████▉ | 300/335 [00:00<00:00, 464.39it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00353]\n",
      "Epoch 13: 100%|██████████| 335/335 [00:00<00:00, 435.96it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]           \n",
      "Epoch 13: 100%|██████████| 335/335 [00:00<00:00, 435.96it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]           \n",
      "Epoch 13: 100%|██████████| 335/335 [00:00<00:00, 435.96it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]           \n",
      "Epoch 13: 100%|██████████| 335/335 [00:00<00:00, 435.96it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00297]           \n",
      "Epoch 14:   6%|▌         | 20/335 [00:00<00:00, 444.59it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   6%|▌         | 20/335 [00:00<00:00, 444.59it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   6%|▌         | 20/335 [00:00<00:00, 444.59it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:   6%|▌         | 20/335 [00:00<00:00, 444.59it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  12%|█▏        | 40/335 [00:00<00:00, 447.62it/s, loss=0.0207, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  12%|█▏        | 40/335 [00:00<00:00, 447.62it/s, loss=0.0207, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  12%|█▏        | 40/335 [00:00<00:00, 447.62it/s, loss=0.0207, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  12%|█▏        | 40/335 [00:00<00:00, 447.62it/s, loss=0.0207, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  18%|█▊        | 60/335 [00:00<00:00, 444.60it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  18%|█▊        | 60/335 [00:00<00:00, 444.60it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  18%|█▊        | 60/335 [00:00<00:00, 444.60it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  18%|█▊        | 60/335 [00:00<00:00, 444.60it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  30%|██▉       | 100/335 [00:00<00:00, 446.03it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  30%|██▉       | 100/335 [00:00<00:00, 446.03it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  30%|██▉       | 100/335 [00:00<00:00, 446.03it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  30%|██▉       | 100/335 [00:00<00:00, 446.03it/s, loss=0.0183, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0168, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  42%|████▏     | 140/335 [00:00<00:00, 446.86it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  42%|████▏     | 140/335 [00:00<00:00, 446.86it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  42%|████▏     | 140/335 [00:00<00:00, 446.86it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  42%|████▏     | 140/335 [00:00<00:00, 446.86it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  48%|████▊     | 160/335 [00:00<00:00, 447.52it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  48%|████▊     | 160/335 [00:00<00:00, 447.52it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  48%|████▊     | 160/335 [00:00<00:00, 447.52it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  48%|████▊     | 160/335 [00:00<00:00, 447.52it/s, loss=0.0199, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  54%|█████▎    | 180/335 [00:00<00:00, 449.12it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  54%|█████▎    | 180/335 [00:00<00:00, 449.12it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  54%|█████▎    | 180/335 [00:00<00:00, 449.12it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  54%|█████▎    | 180/335 [00:00<00:00, 449.12it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  60%|█████▉    | 200/335 [00:00<00:00, 449.70it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  60%|█████▉    | 200/335 [00:00<00:00, 449.70it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  60%|█████▉    | 200/335 [00:00<00:00, 449.70it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  60%|█████▉    | 200/335 [00:00<00:00, 449.70it/s, loss=0.0165, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  66%|██████▌   | 220/335 [00:00<00:00, 449.84it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  66%|██████▌   | 220/335 [00:00<00:00, 449.84it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  66%|██████▌   | 220/335 [00:00<00:00, 449.84it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  66%|██████▌   | 220/335 [00:00<00:00, 449.84it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  72%|███████▏  | 240/335 [00:00<00:00, 448.33it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  72%|███████▏  | 240/335 [00:00<00:00, 448.33it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  72%|███████▏  | 240/335 [00:00<00:00, 448.33it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  72%|███████▏  | 240/335 [00:00<00:00, 448.33it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  78%|███████▊  | 260/335 [00:00<00:00, 448.23it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  78%|███████▊  | 260/335 [00:00<00:00, 448.23it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  78%|███████▊  | 260/335 [00:00<00:00, 448.23it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14:  78%|███████▊  | 260/335 [00:00<00:00, 448.23it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14:  90%|████████▉ | 300/335 [00:00<00:00, 484.12it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14:  90%|████████▉ | 300/335 [00:00<00:00, 484.12it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14:  90%|████████▉ | 300/335 [00:00<00:00, 484.12it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 14:  90%|████████▉ | 300/335 [00:00<00:00, 484.12it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00297]\n",
      "Epoch 14: 100%|██████████| 335/335 [00:00<00:00, 515.50it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]           \n",
      "Epoch 14: 100%|██████████| 335/335 [00:00<00:00, 515.50it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]           \n",
      "Epoch 14: 100%|██████████| 335/335 [00:00<00:00, 515.50it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]           \n",
      "Epoch 14: 100%|██████████| 335/335 [00:00<00:00, 515.50it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00311]           \n",
      "Epoch 15:   6%|▌         | 20/335 [00:00<00:00, 452.39it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   6%|▌         | 20/335 [00:00<00:00, 452.39it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   6%|▌         | 20/335 [00:00<00:00, 452.39it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:   6%|▌         | 20/335 [00:00<00:00, 452.39it/s, loss=0.0179, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  18%|█▊        | 60/335 [00:00<00:00, 454.59it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  18%|█▊        | 60/335 [00:00<00:00, 454.59it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  18%|█▊        | 60/335 [00:00<00:00, 454.59it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  18%|█▊        | 60/335 [00:00<00:00, 454.59it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  24%|██▍       | 80/335 [00:00<00:00, 457.96it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  24%|██▍       | 80/335 [00:00<00:00, 457.96it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  24%|██▍       | 80/335 [00:00<00:00, 457.96it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  24%|██▍       | 80/335 [00:00<00:00, 457.96it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  30%|██▉       | 100/335 [00:00<00:00, 457.25it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  30%|██▉       | 100/335 [00:00<00:00, 457.25it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  30%|██▉       | 100/335 [00:00<00:00, 457.25it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  30%|██▉       | 100/335 [00:00<00:00, 457.25it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  36%|███▌      | 120/335 [00:00<00:00, 459.52it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  36%|███▌      | 120/335 [00:00<00:00, 459.52it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  36%|███▌      | 120/335 [00:00<00:00, 459.52it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  36%|███▌      | 120/335 [00:00<00:00, 459.52it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  42%|████▏     | 140/335 [00:00<00:00, 459.32it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  42%|████▏     | 140/335 [00:00<00:00, 459.32it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  42%|████▏     | 140/335 [00:00<00:00, 459.32it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  42%|████▏     | 140/335 [00:00<00:00, 459.32it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  54%|█████▎    | 180/335 [00:00<00:00, 461.27it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  54%|█████▎    | 180/335 [00:00<00:00, 461.27it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  54%|█████▎    | 180/335 [00:00<00:00, 461.27it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  54%|█████▎    | 180/335 [00:00<00:00, 461.27it/s, loss=0.0163, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  60%|█████▉    | 200/335 [00:00<00:00, 461.51it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  60%|█████▉    | 200/335 [00:00<00:00, 461.51it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  60%|█████▉    | 200/335 [00:00<00:00, 461.51it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  60%|█████▉    | 200/335 [00:00<00:00, 461.51it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  66%|██████▌   | 220/335 [00:00<00:00, 462.26it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  66%|██████▌   | 220/335 [00:00<00:00, 462.26it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  66%|██████▌   | 220/335 [00:00<00:00, 462.26it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  66%|██████▌   | 220/335 [00:00<00:00, 462.26it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  72%|███████▏  | 240/335 [00:00<00:00, 461.83it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  72%|███████▏  | 240/335 [00:00<00:00, 461.83it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  72%|███████▏  | 240/335 [00:00<00:00, 461.83it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  72%|███████▏  | 240/335 [00:00<00:00, 461.83it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  78%|███████▊  | 260/335 [00:00<00:00, 462.42it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  78%|███████▊  | 260/335 [00:00<00:00, 462.42it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  78%|███████▊  | 260/335 [00:00<00:00, 462.42it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  78%|███████▊  | 260/335 [00:00<00:00, 462.42it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  90%|████████▉ | 300/335 [00:00<00:00, 498.59it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  90%|████████▉ | 300/335 [00:00<00:00, 498.59it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  90%|████████▉ | 300/335 [00:00<00:00, 498.59it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15:  90%|████████▉ | 300/335 [00:00<00:00, 498.59it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00311]\n",
      "Epoch 15: 100%|██████████| 335/335 [00:00<00:00, 463.75it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 15: 100%|██████████| 335/335 [00:00<00:00, 463.75it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 15: 100%|██████████| 335/335 [00:00<00:00, 463.75it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 15: 100%|██████████| 335/335 [00:00<00:00, 463.75it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 16:   6%|▌         | 20/335 [00:00<00:00, 465.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   6%|▌         | 20/335 [00:00<00:00, 465.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   6%|▌         | 20/335 [00:00<00:00, 465.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:   6%|▌         | 20/335 [00:00<00:00, 465.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  12%|█▏        | 40/335 [00:00<00:00, 466.91it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  12%|█▏        | 40/335 [00:00<00:00, 466.91it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  12%|█▏        | 40/335 [00:00<00:00, 466.91it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  12%|█▏        | 40/335 [00:00<00:00, 466.91it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  24%|██▍       | 80/335 [00:00<00:00, 463.55it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  24%|██▍       | 80/335 [00:00<00:00, 463.55it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  24%|██▍       | 80/335 [00:00<00:00, 463.55it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  24%|██▍       | 80/335 [00:00<00:00, 463.55it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  30%|██▉       | 100/335 [00:00<00:00, 462.89it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  30%|██▉       | 100/335 [00:00<00:00, 462.89it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  30%|██▉       | 100/335 [00:00<00:00, 462.89it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  30%|██▉       | 100/335 [00:00<00:00, 462.89it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  36%|███▌      | 120/335 [00:00<00:00, 463.24it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  36%|███▌      | 120/335 [00:00<00:00, 463.24it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  36%|███▌      | 120/335 [00:00<00:00, 463.24it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  36%|███▌      | 120/335 [00:00<00:00, 463.24it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  42%|████▏     | 140/335 [00:00<00:00, 463.08it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  42%|████▏     | 140/335 [00:00<00:00, 463.08it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  42%|████▏     | 140/335 [00:00<00:00, 463.08it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  42%|████▏     | 140/335 [00:00<00:00, 463.08it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  48%|████▊     | 160/335 [00:00<00:00, 462.58it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  48%|████▊     | 160/335 [00:00<00:00, 462.58it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  48%|████▊     | 160/335 [00:00<00:00, 462.58it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  48%|████▊     | 160/335 [00:00<00:00, 462.58it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  54%|█████▎    | 180/335 [00:00<00:00, 462.29it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  54%|█████▎    | 180/335 [00:00<00:00, 462.29it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  54%|█████▎    | 180/335 [00:00<00:00, 462.29it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  54%|█████▎    | 180/335 [00:00<00:00, 462.29it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  60%|█████▉    | 200/335 [00:00<00:00, 461.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  60%|█████▉    | 200/335 [00:00<00:00, 461.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  60%|█████▉    | 200/335 [00:00<00:00, 461.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  60%|█████▉    | 200/335 [00:00<00:00, 461.61it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  66%|██████▌   | 220/335 [00:00<00:00, 459.78it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  66%|██████▌   | 220/335 [00:00<00:00, 459.78it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  66%|██████▌   | 220/335 [00:00<00:00, 459.78it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  66%|██████▌   | 220/335 [00:00<00:00, 459.78it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  72%|███████▏  | 240/335 [00:00<00:00, 458.88it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  72%|███████▏  | 240/335 [00:00<00:00, 458.88it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  72%|███████▏  | 240/335 [00:00<00:00, 458.88it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  72%|███████▏  | 240/335 [00:00<00:00, 458.88it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  78%|███████▊  | 260/335 [00:00<00:00, 457.99it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  78%|███████▊  | 260/335 [00:00<00:00, 457.99it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  78%|███████▊  | 260/335 [00:00<00:00, 457.99it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16:  78%|███████▊  | 260/335 [00:00<00:00, 457.99it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16:  90%|████████▉ | 300/335 [00:00<00:00, 492.06it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16:  90%|████████▉ | 300/335 [00:00<00:00, 492.06it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16:  90%|████████▉ | 300/335 [00:00<00:00, 492.06it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 16:  90%|████████▉ | 300/335 [00:00<00:00, 492.06it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 16: 100%|██████████| 335/335 [00:00<00:00, 457.87it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 16: 100%|██████████| 335/335 [00:00<00:00, 457.87it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 16: 100%|██████████| 335/335 [00:00<00:00, 457.87it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 16: 100%|██████████| 335/335 [00:00<00:00, 457.87it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0144, v_num=0.0, ptl/val_loss=0.00262]           \n",
      "Epoch 17:   6%|▌         | 20/335 [00:00<00:00, 431.84it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   6%|▌         | 20/335 [00:00<00:00, 431.84it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   6%|▌         | 20/335 [00:00<00:00, 431.84it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:   6%|▌         | 20/335 [00:00<00:00, 431.84it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  12%|█▏        | 40/335 [00:00<00:00, 431.54it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  12%|█▏        | 40/335 [00:00<00:00, 431.54it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  12%|█▏        | 40/335 [00:00<00:00, 431.54it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  12%|█▏        | 40/335 [00:00<00:00, 431.54it/s, loss=0.0184, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  18%|█▊        | 60/335 [00:00<00:00, 425.20it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  18%|█▊        | 60/335 [00:00<00:00, 425.20it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  18%|█▊        | 60/335 [00:00<00:00, 425.20it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  18%|█▊        | 60/335 [00:00<00:00, 425.20it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  24%|██▍       | 80/335 [00:00<00:00, 425.93it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  24%|██▍       | 80/335 [00:00<00:00, 425.93it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  24%|██▍       | 80/335 [00:00<00:00, 425.93it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  24%|██▍       | 80/335 [00:00<00:00, 425.93it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  30%|██▉       | 100/335 [00:00<00:00, 423.51it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  30%|██▉       | 100/335 [00:00<00:00, 423.51it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  30%|██▉       | 100/335 [00:00<00:00, 423.51it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  30%|██▉       | 100/335 [00:00<00:00, 423.51it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  36%|███▌      | 120/335 [00:00<00:00, 422.06it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  36%|███▌      | 120/335 [00:00<00:00, 422.06it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  36%|███▌      | 120/335 [00:00<00:00, 422.06it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  36%|███▌      | 120/335 [00:00<00:00, 422.06it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  42%|████▏     | 140/335 [00:00<00:00, 421.50it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  42%|████▏     | 140/335 [00:00<00:00, 421.50it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  42%|████▏     | 140/335 [00:00<00:00, 421.50it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  42%|████▏     | 140/335 [00:00<00:00, 421.50it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  48%|████▊     | 160/335 [00:00<00:00, 420.91it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  48%|████▊     | 160/335 [00:00<00:00, 420.91it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  48%|████▊     | 160/335 [00:00<00:00, 420.91it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  48%|████▊     | 160/335 [00:00<00:00, 420.91it/s, loss=0.0155, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  54%|█████▎    | 180/335 [00:00<00:00, 420.98it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  54%|█████▎    | 180/335 [00:00<00:00, 420.98it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  54%|█████▎    | 180/335 [00:00<00:00, 420.98it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  54%|█████▎    | 180/335 [00:00<00:00, 420.98it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  60%|█████▉    | 200/335 [00:00<00:00, 420.64it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  60%|█████▉    | 200/335 [00:00<00:00, 420.64it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  60%|█████▉    | 200/335 [00:00<00:00, 420.64it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  60%|█████▉    | 200/335 [00:00<00:00, 420.64it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  66%|██████▌   | 220/335 [00:00<00:00, 421.95it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  66%|██████▌   | 220/335 [00:00<00:00, 421.95it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  66%|██████▌   | 220/335 [00:00<00:00, 421.95it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  66%|██████▌   | 220/335 [00:00<00:00, 421.95it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  72%|███████▏  | 240/335 [00:00<00:00, 422.86it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  72%|███████▏  | 240/335 [00:00<00:00, 422.86it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  72%|███████▏  | 240/335 [00:00<00:00, 422.86it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  72%|███████▏  | 240/335 [00:00<00:00, 422.86it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  78%|███████▊  | 260/335 [00:00<00:00, 422.48it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  78%|███████▊  | 260/335 [00:00<00:00, 422.48it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  78%|███████▊  | 260/335 [00:00<00:00, 422.48it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17:  78%|███████▊  | 260/335 [00:00<00:00, 422.48it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17:  90%|████████▉ | 300/335 [00:00<00:00, 455.21it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17:  90%|████████▉ | 300/335 [00:00<00:00, 455.21it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17:  90%|████████▉ | 300/335 [00:00<00:00, 455.21it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 17:  90%|████████▉ | 300/335 [00:00<00:00, 455.21it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00262]\n",
      "Epoch 17: 100%|██████████| 335/335 [00:00<00:00, 485.37it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]           \n",
      "Epoch 17: 100%|██████████| 335/335 [00:00<00:00, 485.37it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]           \n",
      "Epoch 17: 100%|██████████| 335/335 [00:00<00:00, 485.37it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]           \n",
      "Epoch 17: 100%|██████████| 335/335 [00:00<00:00, 485.37it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00272]           \n",
      "Epoch 18:   6%|▌         | 20/335 [00:00<00:00, 424.56it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   6%|▌         | 20/335 [00:00<00:00, 424.56it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   6%|▌         | 20/335 [00:00<00:00, 424.56it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:   6%|▌         | 20/335 [00:00<00:00, 424.56it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  12%|█▏        | 40/335 [00:00<00:00, 428.08it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  12%|█▏        | 40/335 [00:00<00:00, 428.08it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  12%|█▏        | 40/335 [00:00<00:00, 428.08it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  12%|█▏        | 40/335 [00:00<00:00, 428.08it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  18%|█▊        | 60/335 [00:00<00:00, 427.12it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  18%|█▊        | 60/335 [00:00<00:00, 427.12it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  18%|█▊        | 60/335 [00:00<00:00, 427.12it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  18%|█▊        | 60/335 [00:00<00:00, 427.12it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  24%|██▍       | 80/335 [00:00<00:00, 420.00it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  24%|██▍       | 80/335 [00:00<00:00, 420.00it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  24%|██▍       | 80/335 [00:00<00:00, 420.00it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  24%|██▍       | 80/335 [00:00<00:00, 420.00it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  30%|██▉       | 100/335 [00:00<00:00, 419.14it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  30%|██▉       | 100/335 [00:00<00:00, 419.14it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  30%|██▉       | 100/335 [00:00<00:00, 419.14it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  30%|██▉       | 100/335 [00:00<00:00, 419.14it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  42%|████▏     | 140/335 [00:00<00:00, 424.78it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  42%|████▏     | 140/335 [00:00<00:00, 424.78it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  42%|████▏     | 140/335 [00:00<00:00, 424.78it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  42%|████▏     | 140/335 [00:00<00:00, 424.78it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  48%|████▊     | 160/335 [00:00<00:00, 428.42it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  48%|████▊     | 160/335 [00:00<00:00, 428.42it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  48%|████▊     | 160/335 [00:00<00:00, 428.42it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  48%|████▊     | 160/335 [00:00<00:00, 428.42it/s, loss=0.0156, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  54%|█████▎    | 180/335 [00:00<00:00, 429.86it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  54%|█████▎    | 180/335 [00:00<00:00, 429.86it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  54%|█████▎    | 180/335 [00:00<00:00, 429.86it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  54%|█████▎    | 180/335 [00:00<00:00, 429.86it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  60%|█████▉    | 200/335 [00:00<00:00, 431.20it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  60%|█████▉    | 200/335 [00:00<00:00, 431.20it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  60%|█████▉    | 200/335 [00:00<00:00, 431.20it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  60%|█████▉    | 200/335 [00:00<00:00, 431.20it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  66%|██████▌   | 220/335 [00:00<00:00, 432.72it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  66%|██████▌   | 220/335 [00:00<00:00, 432.72it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  66%|██████▌   | 220/335 [00:00<00:00, 432.72it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  66%|██████▌   | 220/335 [00:00<00:00, 432.72it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  72%|███████▏  | 240/335 [00:00<00:00, 432.93it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  72%|███████▏  | 240/335 [00:00<00:00, 432.93it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  72%|███████▏  | 240/335 [00:00<00:00, 432.93it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  72%|███████▏  | 240/335 [00:00<00:00, 432.93it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00272] \n",
      "Epoch 18:  78%|███████▊  | 260/335 [00:00<00:00, 433.42it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  78%|███████▊  | 260/335 [00:00<00:00, 433.42it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  78%|███████▊  | 260/335 [00:00<00:00, 433.42it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  78%|███████▊  | 260/335 [00:00<00:00, 433.42it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  90%|████████▉ | 300/335 [00:00<00:00, 468.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  90%|████████▉ | 300/335 [00:00<00:00, 468.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  90%|████████▉ | 300/335 [00:00<00:00, 468.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18:  90%|████████▉ | 300/335 [00:00<00:00, 468.20it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00272]\n",
      "Epoch 18: 100%|██████████| 335/335 [00:00<00:00, 438.54it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]           \n",
      "Epoch 18: 100%|██████████| 335/335 [00:00<00:00, 438.54it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]           \n",
      "Epoch 18: 100%|██████████| 335/335 [00:00<00:00, 438.54it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]           \n",
      "Epoch 18: 100%|██████████| 335/335 [00:00<00:00, 438.54it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]           \n",
      "Epoch 19:   6%|▌         | 20/335 [00:00<00:00, 416.57it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   6%|▌         | 20/335 [00:00<00:00, 416.57it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   6%|▌         | 20/335 [00:00<00:00, 416.57it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:   6%|▌         | 20/335 [00:00<00:00, 416.57it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  12%|█▏        | 40/335 [00:00<00:00, 426.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  12%|█▏        | 40/335 [00:00<00:00, 426.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  12%|█▏        | 40/335 [00:00<00:00, 426.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  12%|█▏        | 40/335 [00:00<00:00, 426.09it/s, loss=0.0154, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  18%|█▊        | 60/335 [00:00<00:00, 404.36it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  18%|█▊        | 60/335 [00:00<00:00, 404.36it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  18%|█▊        | 60/335 [00:00<00:00, 404.36it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  18%|█▊        | 60/335 [00:00<00:00, 404.36it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  24%|██▍       | 80/335 [00:00<00:00, 412.74it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  24%|██▍       | 80/335 [00:00<00:00, 412.74it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  24%|██▍       | 80/335 [00:00<00:00, 412.74it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  24%|██▍       | 80/335 [00:00<00:00, 412.74it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  30%|██▉       | 100/335 [00:00<00:00, 419.52it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  30%|██▉       | 100/335 [00:00<00:00, 419.52it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  30%|██▉       | 100/335 [00:00<00:00, 419.52it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  30%|██▉       | 100/335 [00:00<00:00, 419.52it/s, loss=0.0143, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  42%|████▏     | 140/335 [00:00<00:00, 426.49it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  42%|████▏     | 140/335 [00:00<00:00, 426.49it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  42%|████▏     | 140/335 [00:00<00:00, 426.49it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  42%|████▏     | 140/335 [00:00<00:00, 426.49it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  48%|████▊     | 160/335 [00:00<00:00, 429.07it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  48%|████▊     | 160/335 [00:00<00:00, 429.07it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  48%|████▊     | 160/335 [00:00<00:00, 429.07it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  48%|████▊     | 160/335 [00:00<00:00, 429.07it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  54%|█████▎    | 180/335 [00:00<00:00, 431.71it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  54%|█████▎    | 180/335 [00:00<00:00, 431.71it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  54%|█████▎    | 180/335 [00:00<00:00, 431.71it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  54%|█████▎    | 180/335 [00:00<00:00, 431.71it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  60%|█████▉    | 200/335 [00:00<00:00, 433.49it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  60%|█████▉    | 200/335 [00:00<00:00, 433.49it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  60%|█████▉    | 200/335 [00:00<00:00, 433.49it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  60%|█████▉    | 200/335 [00:00<00:00, 433.49it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  66%|██████▌   | 220/335 [00:00<00:00, 432.80it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  66%|██████▌   | 220/335 [00:00<00:00, 432.80it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  66%|██████▌   | 220/335 [00:00<00:00, 432.80it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  66%|██████▌   | 220/335 [00:00<00:00, 432.80it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  72%|███████▏  | 240/335 [00:00<00:00, 434.87it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  72%|███████▏  | 240/335 [00:00<00:00, 434.87it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  72%|███████▏  | 240/335 [00:00<00:00, 434.87it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  72%|███████▏  | 240/335 [00:00<00:00, 434.87it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  78%|███████▊  | 260/335 [00:00<00:00, 433.10it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  78%|███████▊  | 260/335 [00:00<00:00, 433.10it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  78%|███████▊  | 260/335 [00:00<00:00, 433.10it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  78%|███████▊  | 260/335 [00:00<00:00, 433.10it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  90%|████████▉ | 300/335 [00:00<00:00, 467.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  90%|████████▉ | 300/335 [00:00<00:00, 467.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  90%|████████▉ | 300/335 [00:00<00:00, 467.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19:  90%|████████▉ | 300/335 [00:00<00:00, 467.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00243]\n",
      "Epoch 19: 100%|██████████| 335/335 [00:00<00:00, 437.02it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023]           \n",
      "Epoch 19: 100%|██████████| 335/335 [00:00<00:00, 437.02it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023]           \n",
      "Epoch 19: 100%|██████████| 335/335 [00:00<00:00, 437.02it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023]           \n",
      "Epoch 19: 100%|██████████| 335/335 [00:00<00:00, 437.02it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0023]           \n",
      "Epoch 20:   6%|▌         | 20/335 [00:00<00:00, 434.79it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:   6%|▌         | 20/335 [00:00<00:00, 434.79it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:   6%|▌         | 20/335 [00:00<00:00, 434.79it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:   6%|▌         | 20/335 [00:00<00:00, 434.79it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  12%|█▏        | 40/335 [00:00<00:00, 442.20it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  12%|█▏        | 40/335 [00:00<00:00, 442.20it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  12%|█▏        | 40/335 [00:00<00:00, 442.20it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  12%|█▏        | 40/335 [00:00<00:00, 442.20it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  18%|█▊        | 60/335 [00:00<00:00, 439.65it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  18%|█▊        | 60/335 [00:00<00:00, 439.65it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  18%|█▊        | 60/335 [00:00<00:00, 439.65it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  18%|█▊        | 60/335 [00:00<00:00, 439.65it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  24%|██▍       | 80/335 [00:00<00:00, 438.53it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  24%|██▍       | 80/335 [00:00<00:00, 438.53it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  24%|██▍       | 80/335 [00:00<00:00, 438.53it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  24%|██▍       | 80/335 [00:00<00:00, 438.53it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  30%|██▉       | 100/335 [00:00<00:00, 434.46it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  30%|██▉       | 100/335 [00:00<00:00, 434.46it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  30%|██▉       | 100/335 [00:00<00:00, 434.46it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  30%|██▉       | 100/335 [00:00<00:00, 434.46it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  42%|████▏     | 140/335 [00:00<00:00, 433.96it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  42%|████▏     | 140/335 [00:00<00:00, 433.96it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  42%|████▏     | 140/335 [00:00<00:00, 433.96it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  42%|████▏     | 140/335 [00:00<00:00, 433.96it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  48%|████▊     | 160/335 [00:00<00:00, 432.17it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  48%|████▊     | 160/335 [00:00<00:00, 432.17it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  48%|████▊     | 160/335 [00:00<00:00, 432.17it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  48%|████▊     | 160/335 [00:00<00:00, 432.17it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  54%|█████▎    | 180/335 [00:00<00:00, 432.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  54%|█████▎    | 180/335 [00:00<00:00, 432.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  54%|█████▎    | 180/335 [00:00<00:00, 432.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  54%|█████▎    | 180/335 [00:00<00:00, 432.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  60%|█████▉    | 200/335 [00:00<00:00, 430.24it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  60%|█████▉    | 200/335 [00:00<00:00, 430.24it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  60%|█████▉    | 200/335 [00:00<00:00, 430.24it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  60%|█████▉    | 200/335 [00:00<00:00, 430.24it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  66%|██████▌   | 220/335 [00:00<00:00, 431.78it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:  66%|██████▌   | 220/335 [00:00<00:00, 431.78it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:  66%|██████▌   | 220/335 [00:00<00:00, 431.78it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:  66%|██████▌   | 220/335 [00:00<00:00, 431.78it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Epoch 20:  72%|███████▏  | 240/335 [00:00<00:00, 432.19it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  72%|███████▏  | 240/335 [00:00<00:00, 432.19it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  72%|███████▏  | 240/335 [00:00<00:00, 432.19it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  72%|███████▏  | 240/335 [00:00<00:00, 432.19it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  78%|███████▊  | 260/335 [00:00<00:00, 432.88it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  78%|███████▊  | 260/335 [00:00<00:00, 432.88it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  78%|███████▊  | 260/335 [00:00<00:00, 432.88it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  78%|███████▊  | 260/335 [00:00<00:00, 432.88it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  90%|████████▉ | 300/335 [00:00<00:00, 467.23it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  90%|████████▉ | 300/335 [00:00<00:00, 467.23it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  90%|████████▉ | 300/335 [00:00<00:00, 467.23it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20:  90%|████████▉ | 300/335 [00:00<00:00, 467.23it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0023]\n",
      "Epoch 20: 100%|██████████| 335/335 [00:00<00:00, 438.68it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]           \n",
      "Epoch 20: 100%|██████████| 335/335 [00:00<00:00, 438.68it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]           \n",
      "Epoch 20: 100%|██████████| 335/335 [00:00<00:00, 438.68it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]           \n",
      "Epoch 20: 100%|██████████| 335/335 [00:00<00:00, 438.68it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00181]           \n",
      "Epoch 21:   6%|▌         | 20/335 [00:00<00:00, 416.77it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   6%|▌         | 20/335 [00:00<00:00, 416.77it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   6%|▌         | 20/335 [00:00<00:00, 416.77it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:   6%|▌         | 20/335 [00:00<00:00, 416.77it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  12%|█▏        | 40/335 [00:00<00:00, 419.31it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  12%|█▏        | 40/335 [00:00<00:00, 419.31it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  12%|█▏        | 40/335 [00:00<00:00, 419.31it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  12%|█▏        | 40/335 [00:00<00:00, 419.31it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  18%|█▊        | 60/335 [00:00<00:00, 425.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  18%|█▊        | 60/335 [00:00<00:00, 425.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  18%|█▊        | 60/335 [00:00<00:00, 425.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  18%|█▊        | 60/335 [00:00<00:00, 425.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  24%|██▍       | 80/335 [00:00<00:00, 425.38it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  24%|██▍       | 80/335 [00:00<00:00, 425.38it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  24%|██▍       | 80/335 [00:00<00:00, 425.38it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  24%|██▍       | 80/335 [00:00<00:00, 425.38it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  30%|██▉       | 100/335 [00:00<00:00, 424.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  30%|██▉       | 100/335 [00:00<00:00, 424.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  30%|██▉       | 100/335 [00:00<00:00, 424.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  30%|██▉       | 100/335 [00:00<00:00, 424.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  36%|███▌      | 120/335 [00:00<00:00, 426.37it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  36%|███▌      | 120/335 [00:00<00:00, 426.37it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  36%|███▌      | 120/335 [00:00<00:00, 426.37it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  36%|███▌      | 120/335 [00:00<00:00, 426.37it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  48%|████▊     | 160/335 [00:00<00:00, 427.69it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  48%|████▊     | 160/335 [00:00<00:00, 427.69it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  48%|████▊     | 160/335 [00:00<00:00, 427.69it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  48%|████▊     | 160/335 [00:00<00:00, 427.69it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  54%|█████▎    | 180/335 [00:00<00:00, 427.30it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  54%|█████▎    | 180/335 [00:00<00:00, 427.30it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  54%|█████▎    | 180/335 [00:00<00:00, 427.30it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  54%|█████▎    | 180/335 [00:00<00:00, 427.30it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  60%|█████▉    | 200/335 [00:00<00:00, 427.86it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  60%|█████▉    | 200/335 [00:00<00:00, 427.86it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  60%|█████▉    | 200/335 [00:00<00:00, 427.86it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  60%|█████▉    | 200/335 [00:00<00:00, 427.86it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  66%|██████▌   | 220/335 [00:00<00:00, 428.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  66%|██████▌   | 220/335 [00:00<00:00, 428.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  66%|██████▌   | 220/335 [00:00<00:00, 428.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  66%|██████▌   | 220/335 [00:00<00:00, 428.88it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  72%|███████▏  | 240/335 [00:00<00:00, 428.56it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  72%|███████▏  | 240/335 [00:00<00:00, 428.56it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  72%|███████▏  | 240/335 [00:00<00:00, 428.56it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  72%|███████▏  | 240/335 [00:00<00:00, 428.56it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00181]\n",
      "Epoch 21:  78%|███████▊  | 260/335 [00:00<00:00, 428.39it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  78%|███████▊  | 260/335 [00:00<00:00, 428.39it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  78%|███████▊  | 260/335 [00:00<00:00, 428.39it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00181] \n",
      "Epoch 21:  78%|███████▊  | 260/335 [00:00<00:00, 428.39it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00181] \n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 335/335 [00:00<00:00, 493.82it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 335/335 [00:00<00:00, 493.82it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 335/335 [00:00<00:00, 493.82it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]\n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 335/335 [00:00<00:00, 493.82it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]\n",
      "                                                  \u001B[A\n",
      "Epoch 22:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]           \n",
      "Epoch 22:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]           \n",
      "Epoch 22:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]           \n",
      "Epoch 22:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.0026]           \n",
      "Epoch 22:   6%|▌         | 20/335 [00:00<00:00, 423.95it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:   6%|▌         | 20/335 [00:00<00:00, 423.95it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:   6%|▌         | 20/335 [00:00<00:00, 423.95it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:   6%|▌         | 20/335 [00:00<00:00, 423.95it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  12%|█▏        | 40/335 [00:00<00:00, 431.52it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  12%|█▏        | 40/335 [00:00<00:00, 431.52it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  12%|█▏        | 40/335 [00:00<00:00, 431.52it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  12%|█▏        | 40/335 [00:00<00:00, 431.52it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  18%|█▊        | 60/335 [00:00<00:00, 440.15it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  18%|█▊        | 60/335 [00:00<00:00, 440.15it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  18%|█▊        | 60/335 [00:00<00:00, 440.15it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  18%|█▊        | 60/335 [00:00<00:00, 440.15it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  24%|██▍       | 80/335 [00:00<00:00, 444.51it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  24%|██▍       | 80/335 [00:00<00:00, 444.51it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  24%|██▍       | 80/335 [00:00<00:00, 444.51it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  24%|██▍       | 80/335 [00:00<00:00, 444.51it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  30%|██▉       | 100/335 [00:00<00:00, 446.52it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  30%|██▉       | 100/335 [00:00<00:00, 446.52it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  30%|██▉       | 100/335 [00:00<00:00, 446.52it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  30%|██▉       | 100/335 [00:00<00:00, 446.52it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  36%|███▌      | 120/335 [00:00<00:00, 448.38it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  36%|███▌      | 120/335 [00:00<00:00, 448.38it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  36%|███▌      | 120/335 [00:00<00:00, 448.38it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  36%|███▌      | 120/335 [00:00<00:00, 448.38it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  54%|█████▎    | 180/335 [00:00<00:00, 453.14it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  54%|█████▎    | 180/335 [00:00<00:00, 453.14it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  54%|█████▎    | 180/335 [00:00<00:00, 453.14it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  54%|█████▎    | 180/335 [00:00<00:00, 453.14it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  60%|█████▉    | 200/335 [00:00<00:00, 454.01it/s, loss=0.00951, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  60%|█████▉    | 200/335 [00:00<00:00, 454.01it/s, loss=0.00951, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  60%|█████▉    | 200/335 [00:00<00:00, 454.01it/s, loss=0.00951, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  60%|█████▉    | 200/335 [00:00<00:00, 454.01it/s, loss=0.00951, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  66%|██████▌   | 220/335 [00:00<00:00, 454.32it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  66%|██████▌   | 220/335 [00:00<00:00, 454.32it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  66%|██████▌   | 220/335 [00:00<00:00, 454.32it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  66%|██████▌   | 220/335 [00:00<00:00, 454.32it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.0026] \n",
      "Epoch 22:  72%|███████▏  | 240/335 [00:00<00:00, 456.14it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  72%|███████▏  | 240/335 [00:00<00:00, 456.14it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  72%|███████▏  | 240/335 [00:00<00:00, 456.14it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  72%|███████▏  | 240/335 [00:00<00:00, 456.14it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  78%|███████▊  | 260/335 [00:00<00:00, 456.30it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22:  90%|████████▉ | 300/335 [00:00<00:00, 493.20it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  78%|███████▊  | 260/335 [00:00<00:00, 456.30it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22:  90%|████████▉ | 300/335 [00:00<00:00, 493.20it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  78%|███████▊  | 260/335 [00:00<00:00, 456.30it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22:  90%|████████▉ | 300/335 [00:00<00:00, 493.20it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22:  78%|███████▊  | 260/335 [00:00<00:00, 456.30it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 22:  90%|████████▉ | 300/335 [00:00<00:00, 493.20it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.0026]\n",
      "Epoch 22: 100%|██████████| 335/335 [00:00<00:00, 525.15it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]           \n",
      "Epoch 22: 100%|██████████| 335/335 [00:00<00:00, 525.15it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]           \n",
      "Epoch 22: 100%|██████████| 335/335 [00:00<00:00, 525.15it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]           \n",
      "Epoch 22: 100%|██████████| 335/335 [00:00<00:00, 525.15it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0109, v_num=0.0, ptl/val_loss=0.00271]           \n",
      "Epoch 23:   6%|▌         | 20/335 [00:00<00:00, 468.38it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   6%|▌         | 20/335 [00:00<00:00, 468.38it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   6%|▌         | 20/335 [00:00<00:00, 468.38it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:   6%|▌         | 20/335 [00:00<00:00, 468.38it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  12%|█▏        | 40/335 [00:00<00:00, 467.15it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  12%|█▏        | 40/335 [00:00<00:00, 467.15it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  12%|█▏        | 40/335 [00:00<00:00, 467.15it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  12%|█▏        | 40/335 [00:00<00:00, 467.15it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  24%|██▍       | 80/335 [00:00<00:00, 461.16it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  24%|██▍       | 80/335 [00:00<00:00, 461.16it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  24%|██▍       | 80/335 [00:00<00:00, 461.16it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  24%|██▍       | 80/335 [00:00<00:00, 461.16it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  30%|██▉       | 100/335 [00:00<00:00, 456.75it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  30%|██▉       | 100/335 [00:00<00:00, 456.75it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  30%|██▉       | 100/335 [00:00<00:00, 456.75it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  30%|██▉       | 100/335 [00:00<00:00, 456.75it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  36%|███▌      | 120/335 [00:00<00:00, 456.79it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  36%|███▌      | 120/335 [00:00<00:00, 456.79it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  36%|███▌      | 120/335 [00:00<00:00, 456.79it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  36%|███▌      | 120/335 [00:00<00:00, 456.79it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  42%|████▏     | 140/335 [00:00<00:00, 456.31it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  42%|████▏     | 140/335 [00:00<00:00, 456.31it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  42%|████▏     | 140/335 [00:00<00:00, 456.31it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  42%|████▏     | 140/335 [00:00<00:00, 456.31it/s, loss=0.0104, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  48%|████▊     | 160/335 [00:00<00:00, 455.70it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  48%|████▊     | 160/335 [00:00<00:00, 455.70it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  48%|████▊     | 160/335 [00:00<00:00, 455.70it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  48%|████▊     | 160/335 [00:00<00:00, 455.70it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  54%|█████▎    | 180/335 [00:00<00:00, 454.90it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  54%|█████▎    | 180/335 [00:00<00:00, 454.90it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  54%|█████▎    | 180/335 [00:00<00:00, 454.90it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  54%|█████▎    | 180/335 [00:00<00:00, 454.90it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  66%|██████▌   | 220/335 [00:00<00:00, 453.78it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  66%|██████▌   | 220/335 [00:00<00:00, 453.78it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  66%|██████▌   | 220/335 [00:00<00:00, 453.78it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  66%|██████▌   | 220/335 [00:00<00:00, 453.78it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  72%|███████▏  | 240/335 [00:00<00:00, 453.65it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  72%|███████▏  | 240/335 [00:00<00:00, 453.65it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  72%|███████▏  | 240/335 [00:00<00:00, 453.65it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  72%|███████▏  | 240/335 [00:00<00:00, 453.65it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00271] \n",
      "Epoch 23:  78%|███████▊  | 260/335 [00:00<00:00, 453.02it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  78%|███████▊  | 260/335 [00:00<00:00, 453.02it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  78%|███████▊  | 260/335 [00:00<00:00, 453.02it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23:  78%|███████▊  | 260/335 [00:00<00:00, 453.02it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23:  90%|████████▉ | 300/335 [00:00<00:00, 488.66it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23:  90%|████████▉ | 300/335 [00:00<00:00, 488.66it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23:  90%|████████▉ | 300/335 [00:00<00:00, 488.66it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 23:  90%|████████▉ | 300/335 [00:00<00:00, 488.66it/s, loss=0.00996, v_num=0.0, ptl/val_loss=0.00271]\n",
      "Epoch 23: 100%|██████████| 335/335 [00:00<00:00, 454.60it/s, loss=0.00981, v_num=0.0, ptl/val_loss=0.0018] \n",
      "                                                  \u001B[A\n",
      "Epoch 23: 100%|██████████| 335/335 [00:00<00:00, 454.60it/s, loss=0.00981, v_num=0.0, ptl/val_loss=0.0018] \n",
      "                                                  \u001B[A\n",
      "Epoch 23: 100%|██████████| 335/335 [00:00<00:00, 454.60it/s, loss=0.00981, v_num=0.0, ptl/val_loss=0.0018] \n",
      "                                                  \u001B[A\n",
      "Epoch 23: 100%|██████████| 335/335 [00:00<00:00, 454.60it/s, loss=0.00981, v_num=0.0, ptl/val_loss=0.0018] \n",
      "                                                  \u001B[A\n",
      "Epoch 24:   6%|▌         | 20/335 [00:00<00:00, 428.00it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0018]   \n",
      "Epoch 24:   6%|▌         | 20/335 [00:00<00:00, 428.00it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0018]   \n",
      "Epoch 24:   6%|▌         | 20/335 [00:00<00:00, 428.00it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0018]   \n",
      "Epoch 24:   6%|▌         | 20/335 [00:00<00:00, 428.00it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.0018]   \n",
      "Epoch 24:  12%|█▏        | 40/335 [00:00<00:00, 436.55it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  12%|█▏        | 40/335 [00:00<00:00, 436.55it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  12%|█▏        | 40/335 [00:00<00:00, 436.55it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  12%|█▏        | 40/335 [00:00<00:00, 436.55it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  18%|█▊        | 60/335 [00:00<00:00, 440.19it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  18%|█▊        | 60/335 [00:00<00:00, 440.19it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  18%|█▊        | 60/335 [00:00<00:00, 440.19it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  18%|█▊        | 60/335 [00:00<00:00, 440.19it/s, loss=0.0115, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  24%|██▍       | 80/335 [00:00<00:00, 436.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  24%|██▍       | 80/335 [00:00<00:00, 436.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  24%|██▍       | 80/335 [00:00<00:00, 436.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  24%|██▍       | 80/335 [00:00<00:00, 436.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  30%|██▉       | 100/335 [00:00<00:00, 438.45it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  30%|██▉       | 100/335 [00:00<00:00, 438.45it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  30%|██▉       | 100/335 [00:00<00:00, 438.45it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  30%|██▉       | 100/335 [00:00<00:00, 438.45it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  36%|███▌      | 120/335 [00:00<00:00, 435.82it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  36%|███▌      | 120/335 [00:00<00:00, 435.82it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  36%|███▌      | 120/335 [00:00<00:00, 435.82it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  36%|███▌      | 120/335 [00:00<00:00, 435.82it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  42%|████▏     | 140/335 [00:00<00:00, 435.31it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  42%|████▏     | 140/335 [00:00<00:00, 435.31it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  42%|████▏     | 140/335 [00:00<00:00, 435.31it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  42%|████▏     | 140/335 [00:00<00:00, 435.31it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  48%|████▊     | 160/335 [00:00<00:00, 432.37it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  48%|████▊     | 160/335 [00:00<00:00, 432.37it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  48%|████▊     | 160/335 [00:00<00:00, 432.37it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  48%|████▊     | 160/335 [00:00<00:00, 432.37it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  54%|█████▎    | 180/335 [00:00<00:00, 431.63it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  54%|█████▎    | 180/335 [00:00<00:00, 431.63it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  54%|█████▎    | 180/335 [00:00<00:00, 431.63it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  54%|█████▎    | 180/335 [00:00<00:00, 431.63it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  60%|█████▉    | 200/335 [00:00<00:00, 433.38it/s, loss=0.00898, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  60%|█████▉    | 200/335 [00:00<00:00, 433.38it/s, loss=0.00898, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  60%|█████▉    | 200/335 [00:00<00:00, 433.38it/s, loss=0.00898, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  60%|█████▉    | 200/335 [00:00<00:00, 433.38it/s, loss=0.00898, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  66%|██████▌   | 220/335 [00:00<00:00, 432.47it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0018] \n",
      "Epoch 24:  66%|██████▌   | 220/335 [00:00<00:00, 432.47it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0018] \n",
      "Epoch 24:  66%|██████▌   | 220/335 [00:00<00:00, 432.47it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0018] \n",
      "Epoch 24:  66%|██████▌   | 220/335 [00:00<00:00, 432.47it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0018] \n",
      "Epoch 24:  72%|███████▏  | 240/335 [00:00<00:00, 432.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  72%|███████▏  | 240/335 [00:00<00:00, 432.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  72%|███████▏  | 240/335 [00:00<00:00, 432.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  72%|███████▏  | 240/335 [00:00<00:00, 432.84it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  78%|███████▊  | 260/335 [00:00<00:00, 433.17it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  78%|███████▊  | 260/335 [00:00<00:00, 433.17it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  78%|███████▊  | 260/335 [00:00<00:00, 433.17it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24:  78%|███████▊  | 260/335 [00:00<00:00, 433.17it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24:  90%|████████▉ | 300/335 [00:00<00:00, 466.92it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24:  90%|████████▉ | 300/335 [00:00<00:00, 466.92it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24:  90%|████████▉ | 300/335 [00:00<00:00, 466.92it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 24:  90%|████████▉ | 300/335 [00:00<00:00, 466.92it/s, loss=0.0098, v_num=0.0, ptl/val_loss=0.0018]\n",
      "Epoch 24: 100%|██████████| 335/335 [00:00<00:00, 496.38it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]           \n",
      "Epoch 24: 100%|██████████| 335/335 [00:00<00:00, 496.38it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]           \n",
      "Epoch 24: 100%|██████████| 335/335 [00:00<00:00, 496.38it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]           \n",
      "Epoch 24: 100%|██████████| 335/335 [00:00<00:00, 496.38it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]           \n",
      "Epoch 25:   6%|▌         | 20/335 [00:00<00:00, 443.58it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   6%|▌         | 20/335 [00:00<00:00, 443.58it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   6%|▌         | 20/335 [00:00<00:00, 443.58it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:   6%|▌         | 20/335 [00:00<00:00, 443.58it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  12%|█▏        | 40/335 [00:00<00:00, 446.51it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  12%|█▏        | 40/335 [00:00<00:00, 446.51it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  12%|█▏        | 40/335 [00:00<00:00, 446.51it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  12%|█▏        | 40/335 [00:00<00:00, 446.51it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  18%|█▊        | 60/335 [00:00<00:00, 449.33it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  18%|█▊        | 60/335 [00:00<00:00, 449.33it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  18%|█▊        | 60/335 [00:00<00:00, 449.33it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  18%|█▊        | 60/335 [00:00<00:00, 449.33it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  24%|██▍       | 80/335 [00:00<00:00, 446.56it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  24%|██▍       | 80/335 [00:00<00:00, 446.56it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  24%|██▍       | 80/335 [00:00<00:00, 446.56it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  24%|██▍       | 80/335 [00:00<00:00, 446.56it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  30%|██▉       | 100/335 [00:00<00:00, 433.38it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  30%|██▉       | 100/335 [00:00<00:00, 433.38it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  30%|██▉       | 100/335 [00:00<00:00, 433.38it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  30%|██▉       | 100/335 [00:00<00:00, 433.38it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  36%|███▌      | 120/335 [00:00<00:00, 434.37it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  36%|███▌      | 120/335 [00:00<00:00, 434.37it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  36%|███▌      | 120/335 [00:00<00:00, 434.37it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  36%|███▌      | 120/335 [00:00<00:00, 434.37it/s, loss=0.00966, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  42%|████▏     | 140/335 [00:00<00:00, 434.20it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  42%|████▏     | 140/335 [00:00<00:00, 434.20it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  42%|████▏     | 140/335 [00:00<00:00, 434.20it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  42%|████▏     | 140/335 [00:00<00:00, 434.20it/s, loss=0.00937, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  48%|████▊     | 160/335 [00:00<00:00, 435.02it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  48%|████▊     | 160/335 [00:00<00:00, 435.02it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  48%|████▊     | 160/335 [00:00<00:00, 435.02it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  48%|████▊     | 160/335 [00:00<00:00, 435.02it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00185] \n",
      "Epoch 25:  54%|█████▎    | 180/335 [00:00<00:00, 435.00it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00185]  \n",
      "Epoch 25:  54%|█████▎    | 180/335 [00:00<00:00, 435.00it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00185]  \n",
      "Epoch 25:  54%|█████▎    | 180/335 [00:00<00:00, 435.00it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00185]  \n",
      "Epoch 25:  54%|█████▎    | 180/335 [00:00<00:00, 435.00it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00185]  \n",
      "Epoch 25:  60%|█████▉    | 200/335 [00:00<00:00, 434.73it/s, loss=0.00879, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  60%|█████▉    | 200/335 [00:00<00:00, 434.73it/s, loss=0.00879, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  60%|█████▉    | 200/335 [00:00<00:00, 434.73it/s, loss=0.00879, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  60%|█████▉    | 200/335 [00:00<00:00, 434.73it/s, loss=0.00879, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  72%|███████▏  | 240/335 [00:00<00:00, 437.78it/s, loss=0.00984, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  72%|███████▏  | 240/335 [00:00<00:00, 437.78it/s, loss=0.00984, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  72%|███████▏  | 240/335 [00:00<00:00, 437.78it/s, loss=0.00984, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  72%|███████▏  | 240/335 [00:00<00:00, 437.78it/s, loss=0.00984, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  78%|███████▊  | 260/335 [00:00<00:00, 437.51it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  78%|███████▊  | 260/335 [00:00<00:00, 437.51it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  78%|███████▊  | 260/335 [00:00<00:00, 437.51it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25:  78%|███████▊  | 260/335 [00:00<00:00, 437.51it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25:  90%|████████▉ | 300/335 [00:00<00:00, 471.40it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25:  90%|████████▉ | 300/335 [00:00<00:00, 471.40it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25:  90%|████████▉ | 300/335 [00:00<00:00, 471.40it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 25:  90%|████████▉ | 300/335 [00:00<00:00, 471.40it/s, loss=0.00921, v_num=0.0, ptl/val_loss=0.00185]\n",
      "Epoch 25: 100%|██████████| 335/335 [00:00<00:00, 441.36it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 25: 100%|██████████| 335/335 [00:00<00:00, 441.36it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 25: 100%|██████████| 335/335 [00:00<00:00, 441.36it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 25: 100%|██████████| 335/335 [00:00<00:00, 441.36it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00963, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 26:  12%|█▏        | 40/335 [00:00<00:00, 434.97it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  12%|█▏        | 40/335 [00:00<00:00, 434.97it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  12%|█▏        | 40/335 [00:00<00:00, 434.97it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  12%|█▏        | 40/335 [00:00<00:00, 434.97it/s, loss=0.0117, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  18%|█▊        | 60/335 [00:00<00:00, 441.86it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  18%|█▊        | 60/335 [00:00<00:00, 441.86it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  18%|█▊        | 60/335 [00:00<00:00, 441.86it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  18%|█▊        | 60/335 [00:00<00:00, 441.86it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  24%|██▍       | 80/335 [00:00<00:00, 442.83it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  24%|██▍       | 80/335 [00:00<00:00, 442.83it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  24%|██▍       | 80/335 [00:00<00:00, 442.83it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  24%|██▍       | 80/335 [00:00<00:00, 442.83it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  30%|██▉       | 100/335 [00:00<00:00, 442.65it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  30%|██▉       | 100/335 [00:00<00:00, 442.65it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  30%|██▉       | 100/335 [00:00<00:00, 442.65it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  30%|██▉       | 100/335 [00:00<00:00, 442.65it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  36%|███▌      | 120/335 [00:00<00:00, 442.43it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  36%|███▌      | 120/335 [00:00<00:00, 442.43it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  36%|███▌      | 120/335 [00:00<00:00, 442.43it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  36%|███▌      | 120/335 [00:00<00:00, 442.43it/s, loss=0.00973, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  42%|████▏     | 140/335 [00:00<00:00, 442.32it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  42%|████▏     | 140/335 [00:00<00:00, 442.32it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  42%|████▏     | 140/335 [00:00<00:00, 442.32it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  42%|████▏     | 140/335 [00:00<00:00, 442.32it/s, loss=0.00911, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  48%|████▊     | 160/335 [00:00<00:00, 443.02it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  48%|████▊     | 160/335 [00:00<00:00, 443.02it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  48%|████▊     | 160/335 [00:00<00:00, 443.02it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  48%|████▊     | 160/335 [00:00<00:00, 443.02it/s, loss=0.0107, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  60%|█████▉    | 200/335 [00:00<00:00, 443.44it/s, loss=0.00864, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  60%|█████▉    | 200/335 [00:00<00:00, 443.44it/s, loss=0.00864, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  60%|█████▉    | 200/335 [00:00<00:00, 443.44it/s, loss=0.00864, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  60%|█████▉    | 200/335 [00:00<00:00, 443.44it/s, loss=0.00864, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  66%|██████▌   | 220/335 [00:00<00:00, 444.64it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  66%|██████▌   | 220/335 [00:00<00:00, 444.64it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  66%|██████▌   | 220/335 [00:00<00:00, 444.64it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  66%|██████▌   | 220/335 [00:00<00:00, 444.64it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 26:  72%|███████▏  | 240/335 [00:00<00:00, 445.09it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  72%|███████▏  | 240/335 [00:00<00:00, 445.09it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  72%|███████▏  | 240/335 [00:00<00:00, 445.09it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  72%|███████▏  | 240/335 [00:00<00:00, 445.09it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  78%|███████▊  | 260/335 [00:00<00:00, 445.86it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  78%|███████▊  | 260/335 [00:00<00:00, 445.86it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  78%|███████▊  | 260/335 [00:00<00:00, 445.86it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26:  78%|███████▊  | 260/335 [00:00<00:00, 445.86it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26:  90%|████████▉ | 300/335 [00:00<00:00, 480.49it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26:  90%|████████▉ | 300/335 [00:00<00:00, 480.49it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26:  90%|████████▉ | 300/335 [00:00<00:00, 480.49it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "\u001B[2m\u001B[36m(pid=1892870)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 26:  90%|████████▉ | 300/335 [00:00<00:00, 480.49it/s, loss=0.00873, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 26: 100%|██████████| 335/335 [00:00<00:00, 510.58it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 26: 100%|██████████| 335/335 [00:00<00:00, 510.58it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 26: 100%|██████████| 335/335 [00:00<00:00, 510.58it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 26: 100%|██████████| 335/335 [00:00<00:00, 510.58it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00895, v_num=0.0, ptl/val_loss=0.00173]           \n",
      "Epoch 27:  12%|█▏        | 40/335 [00:00<00:00, 440.42it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  12%|█▏        | 40/335 [00:00<00:00, 440.42it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  12%|█▏        | 40/335 [00:00<00:00, 440.42it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  12%|█▏        | 40/335 [00:00<00:00, 440.42it/s, loss=0.0116, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  18%|█▊        | 60/335 [00:00<00:00, 443.09it/s, loss=0.00982, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  18%|█▊        | 60/335 [00:00<00:00, 443.09it/s, loss=0.00982, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  18%|█▊        | 60/335 [00:00<00:00, 443.09it/s, loss=0.00982, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  18%|█▊        | 60/335 [00:00<00:00, 443.09it/s, loss=0.00982, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  24%|██▍       | 80/335 [00:00<00:00, 444.99it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00173]  \n",
      "Epoch 27:  24%|██▍       | 80/335 [00:00<00:00, 444.99it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00173]  \n",
      "Epoch 27:  24%|██▍       | 80/335 [00:00<00:00, 444.99it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00173]  \n",
      "Epoch 27:  24%|██▍       | 80/335 [00:00<00:00, 444.99it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00173]  \n",
      "Epoch 27:  30%|██▉       | 100/335 [00:00<00:00, 448.04it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  30%|██▉       | 100/335 [00:00<00:00, 448.04it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  30%|██▉       | 100/335 [00:00<00:00, 448.04it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  30%|██▉       | 100/335 [00:00<00:00, 448.04it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  36%|███▌      | 120/335 [00:00<00:00, 447.19it/s, loss=0.00856, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  36%|███▌      | 120/335 [00:00<00:00, 447.19it/s, loss=0.00856, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  36%|███▌      | 120/335 [00:00<00:00, 447.19it/s, loss=0.00856, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  36%|███▌      | 120/335 [00:00<00:00, 447.19it/s, loss=0.00856, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  42%|████▏     | 140/335 [00:00<00:00, 448.53it/s, loss=0.00922, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  42%|████▏     | 140/335 [00:00<00:00, 448.53it/s, loss=0.00922, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  42%|████▏     | 140/335 [00:00<00:00, 448.53it/s, loss=0.00922, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  42%|████▏     | 140/335 [00:00<00:00, 448.53it/s, loss=0.00922, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  54%|█████▎    | 180/335 [00:00<00:00, 447.68it/s, loss=0.00968, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  54%|█████▎    | 180/335 [00:00<00:00, 447.68it/s, loss=0.00968, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  54%|█████▎    | 180/335 [00:00<00:00, 447.68it/s, loss=0.00968, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  54%|█████▎    | 180/335 [00:00<00:00, 447.68it/s, loss=0.00968, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  60%|█████▉    | 200/335 [00:00<00:00, 446.94it/s, loss=0.0091, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  60%|█████▉    | 200/335 [00:00<00:00, 446.94it/s, loss=0.0091, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  60%|█████▉    | 200/335 [00:00<00:00, 446.94it/s, loss=0.0091, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  60%|█████▉    | 200/335 [00:00<00:00, 446.94it/s, loss=0.0091, v_num=0.0, ptl/val_loss=0.00173] \n",
      "Epoch 27:  66%|██████▌   | 220/335 [00:00<00:00, 447.94it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  66%|██████▌   | 220/335 [00:00<00:00, 447.94it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  66%|██████▌   | 220/335 [00:00<00:00, 447.94it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  66%|██████▌   | 220/335 [00:00<00:00, 447.94it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  72%|███████▏  | 240/335 [00:00<00:00, 448.32it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  72%|███████▏  | 240/335 [00:00<00:00, 448.32it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  72%|███████▏  | 240/335 [00:00<00:00, 448.32it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  72%|███████▏  | 240/335 [00:00<00:00, 448.32it/s, loss=0.0103, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Epoch 27:  78%|███████▊  | 260/335 [00:00<00:00, 448.84it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  78%|███████▊  | 260/335 [00:00<00:00, 448.84it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  78%|███████▊  | 260/335 [00:00<00:00, 448.84it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  78%|███████▊  | 260/335 [00:00<00:00, 448.84it/s, loss=0.00931, v_num=0.0, ptl/val_loss=0.00173]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 335/335 [00:00<00:00, 515.13it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]           \n",
      "Epoch 27: 100%|██████████| 335/335 [00:00<00:00, 515.13it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]           \n",
      "Epoch 27: 100%|██████████| 335/335 [00:00<00:00, 515.13it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]           \n",
      "Epoch 27: 100%|██████████| 335/335 [00:00<00:00, 515.13it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.00942, v_num=0.0, ptl/val_loss=0.00236]           \n",
      "Epoch 28:   6%|▌         | 20/335 [00:00<00:00, 461.52it/s, loss=0.00969, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   6%|▌         | 20/335 [00:00<00:00, 461.52it/s, loss=0.00969, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   6%|▌         | 20/335 [00:00<00:00, 461.52it/s, loss=0.00969, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:   6%|▌         | 20/335 [00:00<00:00, 461.52it/s, loss=0.00969, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  12%|█▏        | 40/335 [00:00<00:00, 460.18it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  12%|█▏        | 40/335 [00:00<00:00, 460.18it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  12%|█▏        | 40/335 [00:00<00:00, 460.18it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  12%|█▏        | 40/335 [00:00<00:00, 460.18it/s, loss=0.0105, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  18%|█▊        | 60/335 [00:00<00:00, 455.66it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  18%|█▊        | 60/335 [00:00<00:00, 455.66it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  18%|█▊        | 60/335 [00:00<00:00, 455.66it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  18%|█▊        | 60/335 [00:00<00:00, 455.66it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  24%|██▍       | 80/335 [00:00<00:00, 455.37it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  24%|██▍       | 80/335 [00:00<00:00, 455.37it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  24%|██▍       | 80/335 [00:00<00:00, 455.37it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  24%|██▍       | 80/335 [00:00<00:00, 455.37it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  30%|██▉       | 100/335 [00:00<00:00, 453.44it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  30%|██▉       | 100/335 [00:00<00:00, 453.44it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  30%|██▉       | 100/335 [00:00<00:00, 453.44it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  30%|██▉       | 100/335 [00:00<00:00, 453.44it/s, loss=0.0102, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  42%|████▏     | 140/335 [00:00<00:00, 454.02it/s, loss=0.00902, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  42%|████▏     | 140/335 [00:00<00:00, 454.02it/s, loss=0.00902, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  42%|████▏     | 140/335 [00:00<00:00, 454.02it/s, loss=0.00902, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  42%|████▏     | 140/335 [00:00<00:00, 454.02it/s, loss=0.00902, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  48%|████▊     | 160/335 [00:00<00:00, 452.60it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  48%|████▊     | 160/335 [00:00<00:00, 452.60it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  48%|████▊     | 160/335 [00:00<00:00, 452.60it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  48%|████▊     | 160/335 [00:00<00:00, 452.60it/s, loss=0.0101, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  54%|█████▎    | 180/335 [00:00<00:00, 452.04it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  54%|█████▎    | 180/335 [00:00<00:00, 452.04it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  54%|█████▎    | 180/335 [00:00<00:00, 452.04it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  54%|█████▎    | 180/335 [00:00<00:00, 452.04it/s, loss=0.00983, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  60%|█████▉    | 200/335 [00:00<00:00, 450.99it/s, loss=0.0079, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  60%|█████▉    | 200/335 [00:00<00:00, 450.99it/s, loss=0.0079, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  60%|█████▉    | 200/335 [00:00<00:00, 450.99it/s, loss=0.0079, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  60%|█████▉    | 200/335 [00:00<00:00, 450.99it/s, loss=0.0079, v_num=0.0, ptl/val_loss=0.00236] \n",
      "Epoch 28:  66%|██████▌   | 220/335 [00:00<00:00, 451.15it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00236]  \n",
      "Epoch 28:  66%|██████▌   | 220/335 [00:00<00:00, 451.15it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00236]  \n",
      "Epoch 28:  66%|██████▌   | 220/335 [00:00<00:00, 451.15it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00236]  \n",
      "Epoch 28:  66%|██████▌   | 220/335 [00:00<00:00, 451.15it/s, loss=0.01, v_num=0.0, ptl/val_loss=0.00236]  \n",
      "Epoch 28:  72%|███████▏  | 240/335 [00:00<00:00, 451.49it/s, loss=0.00967, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  72%|███████▏  | 240/335 [00:00<00:00, 451.49it/s, loss=0.00967, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  72%|███████▏  | 240/335 [00:00<00:00, 451.49it/s, loss=0.00967, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  72%|███████▏  | 240/335 [00:00<00:00, 451.49it/s, loss=0.00967, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Epoch 28:  78%|███████▊  | 260/335 [00:00<00:00, 450.71it/s, loss=0.00847, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  78%|███████▊  | 260/335 [00:00<00:00, 450.71it/s, loss=0.00847, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  78%|███████▊  | 260/335 [00:00<00:00, 450.71it/s, loss=0.00847, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  78%|███████▊  | 260/335 [00:00<00:00, 450.71it/s, loss=0.00847, v_num=0.0, ptl/val_loss=0.00236]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 515.85it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 510.98it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 515.85it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 510.98it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 515.85it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 510.98it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 515.85it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Epoch 28: 100%|██████████| 335/335 [00:00<00:00, 510.98it/s, loss=0.00894, v_num=0.0, ptl/val_loss=0.00263]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 435.62it/s, loss=0.964, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 435.62it/s, loss=0.964, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 435.62it/s, loss=0.964, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 435.62it/s, loss=0.964, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.02it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.02it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.02it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 455.02it/s, loss=0.89, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 456.33it/s, loss=0.708, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 456.33it/s, loss=0.708, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 456.33it/s, loss=0.708, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 456.33it/s, loss=0.708, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 460.41it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 460.41it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 460.41it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 460.41it/s, loss=0.69, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 460.15it/s, loss=0.598, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 460.15it/s, loss=0.598, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 460.15it/s, loss=0.598, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 460.15it/s, loss=0.598, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 458.90it/s, loss=0.471, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 458.90it/s, loss=0.471, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 458.90it/s, loss=0.471, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 458.90it/s, loss=0.471, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 456.83it/s, loss=0.492, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 456.83it/s, loss=0.492, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 456.83it/s, loss=0.492, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 456.83it/s, loss=0.492, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.48it/s, loss=0.43, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.48it/s, loss=0.43, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.48it/s, loss=0.43, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.48it/s, loss=0.43, v_num=0.0, ptl/val_loss=0.797] \n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 451.56it/s, loss=0.363, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 451.56it/s, loss=0.363, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 451.56it/s, loss=0.363, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 451.56it/s, loss=0.363, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.15it/s, loss=0.402, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.15it/s, loss=0.402, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.15it/s, loss=0.402, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 447.15it/s, loss=0.402, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.90it/s, loss=0.382, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.90it/s, loss=0.382, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.90it/s, loss=0.382, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 443.90it/s, loss=0.382, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 442.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 442.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 442.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 442.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 508.44it/s, loss=0.33, v_num=0.0, ptl/val_loss=0.797]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 444.86it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 444.86it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 444.86it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 444.86it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.332, v_num=0.0, ptl/val_loss=0.191]           \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 413.01it/s, loss=0.317, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 413.01it/s, loss=0.317, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 413.01it/s, loss=0.317, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 413.01it/s, loss=0.317, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 426.19it/s, loss=0.358, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 426.19it/s, loss=0.358, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 426.19it/s, loss=0.358, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 426.19it/s, loss=0.358, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 426.76it/s, loss=0.302, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 426.76it/s, loss=0.302, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 426.76it/s, loss=0.302, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 426.76it/s, loss=0.302, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 431.42it/s, loss=0.318, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 431.42it/s, loss=0.318, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 431.42it/s, loss=0.318, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  24%|██▍       | 80/335 [00:00<00:00, 431.42it/s, loss=0.318, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.17it/s, loss=0.29, v_num=0.0, ptl/val_loss=0.191] \n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.17it/s, loss=0.29, v_num=0.0, ptl/val_loss=0.191] \n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.17it/s, loss=0.29, v_num=0.0, ptl/val_loss=0.191] \n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 430.17it/s, loss=0.29, v_num=0.0, ptl/val_loss=0.191] \n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.45it/s, loss=0.257, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.45it/s, loss=0.257, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.45it/s, loss=0.257, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 432.45it/s, loss=0.257, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 434.10it/s, loss=0.273, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 434.10it/s, loss=0.273, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 434.10it/s, loss=0.273, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 434.10it/s, loss=0.273, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 435.88it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 435.88it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 435.88it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 435.88it/s, loss=0.248, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 437.16it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 437.16it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 437.16it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 437.16it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.59it/s, loss=0.245, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.59it/s, loss=0.245, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.59it/s, loss=0.245, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  66%|██████▌   | 220/335 [00:00<00:00, 438.59it/s, loss=0.245, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.15it/s, loss=0.243, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.15it/s, loss=0.243, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.15it/s, loss=0.243, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 439.15it/s, loss=0.243, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 438.87it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 438.87it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 438.87it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 438.87it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 473.39it/s, loss=0.212, v_num=0.0, ptl/val_loss=0.191]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 450.18it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 450.18it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 450.18it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 450.18it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.214, v_num=0.0, ptl/val_loss=0.105]           \n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.12it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.12it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.12it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.12it/s, loss=0.211, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 443.16it/s, loss=0.247, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 443.16it/s, loss=0.247, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 443.16it/s, loss=0.247, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  12%|█▏        | 40/335 [00:00<00:00, 443.16it/s, loss=0.247, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 448.46it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 448.46it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 448.46it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 448.46it/s, loss=0.209, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  24%|██▍       | 80/335 [00:00<00:00, 450.68it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  24%|██▍       | 80/335 [00:00<00:00, 450.68it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  24%|██▍       | 80/335 [00:00<00:00, 450.68it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  24%|██▍       | 80/335 [00:00<00:00, 450.68it/s, loss=0.217, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 448.01it/s, loss=0.201, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 448.01it/s, loss=0.201, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 448.01it/s, loss=0.201, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 448.01it/s, loss=0.201, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 447.43it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 447.43it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 447.43it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 447.43it/s, loss=0.184, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 449.36it/s, loss=0.177, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 449.36it/s, loss=0.177, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 449.36it/s, loss=0.177, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 449.36it/s, loss=0.177, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 451.78it/s, loss=0.2, v_num=0.0, ptl/val_loss=0.105]  \n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 451.78it/s, loss=0.2, v_num=0.0, ptl/val_loss=0.105]  \n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 451.78it/s, loss=0.2, v_num=0.0, ptl/val_loss=0.105]  \n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 451.78it/s, loss=0.2, v_num=0.0, ptl/val_loss=0.105]  \n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 450.70it/s, loss=0.179, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 450.70it/s, loss=0.179, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 450.70it/s, loss=0.179, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 450.70it/s, loss=0.179, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 451.16it/s, loss=0.181, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 451.16it/s, loss=0.181, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 451.16it/s, loss=0.181, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 451.16it/s, loss=0.181, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 451.97it/s, loss=0.182, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 451.97it/s, loss=0.182, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 451.97it/s, loss=0.182, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 451.97it/s, loss=0.182, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 452.41it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 452.41it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 452.41it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 452.41it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 487.59it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 487.59it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 487.59it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 487.59it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.105]\n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 462.89it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 462.89it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 462.89it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 462.89it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0657]           \n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 444.05it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 444.05it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 444.05it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 444.05it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 440.90it/s, loss=0.187, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 440.90it/s, loss=0.187, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 440.90it/s, loss=0.187, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 440.90it/s, loss=0.187, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 438.98it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 438.98it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 438.98it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 438.98it/s, loss=0.161, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 442.63it/s, loss=0.167, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 442.63it/s, loss=0.167, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 442.63it/s, loss=0.167, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  24%|██▍       | 80/335 [00:00<00:00, 442.63it/s, loss=0.167, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 440.38it/s, loss=0.141, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 440.38it/s, loss=0.141, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 440.38it/s, loss=0.141, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 440.38it/s, loss=0.141, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.05it/s, loss=0.139, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.05it/s, loss=0.139, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.05it/s, loss=0.139, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 440.05it/s, loss=0.139, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 439.28it/s, loss=0.155, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 439.28it/s, loss=0.155, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 439.28it/s, loss=0.155, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 439.28it/s, loss=0.155, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 439.97it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 439.97it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 439.97it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 439.97it/s, loss=0.143, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.49it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.49it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.49it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 440.49it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 441.98it/s, loss=0.145, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 441.98it/s, loss=0.145, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 441.98it/s, loss=0.145, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 441.98it/s, loss=0.145, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 438.52it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 438.52it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 438.52it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 438.52it/s, loss=0.146, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 438.80it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 438.80it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 438.80it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 438.80it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 474.53it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 474.53it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 474.53it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 474.53it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 446.64it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 446.64it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 446.64it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 446.64it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0657]\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892863)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 421.12it/s, loss=0.986, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 421.12it/s, loss=0.986, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 421.12it/s, loss=0.986, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 421.12it/s, loss=0.986, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 434.32it/s, loss=0.526, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 434.32it/s, loss=0.526, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 434.32it/s, loss=0.526, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 434.32it/s, loss=0.526, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 436.75it/s, loss=0.436, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 436.75it/s, loss=0.436, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 436.75it/s, loss=0.436, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 436.75it/s, loss=0.436, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.40it/s, loss=0.352, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.40it/s, loss=0.352, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.40it/s, loss=0.352, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 436.40it/s, loss=0.352, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 433.35it/s, loss=0.341, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 433.35it/s, loss=0.341, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 433.35it/s, loss=0.341, v_num=0.0, ptl/val_loss=1.010]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 433.35it/s, loss=0.341, v_num=0.0, ptl/val_loss=1.010]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 387.82it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 387.82it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 387.82it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 387.82it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.299, v_num=0.0, ptl/val_loss=0.151]           \n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 399.23it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 399.23it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 399.23it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 399.23it/s, loss=0.291, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 409.02it/s, loss=0.265, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 409.02it/s, loss=0.265, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 409.02it/s, loss=0.265, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 409.02it/s, loss=0.265, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 411.90it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 411.90it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 411.90it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 411.90it/s, loss=0.234, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 416.43it/s, loss=0.223, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 416.43it/s, loss=0.223, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 416.43it/s, loss=0.223, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 416.43it/s, loss=0.223, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 419.25it/s, loss=0.194, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 419.25it/s, loss=0.194, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 419.25it/s, loss=0.194, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 419.25it/s, loss=0.194, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 417.90it/s, loss=0.207, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 417.90it/s, loss=0.207, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 417.90it/s, loss=0.207, v_num=0.0, ptl/val_loss=0.151]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 417.90it/s, loss=0.207, v_num=0.0, ptl/val_loss=0.151]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 372.30it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 372.30it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 372.30it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 372.30it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0757]           \n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 388.43it/s, loss=0.188, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 388.43it/s, loss=0.188, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 388.43it/s, loss=0.188, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 388.43it/s, loss=0.188, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 392.90it/s, loss=0.176, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 392.90it/s, loss=0.176, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 392.90it/s, loss=0.176, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 392.90it/s, loss=0.176, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 403.87it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 403.87it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 403.87it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 403.87it/s, loss=0.159, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 411.13it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 411.13it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 411.13it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 411.13it/s, loss=0.156, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 413.78it/s, loss=0.14, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 413.78it/s, loss=0.14, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 413.78it/s, loss=0.14, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 413.78it/s, loss=0.14, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 416.17it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 416.17it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 416.17it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0757]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 416.17it/s, loss=0.15, v_num=0.0, ptl/val_loss=0.0757]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 377.29it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 377.29it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 377.29it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 377.29it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.137, v_num=0.0, ptl/val_loss=0.0453]           \n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 420.17it/s, loss=0.144, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 420.17it/s, loss=0.144, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 420.17it/s, loss=0.144, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 420.17it/s, loss=0.144, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 425.24it/s, loss=0.136, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 425.24it/s, loss=0.136, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 425.24it/s, loss=0.136, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 425.24it/s, loss=0.136, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 428.26it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 428.26it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 428.26it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 428.26it/s, loss=0.125, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 429.82it/s, loss=0.121, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 429.82it/s, loss=0.121, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 429.82it/s, loss=0.121, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 429.82it/s, loss=0.121, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 429.43it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 429.43it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 429.43it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 429.43it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 423.23it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 423.23it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 423.23it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 423.23it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 372.51it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 372.51it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 372.51it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 372.51it/s, loss=0.122, v_num=0.0, ptl/val_loss=0.0453]\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892866)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 438.35it/s, loss=0.948, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 438.35it/s, loss=0.948, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 438.35it/s, loss=0.948, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 438.35it/s, loss=0.948, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 449.01it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 449.01it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 449.01it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 449.01it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 452.99it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 452.99it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 452.99it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 452.99it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 454.58it/s, loss=0.97, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 454.58it/s, loss=0.97, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 454.58it/s, loss=0.97, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 454.58it/s, loss=0.97, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 454.44it/s, loss=0.967, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 454.44it/s, loss=0.967, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 454.44it/s, loss=0.967, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 454.44it/s, loss=0.967, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 452.81it/s, loss=0.91, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 452.81it/s, loss=0.91, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 452.81it/s, loss=0.91, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 452.81it/s, loss=0.91, v_num=0.0, ptl/val_loss=0.734] \n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 453.70it/s, loss=0.986, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 453.70it/s, loss=0.986, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 453.70it/s, loss=0.986, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 453.70it/s, loss=0.986, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.62it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.62it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.62it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 454.62it/s, loss=0.951, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 454.80it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 454.80it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 454.80it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 454.80it/s, loss=0.966, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 455.24it/s, loss=0.975, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 455.24it/s, loss=0.975, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 455.24it/s, loss=0.975, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 455.24it/s, loss=0.975, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.83it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.83it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.83it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.83it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 525.70it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 525.70it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 525.70it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 525.70it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 513.98it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 513.98it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 513.98it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 513.98it/s, loss=0.929, v_num=0.0, ptl/val_loss=0.734]\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892864)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 433.36it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 433.36it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 433.36it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 433.36it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 448.19it/s, loss=0.9, v_num=0.0, ptl/val_loss=0.897]  \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 448.19it/s, loss=0.9, v_num=0.0, ptl/val_loss=0.897]  \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 448.19it/s, loss=0.9, v_num=0.0, ptl/val_loss=0.897]  \n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 448.19it/s, loss=0.9, v_num=0.0, ptl/val_loss=0.897]  \n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.78it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.78it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.78it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 452.78it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.32it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.32it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.32it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 452.32it/s, loss=0.758, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 451.12it/s, loss=0.775, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 451.12it/s, loss=0.775, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 451.12it/s, loss=0.775, v_num=0.0, ptl/val_loss=0.897]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 451.12it/s, loss=0.775, v_num=0.0, ptl/val_loss=0.897]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 401.13it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 401.13it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 401.13it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 401.13it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.718, v_num=0.0, ptl/val_loss=0.536]           \n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 435.97it/s, loss=0.723, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 435.97it/s, loss=0.723, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 435.97it/s, loss=0.723, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 435.97it/s, loss=0.723, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 439.17it/s, loss=0.683, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 439.17it/s, loss=0.683, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 439.17it/s, loss=0.683, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 439.17it/s, loss=0.683, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 442.93it/s, loss=0.641, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 442.93it/s, loss=0.641, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 442.93it/s, loss=0.641, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 442.93it/s, loss=0.641, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.93it/s, loss=0.558, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.93it/s, loss=0.558, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.93it/s, loss=0.558, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 448.93it/s, loss=0.558, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 447.63it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 447.63it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 447.63it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 447.63it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 389.69it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 389.69it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 389.69it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 389.69it/s, loss=0.586, v_num=0.0, ptl/val_loss=0.536]\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892868)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 453.95it/s, loss=0.51, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 453.95it/s, loss=0.51, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 453.95it/s, loss=0.51, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 453.95it/s, loss=0.51, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 458.94it/s, loss=0.292, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 458.94it/s, loss=0.292, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 458.94it/s, loss=0.292, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 458.94it/s, loss=0.292, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 459.87it/s, loss=0.191, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 459.87it/s, loss=0.191, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 459.87it/s, loss=0.191, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 459.87it/s, loss=0.191, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 463.21it/s, loss=0.154, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 463.21it/s, loss=0.154, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 463.21it/s, loss=0.154, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  24%|██▍       | 80/335 [00:00<00:00, 463.21it/s, loss=0.154, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 464.82it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 464.82it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 464.82it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 464.82it/s, loss=0.112, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 463.01it/s, loss=0.0764, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 463.01it/s, loss=0.0764, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 463.01it/s, loss=0.0764, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 463.01it/s, loss=0.0764, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 463.85it/s, loss=0.0826, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 463.85it/s, loss=0.0826, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 463.85it/s, loss=0.0826, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 463.85it/s, loss=0.0826, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 464.21it/s, loss=0.0675, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 464.21it/s, loss=0.0675, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 464.21it/s, loss=0.0675, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 464.21it/s, loss=0.0675, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.67it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.67it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.67it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.67it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 465.15it/s, loss=0.064, v_num=0.0, ptl/val_loss=0.682] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 465.15it/s, loss=0.064, v_num=0.0, ptl/val_loss=0.682] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 465.15it/s, loss=0.064, v_num=0.0, ptl/val_loss=0.682] \n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 465.15it/s, loss=0.064, v_num=0.0, ptl/val_loss=0.682] \n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 462.75it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 462.75it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 462.75it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  72%|███████▏  | 240/335 [00:00<00:00, 462.75it/s, loss=0.058, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 463.10it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 463.10it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 463.10it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 463.10it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 537.14it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 537.14it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 537.14it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 537.14it/s, loss=0.0488, v_num=0.0, ptl/val_loss=0.682]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 486.91it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 486.91it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 486.91it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]           \n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 486.91it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.049, v_num=0.0, ptl/val_loss=0.0177]           \n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 455.42it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 455.42it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 455.42it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:   6%|▌         | 20/335 [00:00<00:00, 455.42it/s, loss=0.0478, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 459.69it/s, loss=0.0568, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 459.69it/s, loss=0.0568, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 459.69it/s, loss=0.0568, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  12%|█▏        | 40/335 [00:00<00:00, 459.69it/s, loss=0.0568, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 449.41it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0177] \n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 449.41it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0177] \n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 449.41it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0177] \n",
      "Epoch 1:  18%|█▊        | 60/335 [00:00<00:00, 449.41it/s, loss=0.046, v_num=0.0, ptl/val_loss=0.0177] \n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 451.42it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 451.42it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 451.42it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  30%|██▉       | 100/335 [00:00<00:00, 451.42it/s, loss=0.0453, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 454.11it/s, loss=0.0369, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 454.11it/s, loss=0.0369, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 454.11it/s, loss=0.0369, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  36%|███▌      | 120/335 [00:00<00:00, 454.11it/s, loss=0.0369, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 453.34it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 453.34it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 453.34it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  42%|████▏     | 140/335 [00:00<00:00, 453.34it/s, loss=0.0355, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 452.61it/s, loss=0.0409, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 452.61it/s, loss=0.0409, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 452.61it/s, loss=0.0409, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  48%|████▊     | 160/335 [00:00<00:00, 452.61it/s, loss=0.0409, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 453.72it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 453.72it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 453.72it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  54%|█████▎    | 180/335 [00:00<00:00, 453.72it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 454.04it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 454.04it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 454.04it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  60%|█████▉    | 200/335 [00:00<00:00, 454.04it/s, loss=0.0314, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 454.80it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 454.80it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 454.80it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  72%|███████▏  | 240/335 [00:00<00:00, 454.80it/s, loss=0.0361, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.22it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.22it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.22it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1:  78%|███████▊  | 260/335 [00:00<00:00, 454.22it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 489.79it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 489.79it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 489.79it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 1:  90%|████████▉ | 300/335 [00:00<00:00, 489.79it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.0177]\n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 478.07it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 478.07it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 478.07it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]           \n",
      "Epoch 1: 100%|██████████| 335/335 [00:00<00:00, 478.07it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0315, v_num=0.0, ptl/val_loss=0.00894]           \n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.13it/s, loss=0.0322, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.13it/s, loss=0.0322, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.13it/s, loss=0.0322, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:   6%|▌         | 20/335 [00:00<00:00, 436.13it/s, loss=0.0322, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 459.33it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 459.33it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 459.33it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  18%|█▊        | 60/335 [00:00<00:00, 459.33it/s, loss=0.0303, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 425.95it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 425.95it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 425.95it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  30%|██▉       | 100/335 [00:00<00:00, 425.95it/s, loss=0.0319, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 428.22it/s, loss=0.0272, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 428.22it/s, loss=0.0272, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 428.22it/s, loss=0.0272, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  36%|███▌      | 120/335 [00:00<00:00, 428.22it/s, loss=0.0272, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 432.02it/s, loss=0.0281, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 432.02it/s, loss=0.0281, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 432.02it/s, loss=0.0281, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  42%|████▏     | 140/335 [00:00<00:00, 432.02it/s, loss=0.0281, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 433.94it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 433.94it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 433.94it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  48%|████▊     | 160/335 [00:00<00:00, 433.94it/s, loss=0.0279, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 434.66it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 434.66it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 434.66it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  54%|█████▎    | 180/335 [00:00<00:00, 434.66it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 435.91it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00894] \n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 435.91it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00894] \n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 435.91it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00894] \n",
      "Epoch 2:  60%|█████▉    | 200/335 [00:00<00:00, 435.91it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00894] \n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 437.14it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 437.14it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 437.14it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  66%|██████▌   | 220/335 [00:00<00:00, 437.14it/s, loss=0.0269, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 437.93it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 437.93it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 437.93it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  72%|███████▏  | 240/335 [00:00<00:00, 437.93it/s, loss=0.0255, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 437.88it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 472.38it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 437.88it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 472.38it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 437.88it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 472.38it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2:  78%|███████▊  | 260/335 [00:00<00:00, 437.88it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 2:  90%|████████▉ | 300/335 [00:00<00:00, 472.38it/s, loss=0.0213, v_num=0.0, ptl/val_loss=0.00894]\n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 464.28it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 464.28it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 464.28it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]           \n",
      "Epoch 2: 100%|██████████| 335/335 [00:00<00:00, 464.28it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0225, v_num=0.0, ptl/val_loss=0.00806]           \n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 451.58it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 451.58it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 451.58it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:   6%|▌         | 20/335 [00:00<00:00, 451.58it/s, loss=0.0241, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 451.02it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 451.02it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 451.02it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  12%|█▏        | 40/335 [00:00<00:00, 451.02it/s, loss=0.0271, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 446.49it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 446.49it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 446.49it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  18%|█▊        | 60/335 [00:00<00:00, 446.49it/s, loss=0.0235, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  30%|██▉       | 100/335 [00:00<00:00, 452.28it/s, loss=0.0236, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 455.35it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 455.35it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 455.35it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  36%|███▌      | 120/335 [00:00<00:00, 455.35it/s, loss=0.0232, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 453.65it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 453.65it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 453.65it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  42%|████▏     | 140/335 [00:00<00:00, 453.65it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 449.36it/s, loss=0.0226, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 449.36it/s, loss=0.0226, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 449.36it/s, loss=0.0226, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  48%|████▊     | 160/335 [00:00<00:00, 449.36it/s, loss=0.0226, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 448.97it/s, loss=0.0206, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 448.97it/s, loss=0.0206, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 448.97it/s, loss=0.0206, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  54%|█████▎    | 180/335 [00:00<00:00, 448.97it/s, loss=0.0206, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 448.85it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00806] \n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 448.85it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00806] \n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 448.85it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00806] \n",
      "Epoch 3:  60%|█████▉    | 200/335 [00:00<00:00, 448.85it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00806] \n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  66%|██████▌   | 220/335 [00:00<00:00, 446.11it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 446.78it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 446.78it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 446.78it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  72%|███████▏  | 240/335 [00:00<00:00, 446.78it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 443.12it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.02it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 443.12it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.02it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 443.12it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.02it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3:  78%|███████▊  | 260/335 [00:00<00:00, 443.12it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 3:  90%|████████▉ | 300/335 [00:00<00:00, 477.02it/s, loss=0.0176, v_num=0.0, ptl/val_loss=0.00806]\n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 468.26it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 468.26it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 468.26it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]           \n",
      "Epoch 3: 100%|██████████| 335/335 [00:00<00:00, 468.26it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0175, v_num=0.0, ptl/val_loss=0.00606]           \n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 387.50it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 387.50it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 387.50it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:   6%|▌         | 20/335 [00:00<00:00, 387.50it/s, loss=0.0173, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 401.81it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 401.81it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 401.81it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  12%|█▏        | 40/335 [00:00<00:00, 401.81it/s, loss=0.0215, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 421.15it/s, loss=0.0195, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 421.15it/s, loss=0.0195, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 421.15it/s, loss=0.0195, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  18%|█▊        | 60/335 [00:00<00:00, 421.15it/s, loss=0.0195, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 433.94it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 433.94it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 433.94it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  24%|██▍       | 80/335 [00:00<00:00, 433.94it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 430.16it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 430.16it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 430.16it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  30%|██▉       | 100/335 [00:00<00:00, 430.16it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 432.82it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 432.82it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 432.82it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  42%|████▏     | 140/335 [00:00<00:00, 432.82it/s, loss=0.0174, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 438.10it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 438.10it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 438.10it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  48%|████▊     | 160/335 [00:00<00:00, 438.10it/s, loss=0.0181, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 440.01it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 440.01it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 440.01it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  54%|█████▎    | 180/335 [00:00<00:00, 440.01it/s, loss=0.0171, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.59it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.59it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.59it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  60%|█████▉    | 200/335 [00:00<00:00, 439.59it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  66%|██████▌   | 220/335 [00:00<00:00, 437.94it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  66%|██████▌   | 220/335 [00:00<00:00, 437.94it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  66%|██████▌   | 220/335 [00:00<00:00, 437.94it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  66%|██████▌   | 220/335 [00:00<00:00, 437.94it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.53it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.53it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.53it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  72%|███████▏  | 240/335 [00:00<00:00, 440.53it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 443.11it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 479.97it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 443.11it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 479.97it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 443.11it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 479.97it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4:  78%|███████▊  | 260/335 [00:00<00:00, 443.11it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 4:  90%|████████▉ | 300/335 [00:00<00:00, 479.97it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00606]\n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 512.31it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]           \n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 512.31it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]           \n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 512.31it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]           \n",
      "Epoch 4: 100%|██████████| 335/335 [00:00<00:00, 512.31it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0159, v_num=0.0, ptl/val_loss=0.00618]           \n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 466.42it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 466.42it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 466.42it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:   6%|▌         | 20/335 [00:00<00:00, 466.42it/s, loss=0.0158, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 469.88it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 469.88it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 469.88it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  12%|█▏        | 40/335 [00:00<00:00, 469.88it/s, loss=0.0177, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 473.04it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 473.04it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 473.04it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  18%|█▊        | 60/335 [00:00<00:00, 473.04it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 448.37it/s, loss=0.0169, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 448.37it/s, loss=0.0169, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 448.37it/s, loss=0.0169, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  30%|██▉       | 100/335 [00:00<00:00, 448.37it/s, loss=0.0169, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 445.40it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 445.40it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 445.40it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  36%|███▌      | 120/335 [00:00<00:00, 445.40it/s, loss=0.0149, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 445.57it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 445.57it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 445.57it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  42%|████▏     | 140/335 [00:00<00:00, 445.57it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.85it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.85it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.85it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  48%|████▊     | 160/335 [00:00<00:00, 444.85it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 431.25it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 431.25it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 431.25it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  54%|█████▎    | 180/335 [00:00<00:00, 431.25it/s, loss=0.0153, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 431.55it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 431.55it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 431.55it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  60%|█████▉    | 200/335 [00:00<00:00, 431.55it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 432.02it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00618] \n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 432.02it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00618] \n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 432.02it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00618] \n",
      "Epoch 5:  66%|██████▌   | 220/335 [00:00<00:00, 432.02it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.00618] \n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 433.39it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 433.39it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 433.39it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  72%|███████▏  | 240/335 [00:00<00:00, 433.39it/s, loss=0.0162, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 434.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 434.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 434.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  78%|███████▊  | 260/335 [00:00<00:00, 434.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 468.52it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 468.52it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 468.52it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5:  90%|████████▉ | 300/335 [00:00<00:00, 468.52it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00618]\n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 460.53it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 460.53it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 460.53it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]           \n",
      "Epoch 5: 100%|██████████| 335/335 [00:00<00:00, 460.53it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0148, v_num=0.0, ptl/val_loss=0.00436]           \n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 448.63it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 448.63it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 448.63it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:   6%|▌         | 20/335 [00:00<00:00, 448.63it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 452.07it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 452.07it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 452.07it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  12%|█▏        | 40/335 [00:00<00:00, 452.07it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 450.99it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 450.99it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 450.99it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  18%|█▊        | 60/335 [00:00<00:00, 450.99it/s, loss=0.0142, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 452.87it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 452.87it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 452.87it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  24%|██▍       | 80/335 [00:00<00:00, 452.87it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  30%|██▉       | 100/335 [00:00<00:00, 450.19it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  30%|██▉       | 100/335 [00:00<00:00, 450.19it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  30%|██▉       | 100/335 [00:00<00:00, 450.19it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  30%|██▉       | 100/335 [00:00<00:00, 450.19it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.91it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.91it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.91it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  42%|████▏     | 140/335 [00:00<00:00, 450.91it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 450.12it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 450.12it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 450.12it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  48%|████▊     | 160/335 [00:00<00:00, 450.12it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.93it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.93it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.93it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  54%|█████▎    | 180/335 [00:00<00:00, 449.93it/s, loss=0.0129, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 450.29it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 450.29it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 450.29it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  60%|█████▉    | 200/335 [00:00<00:00, 450.29it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.59it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.59it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.59it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  66%|██████▌   | 220/335 [00:00<00:00, 449.59it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 450.13it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00436] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 450.13it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00436] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 450.13it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00436] \n",
      "Epoch 6:  72%|███████▏  | 240/335 [00:00<00:00, 450.13it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00436] \n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 450.94it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 486.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 450.94it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 486.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 450.94it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 486.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6:  78%|███████▊  | 260/335 [00:00<00:00, 450.94it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 6:  90%|████████▉ | 300/335 [00:00<00:00, 486.85it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00436]\n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 518.43it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 518.43it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 518.43it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]           \n",
      "Epoch 6: 100%|██████████| 335/335 [00:00<00:00, 518.43it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.0134, v_num=0.0, ptl/val_loss=0.00533]           \n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 450.05it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 450.05it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 450.05it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:   6%|▌         | 20/335 [00:00<00:00, 450.05it/s, loss=0.0132, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  12%|█▏        | 40/335 [00:00<00:00, 447.86it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  12%|█▏        | 40/335 [00:00<00:00, 447.86it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  12%|█▏        | 40/335 [00:00<00:00, 447.86it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  12%|█▏        | 40/335 [00:00<00:00, 447.86it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 447.03it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 447.03it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 447.03it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  18%|█▊        | 60/335 [00:00<00:00, 447.03it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 445.99it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 445.99it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 445.99it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  30%|██▉       | 100/335 [00:00<00:00, 445.99it/s, loss=0.0136, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  36%|███▌      | 120/335 [00:00<00:00, 446.88it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 446.56it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 446.56it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 446.56it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  42%|████▏     | 140/335 [00:00<00:00, 446.56it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 446.82it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 446.82it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 446.82it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  48%|████▊     | 160/335 [00:00<00:00, 446.82it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  54%|█████▎    | 180/335 [00:00<00:00, 446.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  54%|█████▎    | 180/335 [00:00<00:00, 446.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  54%|█████▎    | 180/335 [00:00<00:00, 446.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  54%|█████▎    | 180/335 [00:00<00:00, 446.36it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 446.58it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 446.58it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 446.58it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  60%|█████▉    | 200/335 [00:00<00:00, 446.58it/s, loss=0.0108, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 447.13it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 447.13it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 447.13it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  66%|██████▌   | 220/335 [00:00<00:00, 447.13it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00533] \n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 447.95it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 447.95it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 447.95it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  72%|███████▏  | 240/335 [00:00<00:00, 447.95it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 448.32it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 448.32it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 448.32it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7:  78%|███████▊  | 260/335 [00:00<00:00, 448.32it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 483.51it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 483.51it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 483.51it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7:  90%|████████▉ | 300/335 [00:00<00:00, 483.51it/s, loss=0.0106, v_num=0.0, ptl/val_loss=0.00533]\n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 514.55it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 514.55it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 514.55it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483]           \n",
      "Epoch 7: 100%|██████████| 335/335 [00:00<00:00, 514.55it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:   0%|          | 0/335 [00:00<?, ?it/s, loss=0.011, v_num=0.0, ptl/val_loss=0.00483]           \n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 441.89it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 441.89it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 441.89it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:   6%|▌         | 20/335 [00:00<00:00, 441.89it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 448.50it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 448.50it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 448.50it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  18%|█▊        | 60/335 [00:00<00:00, 448.50it/s, loss=0.0122, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 448.42it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 448.42it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 448.42it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  24%|██▍       | 80/335 [00:00<00:00, 448.42it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.61it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.61it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.61it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  30%|██▉       | 100/335 [00:00<00:00, 452.61it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 453.48it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 453.48it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 453.48it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  36%|███▌      | 120/335 [00:00<00:00, 453.48it/s, loss=0.0114, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 451.28it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 451.28it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 451.28it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  42%|████▏     | 140/335 [00:00<00:00, 451.28it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  48%|████▊     | 160/335 [00:00<00:00, 452.45it/s, loss=0.0128, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 454.60it/s, loss=0.00977, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 454.60it/s, loss=0.00977, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 454.60it/s, loss=0.00977, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  60%|█████▉    | 200/335 [00:00<00:00, 454.60it/s, loss=0.00977, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 452.84it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 452.84it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 452.84it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:  66%|██████▌   | 220/335 [00:00<00:00, 452.84it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00483] \n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 451.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 451.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 451.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  72%|███████▏  | 240/335 [00:00<00:00, 451.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.14it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.14it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.14it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00483]\n",
      "Epoch 8:  78%|███████▊  | 260/335 [00:00<00:00, 452.14it/s, loss=0.0113, v_num=0.0, ptl/val_loss=0.00483]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 520.30it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 516.51it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 520.30it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 516.51it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 520.30it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 516.51it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "\u001B[2m\u001B[36m(pid=1892871)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 520.30it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "Epoch 8: 100%|██████████| 335/335 [00:00<00:00, 516.51it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.00572]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 452.92it/s, loss=0.717, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 452.92it/s, loss=0.717, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 452.92it/s, loss=0.717, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 452.92it/s, loss=0.717, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 464.27it/s, loss=0.787, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 464.27it/s, loss=0.787, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 464.27it/s, loss=0.787, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 464.27it/s, loss=0.787, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 467.79it/s, loss=0.714, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 467.79it/s, loss=0.714, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 467.79it/s, loss=0.714, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 467.79it/s, loss=0.714, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 472.38it/s, loss=0.735, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 472.38it/s, loss=0.735, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 472.38it/s, loss=0.735, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 472.38it/s, loss=0.735, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 472.70it/s, loss=0.736, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 472.70it/s, loss=0.736, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 472.70it/s, loss=0.736, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 472.70it/s, loss=0.736, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 469.59it/s, loss=0.699, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 469.59it/s, loss=0.699, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 469.59it/s, loss=0.699, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 469.59it/s, loss=0.699, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 468.04it/s, loss=0.757, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 468.04it/s, loss=0.757, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 468.04it/s, loss=0.757, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 468.04it/s, loss=0.757, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 467.32it/s, loss=0.729, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 467.32it/s, loss=0.729, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 467.32it/s, loss=0.729, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 467.32it/s, loss=0.729, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.97it/s, loss=0.675, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.97it/s, loss=0.675, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.97it/s, loss=0.675, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 465.97it/s, loss=0.675, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 461.99it/s, loss=0.743, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 461.99it/s, loss=0.743, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 461.99it/s, loss=0.743, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 461.99it/s, loss=0.743, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.10it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.10it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.10it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 453.10it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 515.02it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 515.02it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 515.02it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 515.02it/s, loss=0.707, v_num=0.0, ptl/val_loss=0.611]\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892865)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.654, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.654, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.654, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 423.80it/s, loss=0.654, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 429.84it/s, loss=0.3, v_num=0.0, ptl/val_loss=0.892]  \n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 429.84it/s, loss=0.3, v_num=0.0, ptl/val_loss=0.892]  \n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 429.84it/s, loss=0.3, v_num=0.0, ptl/val_loss=0.892]  \n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 429.84it/s, loss=0.3, v_num=0.0, ptl/val_loss=0.892]  \n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 432.40it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 432.40it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 432.40it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.892]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 432.40it/s, loss=0.204, v_num=0.0, ptl/val_loss=0.892]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.04it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.04it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.04it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.04it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.186, v_num=0.0, ptl/val_loss=0.0643]          \n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 412.33it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 412.33it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 412.33it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 412.33it/s, loss=0.158, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 400.16it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 400.16it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 400.16it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 400.16it/s, loss=0.126, v_num=0.0, ptl/val_loss=0.0643]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 403.35it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0643] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 403.35it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0643] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 403.35it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0643] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 403.35it/s, loss=0.11, v_num=0.0, ptl/val_loss=0.0643] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 310.07it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]          \n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 310.07it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]          \n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 310.07it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]          \n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 310.07it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.106, v_num=0.0, ptl/val_loss=0.027]          \n",
      "Epoch 2:  24%|██▍       | 20/84 [00:00<00:00, 397.81it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  24%|██▍       | 20/84 [00:00<00:00, 397.81it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  24%|██▍       | 20/84 [00:00<00:00, 397.81it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  24%|██▍       | 20/84 [00:00<00:00, 397.81it/s, loss=0.102, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 399.30it/s, loss=0.0861, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 399.30it/s, loss=0.0861, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 399.30it/s, loss=0.0861, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  48%|████▊     | 40/84 [00:00<00:00, 399.30it/s, loss=0.0861, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 398.74it/s, loss=0.0785, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 398.74it/s, loss=0.0785, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 398.74it/s, loss=0.0785, v_num=0.0, ptl/val_loss=0.027]\n",
      "Epoch 2:  71%|███████▏  | 60/84 [00:00<00:00, 398.74it/s, loss=0.0785, v_num=0.0, ptl/val_loss=0.027]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 303.98it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 303.98it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 303.98it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]          \n",
      "Epoch 2: 100%|██████████| 84/84 [00:00<00:00, 303.98it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0773, v_num=0.0, ptl/val_loss=0.0174]          \n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 406.00it/s, loss=0.0758, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 406.00it/s, loss=0.0758, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 406.00it/s, loss=0.0758, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  24%|██▍       | 20/84 [00:00<00:00, 406.00it/s, loss=0.0758, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 405.06it/s, loss=0.0667, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 405.06it/s, loss=0.0667, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 405.06it/s, loss=0.0667, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  48%|████▊     | 40/84 [00:00<00:00, 405.06it/s, loss=0.0667, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 407.78it/s, loss=0.0626, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 407.78it/s, loss=0.0626, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 407.78it/s, loss=0.0626, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  71%|███████▏  | 60/84 [00:00<00:00, 407.78it/s, loss=0.0626, v_num=0.0, ptl/val_loss=0.0174]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 305.01it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 305.01it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 305.01it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]          \n",
      "Epoch 3: 100%|██████████| 84/84 [00:00<00:00, 305.01it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0615, v_num=0.0, ptl/val_loss=0.0126]          \n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 401.71it/s, loss=0.0623, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 401.71it/s, loss=0.0623, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 401.71it/s, loss=0.0623, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  24%|██▍       | 20/84 [00:00<00:00, 401.71it/s, loss=0.0623, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 383.76it/s, loss=0.0555, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 383.76it/s, loss=0.0555, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 383.76it/s, loss=0.0555, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  48%|████▊     | 40/84 [00:00<00:00, 383.76it/s, loss=0.0555, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 396.96it/s, loss=0.0523, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 396.96it/s, loss=0.0523, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 396.96it/s, loss=0.0523, v_num=0.0, ptl/val_loss=0.0126]\n",
      "Epoch 4:  71%|███████▏  | 60/84 [00:00<00:00, 396.96it/s, loss=0.0523, v_num=0.0, ptl/val_loss=0.0126]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 304.33it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 304.33it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 304.33it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]          \n",
      "Epoch 4: 100%|██████████| 84/84 [00:00<00:00, 304.33it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0514, v_num=0.0, ptl/val_loss=0.0113]          \n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 417.35it/s, loss=0.0528, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 417.35it/s, loss=0.0528, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 417.35it/s, loss=0.0528, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  24%|██▍       | 20/84 [00:00<00:00, 417.35it/s, loss=0.0528, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.32it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.32it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.32it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  48%|████▊     | 40/84 [00:00<00:00, 416.32it/s, loss=0.0484, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.15it/s, loss=0.0447, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.15it/s, loss=0.0447, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.15it/s, loss=0.0447, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  71%|███████▏  | 60/84 [00:00<00:00, 418.15it/s, loss=0.0447, v_num=0.0, ptl/val_loss=0.0113]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 311.68it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 311.68it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 311.68it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]          \n",
      "Epoch 5: 100%|██████████| 84/84 [00:00<00:00, 311.68it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0441, v_num=0.0, ptl/val_loss=0.0094]          \n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 413.83it/s, loss=0.0451, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 413.83it/s, loss=0.0451, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 413.83it/s, loss=0.0451, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  24%|██▍       | 20/84 [00:00<00:00, 413.83it/s, loss=0.0451, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 412.62it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 412.62it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 412.62it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  48%|████▊     | 40/84 [00:00<00:00, 412.62it/s, loss=0.0417, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 419.70it/s, loss=0.0397, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 419.70it/s, loss=0.0397, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 419.70it/s, loss=0.0397, v_num=0.0, ptl/val_loss=0.0094]\n",
      "Epoch 6:  71%|███████▏  | 60/84 [00:00<00:00, 419.70it/s, loss=0.0397, v_num=0.0, ptl/val_loss=0.0094]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 316.79it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 316.79it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 316.79it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]          \n",
      "Epoch 6: 100%|██████████| 84/84 [00:00<00:00, 316.79it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.0389, v_num=0.0, ptl/val_loss=0.00745]          \n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 426.70it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 426.70it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 426.70it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  24%|██▍       | 20/84 [00:00<00:00, 426.70it/s, loss=0.0403, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 412.52it/s, loss=0.0379, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 412.52it/s, loss=0.0379, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 412.52it/s, loss=0.0379, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  48%|████▊     | 40/84 [00:00<00:00, 412.52it/s, loss=0.0379, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 411.99it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 411.99it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 411.99it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  71%|███████▏  | 60/84 [00:00<00:00, 411.99it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 302.40it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 302.40it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 302.40it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745]\n",
      "Epoch 7: 100%|██████████| 84/84 [00:00<00:00, 302.40it/s, loss=0.036, v_num=0.0, ptl/val_loss=0.00745]\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1892869)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 425.83it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 425.83it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 425.83it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 425.83it/s, loss=1.14, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 436.11it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 436.11it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 436.11it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 436.11it/s, loss=1.04, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 442.10it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 442.10it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 442.10it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 442.10it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.73it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.73it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.73it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 323.73it/s, loss=0.99, v_num=0.0, ptl/val_loss=0.877]\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902434)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/335 [00:00<?, ?it/s]              \n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 430.21it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 430.21it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 430.21it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:   6%|▌         | 20/335 [00:00<00:00, 430.21it/s, loss=1.22, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 436.95it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.919] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 436.95it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.919] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 436.95it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.919] \n",
      "Epoch 0:  12%|█▏        | 40/335 [00:00<00:00, 436.95it/s, loss=1.3, v_num=0.0, ptl/val_loss=0.919] \n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 439.72it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 439.72it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 439.72it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  18%|█▊        | 60/335 [00:00<00:00, 439.72it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 444.94it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 444.94it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 444.94it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  30%|██▉       | 100/335 [00:00<00:00, 444.94it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 439.75it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 439.75it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 439.75it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  36%|███▌      | 120/335 [00:00<00:00, 439.75it/s, loss=1.17, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 441.06it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 441.06it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 441.06it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  42%|████▏     | 140/335 [00:00<00:00, 441.06it/s, loss=1.09, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.13it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.13it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.13it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  48%|████▊     | 160/335 [00:00<00:00, 443.13it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 443.11it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 443.11it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 443.11it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  54%|█████▎    | 180/335 [00:00<00:00, 443.11it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 444.16it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 444.16it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 444.16it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  60%|█████▉    | 200/335 [00:00<00:00, 444.16it/s, loss=1.02, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.83it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.83it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.83it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  66%|██████▌   | 220/335 [00:00<00:00, 443.83it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 445.41it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 445.41it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 445.41it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0:  78%|███████▊  | 260/335 [00:00<00:00, 445.41it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/67 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.28it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.28it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.28it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 518.28it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.59it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.59it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.59it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "Epoch 0: 100%|██████████| 335/335 [00:00<00:00, 448.59it/s, loss=1.05, v_num=0.0, ptl/val_loss=0.919]\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902460)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 435.15it/s, loss=0.493, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 435.15it/s, loss=0.493, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 435.15it/s, loss=0.493, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 435.15it/s, loss=0.493, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 444.71it/s, loss=0.197, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 444.71it/s, loss=0.197, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 444.71it/s, loss=0.197, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  24%|██▍       | 40/168 [00:00<00:00, 444.71it/s, loss=0.197, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 444.46it/s, loss=0.0929, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 444.46it/s, loss=0.0929, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 444.46it/s, loss=0.0929, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 444.46it/s, loss=0.0929, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 446.30it/s, loss=0.0737, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 446.30it/s, loss=0.0737, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 446.30it/s, loss=0.0737, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 446.30it/s, loss=0.0737, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 445.87it/s, loss=0.0742, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 445.87it/s, loss=0.0742, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 445.87it/s, loss=0.0742, v_num=0.0, ptl/val_loss=0.948]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 445.87it/s, loss=0.0742, v_num=0.0, ptl/val_loss=0.948]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 414.22it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 414.22it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 414.22it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 414.22it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0639, v_num=0.0, ptl/val_loss=0.0199]           \n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 423.37it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 423.37it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 423.37it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 423.37it/s, loss=0.0606, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 429.80it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 429.80it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 429.80it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 429.80it/s, loss=0.0553, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 427.72it/s, loss=0.0469, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 427.72it/s, loss=0.0469, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 427.72it/s, loss=0.0469, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 427.72it/s, loss=0.0469, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 427.01it/s, loss=0.0449, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 427.01it/s, loss=0.0449, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 427.01it/s, loss=0.0449, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 427.01it/s, loss=0.0449, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 427.77it/s, loss=0.0431, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 427.77it/s, loss=0.0431, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 427.77it/s, loss=0.0431, v_num=0.0, ptl/val_loss=0.0199]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 427.77it/s, loss=0.0431, v_num=0.0, ptl/val_loss=0.0199]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 402.19it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116] \n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 402.19it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116] \n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 402.19it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116] \n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116]           \n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 402.19it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116] \n",
      "Epoch 2:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.039, v_num=0.0, ptl/val_loss=0.0116]           \n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 428.46it/s, loss=0.0387, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 428.46it/s, loss=0.0387, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 428.46it/s, loss=0.0387, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  12%|█▏        | 20/168 [00:00<00:00, 428.46it/s, loss=0.0387, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 425.99it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 425.99it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 425.99it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  24%|██▍       | 40/168 [00:00<00:00, 425.99it/s, loss=0.0349, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 426.80it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 426.80it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 426.80it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  36%|███▌      | 60/168 [00:00<00:00, 426.80it/s, loss=0.0325, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 428.92it/s, loss=0.0321, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 428.92it/s, loss=0.0321, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 428.92it/s, loss=0.0321, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  48%|████▊     | 80/168 [00:00<00:00, 428.92it/s, loss=0.0321, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 428.21it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 428.21it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 428.21it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  60%|█████▉    | 100/168 [00:00<00:00, 428.21it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 429.17it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 429.17it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 429.17it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  71%|███████▏  | 120/168 [00:00<00:00, 429.17it/s, loss=0.0309, v_num=0.0, ptl/val_loss=0.0116]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 403.04it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 403.04it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 403.04it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]           \n",
      "Epoch 2: 100%|██████████| 168/168 [00:00<00:00, 403.04it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0285, v_num=0.0, ptl/val_loss=0.00814]           \n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 442.75it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 442.75it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 442.75it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  12%|█▏        | 20/168 [00:00<00:00, 442.75it/s, loss=0.0296, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 442.68it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 442.68it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 442.68it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  24%|██▍       | 40/168 [00:00<00:00, 442.68it/s, loss=0.0293, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 440.53it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 440.53it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 440.53it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  36%|███▌      | 60/168 [00:00<00:00, 440.53it/s, loss=0.0264, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 442.79it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 442.79it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 442.79it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  48%|████▊     | 80/168 [00:00<00:00, 442.79it/s, loss=0.0252, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 437.48it/s, loss=0.0233, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 437.48it/s, loss=0.0233, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 437.48it/s, loss=0.0233, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  60%|█████▉    | 100/168 [00:00<00:00, 437.48it/s, loss=0.0233, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 434.87it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 434.87it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 434.87it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00814]\n",
      "Epoch 3:  71%|███████▏  | 120/168 [00:00<00:00, 434.87it/s, loss=0.0253, v_num=0.0, ptl/val_loss=0.00814]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 402.76it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 402.76it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 402.76it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]           \n",
      "Epoch 3: 100%|██████████| 168/168 [00:00<00:00, 402.76it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0227, v_num=0.0, ptl/val_loss=0.00685]           \n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 410.56it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 410.56it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 410.56it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  12%|█▏        | 20/168 [00:00<00:00, 410.56it/s, loss=0.023, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.78it/s, loss=0.0243, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.78it/s, loss=0.0243, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.78it/s, loss=0.0243, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  24%|██▍       | 40/168 [00:00<00:00, 415.78it/s, loss=0.0243, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 419.49it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 419.49it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 419.49it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  48%|████▊     | 80/168 [00:00<00:00, 419.49it/s, loss=0.0204, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 418.09it/s, loss=0.0197, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 418.09it/s, loss=0.0197, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 418.09it/s, loss=0.0197, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  60%|█████▉    | 100/168 [00:00<00:00, 418.09it/s, loss=0.0197, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 418.90it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 418.90it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 418.90it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00685]\n",
      "Epoch 4:  71%|███████▏  | 120/168 [00:00<00:00, 418.90it/s, loss=0.0214, v_num=0.0, ptl/val_loss=0.00685]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 393.36it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 393.36it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 393.36it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]           \n",
      "Epoch 4: 100%|██████████| 168/168 [00:00<00:00, 393.36it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0201, v_num=0.0, ptl/val_loss=0.00646]           \n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 415.85it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 415.85it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 415.85it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  12%|█▏        | 20/168 [00:00<00:00, 415.85it/s, loss=0.0211, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  24%|██▍       | 40/168 [00:00<00:00, 419.65it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  24%|██▍       | 40/168 [00:00<00:00, 419.65it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  24%|██▍       | 40/168 [00:00<00:00, 419.65it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  24%|██▍       | 40/168 [00:00<00:00, 419.65it/s, loss=0.0209, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 419.29it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 419.29it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 419.29it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  36%|███▌      | 60/168 [00:00<00:00, 419.29it/s, loss=0.0202, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 418.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 418.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 418.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  48%|████▊     | 80/168 [00:00<00:00, 418.47it/s, loss=0.0182, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.69it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.69it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.69it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  60%|█████▉    | 100/168 [00:00<00:00, 419.69it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 419.54it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 419.54it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 419.54it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00646]\n",
      "Epoch 5:  71%|███████▏  | 120/168 [00:00<00:00, 419.54it/s, loss=0.0189, v_num=0.0, ptl/val_loss=0.00646]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 384.50it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 384.50it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 384.50it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]           \n",
      "Epoch 5: 100%|██████████| 168/168 [00:00<00:00, 384.50it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0167, v_num=0.0, ptl/val_loss=0.00551]           \n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 418.31it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 418.31it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 418.31it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  12%|█▏        | 20/168 [00:00<00:00, 418.31it/s, loss=0.0186, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 427.48it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00551] \n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 427.48it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00551] \n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 427.48it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00551] \n",
      "Epoch 6:  24%|██▍       | 40/168 [00:00<00:00, 427.48it/s, loss=0.018, v_num=0.0, ptl/val_loss=0.00551] \n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 426.79it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 426.79it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 426.79it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  48%|████▊     | 80/168 [00:00<00:00, 426.79it/s, loss=0.0164, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  60%|█████▉    | 100/168 [00:00<00:00, 429.85it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  60%|█████▉    | 100/168 [00:00<00:00, 429.85it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  60%|█████▉    | 100/168 [00:00<00:00, 429.85it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  60%|█████▉    | 100/168 [00:00<00:00, 429.85it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 429.64it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 429.64it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 429.64it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00551]\n",
      "Epoch 6:  71%|███████▏  | 120/168 [00:00<00:00, 429.64it/s, loss=0.0152, v_num=0.0, ptl/val_loss=0.00551]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045] \n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045] \n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045] \n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]           \n",
      "Epoch 6: 100%|██████████| 168/168 [00:00<00:00, 404.60it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045] \n",
      "Epoch 7:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]           \n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 449.30it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 449.30it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 449.30it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  12%|█▏        | 20/168 [00:00<00:00, 449.30it/s, loss=0.016, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 448.39it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 448.39it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 448.39it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  24%|██▍       | 40/168 [00:00<00:00, 448.39it/s, loss=0.015, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 448.08it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 448.08it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 448.08it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  48%|████▊     | 80/168 [00:00<00:00, 448.08it/s, loss=0.0147, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 448.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 448.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 448.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  60%|█████▉    | 100/168 [00:00<00:00, 448.48it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 448.53it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 448.53it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 448.53it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.0045]\n",
      "Epoch 7:  71%|███████▏  | 120/168 [00:00<00:00, 448.53it/s, loss=0.0157, v_num=0.0, ptl/val_loss=0.0045]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 513.18it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 513.18it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 513.18it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 168/168 [00:00<00:00, 513.18it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]           \n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 451.88it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 451.88it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 451.88it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  12%|█▏        | 20/168 [00:00<00:00, 451.88it/s, loss=0.0151, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  24%|██▍       | 40/168 [00:00<00:00, 453.61it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  24%|██▍       | 40/168 [00:00<00:00, 453.61it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  24%|██▍       | 40/168 [00:00<00:00, 453.61it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  24%|██▍       | 40/168 [00:00<00:00, 453.61it/s, loss=0.0161, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 446.33it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 446.33it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 446.33it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  36%|███▌      | 60/168 [00:00<00:00, 446.33it/s, loss=0.0135, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 447.62it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 447.62it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 447.62it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  48%|████▊     | 80/168 [00:00<00:00, 447.62it/s, loss=0.0146, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 448.46it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 448.46it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 448.46it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  60%|█████▉    | 100/168 [00:00<00:00, 448.46it/s, loss=0.0133, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 449.12it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 449.12it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 449.12it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  71%|███████▏  | 120/168 [00:00<00:00, 449.12it/s, loss=0.0145, v_num=0.0, ptl/val_loss=0.00457]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 419.95it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 419.95it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 419.95it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]           \n",
      "Epoch 8: 100%|██████████| 168/168 [00:00<00:00, 419.95it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0139, v_num=0.0, ptl/val_loss=0.00346]           \n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 446.24it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 446.24it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 446.24it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  12%|█▏        | 20/168 [00:00<00:00, 446.24it/s, loss=0.0141, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  24%|██▍       | 40/168 [00:00<00:00, 447.75it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  24%|██▍       | 40/168 [00:00<00:00, 447.75it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  24%|██▍       | 40/168 [00:00<00:00, 447.75it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  24%|██▍       | 40/168 [00:00<00:00, 447.75it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 448.50it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 448.50it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 448.50it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  36%|███▌      | 60/168 [00:00<00:00, 448.50it/s, loss=0.0125, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 450.54it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 450.54it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 450.54it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Epoch 9:  48%|████▊     | 80/168 [00:00<00:00, 450.54it/s, loss=0.014, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 451.12it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 451.12it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 451.12it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  60%|█████▉    | 100/168 [00:00<00:00, 451.12it/s, loss=0.0138, v_num=0.0, ptl/val_loss=0.00346]\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 451.72it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 451.72it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 451.72it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  71%|███████▏  | 120/168 [00:00<00:00, 451.72it/s, loss=0.013, v_num=0.0, ptl/val_loss=0.00346] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 515.47it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]          \n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 515.47it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]          \n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 515.47it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]          \n",
      "Epoch 9: 100%|██████████| 168/168 [00:00<00:00, 515.47it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0118, v_num=0.0, ptl/val_loss=0.00351]          \n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 447.34it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 447.34it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 447.34it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  12%|█▏        | 20/168 [00:00<00:00, 447.34it/s, loss=0.0137, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 454.73it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 454.73it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 454.73it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  24%|██▍       | 40/168 [00:00<00:00, 454.73it/s, loss=0.0131, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 458.41it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 458.41it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 458.41it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  36%|███▌      | 60/168 [00:00<00:00, 458.41it/s, loss=0.0127, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 455.55it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00351] \n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 455.55it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00351] \n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 455.55it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00351] \n",
      "Epoch 10:  48%|████▊     | 80/168 [00:00<00:00, 455.55it/s, loss=0.012, v_num=0.0, ptl/val_loss=0.00351] \n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 456.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 456.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 456.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  60%|█████▉    | 100/168 [00:00<00:00, 456.69it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 455.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 455.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 455.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00351]\n",
      "Epoch 10:  71%|███████▏  | 120/168 [00:00<00:00, 455.18it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.00351]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 518.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039] \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 518.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039] \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 518.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039] \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039]           \n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 168/168 [00:00<00:00, 518.61it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039] \n",
      "Epoch 11:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0119, v_num=0.0, ptl/val_loss=0.0039]           \n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 439.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 439.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 439.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  12%|█▏        | 20/168 [00:00<00:00, 439.79it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 438.67it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 438.67it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 438.67it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  24%|██▍       | 40/168 [00:00<00:00, 438.67it/s, loss=0.0123, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 443.22it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 443.22it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 443.22it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  36%|███▌      | 60/168 [00:00<00:00, 443.22it/s, loss=0.0124, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 443.00it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 443.00it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 443.00it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  48%|████▊     | 80/168 [00:00<00:00, 443.00it/s, loss=0.0111, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 446.53it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 446.53it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 446.53it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0039]\n",
      "Epoch 11:  71%|███████▏  | 120/168 [00:00<00:00, 446.53it/s, loss=0.0126, v_num=0.0, ptl/val_loss=0.0039]\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902706)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 510.21it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 502.61it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 510.21it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 502.61it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 510.21it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 502.61it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 510.21it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Epoch 11: 100%|██████████| 168/168 [00:00<00:00, 502.61it/s, loss=0.0121, v_num=0.0, ptl/val_loss=0.00377]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 422.86it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 422.86it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 422.86it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 422.86it/s, loss=1.21, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 431.89it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 431.89it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 431.89it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 431.89it/s, loss=1.19, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 434.54it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 468.19it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 434.54it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 468.19it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 434.54it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 468.19it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 434.54it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 468.19it/s, loss=1.18, v_num=0.0, ptl/val_loss=0.914]\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1902952)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:   0%|          | 0/168 [00:00<?, ?it/s]              \n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 447.58it/s, loss=0.725, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 447.58it/s, loss=0.725, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 447.58it/s, loss=0.725, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  12%|█▏        | 20/168 [00:00<00:00, 447.58it/s, loss=0.725, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.92it/s, loss=0.678, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.92it/s, loss=0.678, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.92it/s, loss=0.678, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  36%|███▌      | 60/168 [00:00<00:00, 452.92it/s, loss=0.678, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 453.23it/s, loss=0.663, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 453.23it/s, loss=0.663, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 453.23it/s, loss=0.663, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  48%|████▊     | 80/168 [00:00<00:00, 453.23it/s, loss=0.663, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 454.37it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 454.37it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 454.37it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  60%|█████▉    | 100/168 [00:00<00:00, 454.37it/s, loss=0.631, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 452.32it/s, loss=0.666, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 452.32it/s, loss=0.666, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 452.32it/s, loss=0.666, v_num=0.0, ptl/val_loss=0.644]\n",
      "Epoch 0:  71%|███████▏  | 120/168 [00:00<00:00, 452.32it/s, loss=0.666, v_num=0.0, ptl/val_loss=0.644]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 438.58it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 438.58it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 438.58it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]           \n",
      "Epoch 0: 100%|██████████| 168/168 [00:00<00:00, 438.58it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.638, v_num=0.0, ptl/val_loss=0.573]           \n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 426.85it/s, loss=0.657, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 426.85it/s, loss=0.657, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 426.85it/s, loss=0.657, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  12%|█▏        | 20/168 [00:00<00:00, 426.85it/s, loss=0.657, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 424.48it/s, loss=0.643, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 424.48it/s, loss=0.643, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 424.48it/s, loss=0.643, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  24%|██▍       | 40/168 [00:00<00:00, 424.48it/s, loss=0.643, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 430.84it/s, loss=0.626, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 430.84it/s, loss=0.626, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 430.84it/s, loss=0.626, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  36%|███▌      | 60/168 [00:00<00:00, 430.84it/s, loss=0.626, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 435.06it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 435.06it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 435.06it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  48%|████▊     | 80/168 [00:00<00:00, 435.06it/s, loss=0.617, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 431.45it/s, loss=0.584, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 431.45it/s, loss=0.584, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 431.45it/s, loss=0.584, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  60%|█████▉    | 100/168 [00:00<00:00, 431.45it/s, loss=0.584, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 428.13it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 428.13it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 428.13it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1:  71%|███████▏  | 120/168 [00:00<00:00, 428.13it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 415.27it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 415.27it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 415.27it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "Epoch 1: 100%|██████████| 168/168 [00:00<00:00, 415.27it/s, loss=0.619, v_num=0.0, ptl/val_loss=0.573]\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "\u001B[2m\u001B[36m(pid=1903198)\u001B[0m \n",
      "                                                  \u001B[A\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:   0%|          | 0/84 [00:00<?, ?it/s]               \n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 421.52it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 421.52it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 421.52it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  24%|██▍       | 20/84 [00:00<00:00, 421.52it/s, loss=1.11, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 425.35it/s, loss=0.939, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 425.35it/s, loss=0.939, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 425.35it/s, loss=0.939, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  48%|████▊     | 40/84 [00:00<00:00, 425.35it/s, loss=0.939, v_num=0.0, ptl/val_loss=0.945]\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 429.57it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.945] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 429.57it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.945] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 429.57it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.945] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 60/84 [00:00<00:00, 429.57it/s, loss=0.84, v_num=0.0, ptl/val_loss=0.945] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 350.02it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 350.02it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 350.02it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]          \n",
      "Epoch 0: 100%|██████████| 84/84 [00:00<00:00, 350.02it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?it/s, loss=0.816, v_num=0.0, ptl/val_loss=0.636]          \n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 426.80it/s, loss=0.778, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 426.80it/s, loss=0.778, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 426.80it/s, loss=0.778, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  24%|██▍       | 20/84 [00:00<00:00, 426.80it/s, loss=0.778, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 426.11it/s, loss=0.704, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 426.11it/s, loss=0.704, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 426.11it/s, loss=0.704, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  48%|████▊     | 40/84 [00:00<00:00, 426.11it/s, loss=0.704, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 415.70it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 415.70it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 415.70it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 60/84 [00:00<00:00, 415.70it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/17 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 336.45it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 336.45it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 336.45it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Epoch 1: 100%|██████████| 84/84 [00:00<00:00, 336.45it/s, loss=0.661, v_num=0.0, ptl/val_loss=0.636]\n",
      "Best hyperparameters found were:  {'k': 92, 'lr': 0.00015822435381733385, 'batch_size': 24}\n",
      "Best achieved loss was:  {'loss': 0.0015353504568338394, 'time_this_iter_s': 0.32740092277526855, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 40, 'experiment_id': 'f06e9985e7f94d1ba2fd9d22b017effb', 'date': '2021-03-30_21-44-51', 'timestamp': 1617133491, 'time_total_s': 16.54563570022583, 'pid': 1892880, 'hostname': 'Desktop', 'node_ip': '192.168.1.35', 'config': {'k': 92, 'lr': 0.00015822435381733385, 'batch_size': 24}, 'time_since_restore': 16.54563570022583, 'timesteps_since_restore': 0, 'iterations_since_restore': 40, 'trial_id': '5a9af_00000', 'experiment_tag': '0_batch_size=24,k=92,lr=0.00015822'}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"k\":tune.randint(2, 256),\n",
    "    \"lr\": tune.loguniform(1e-3, 1e-7),\n",
    "    \"batch_size\":tune.choice([12, 24, 48])\n",
    "}\n",
    "\n",
    "test_sets = train([data_1, data_2, data_3], [\"pos\", \"rotMat\", \"velocity\"],\n",
    "                  config, n_samples=30, model_name=\"MLP-MLP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test encoding default to default, MSE=0.23\n",
      "Test encoding default to small, MSE=0.53\n",
      "Test encoding default to large, MSE=0.30\n",
      "Test encoding small to default, MSE=0.61\n",
      "Test encoding small to small, MSE=0.17\n",
      "Test encoding small to large, MSE=0.73\n",
      "Test encoding large to default, MSE=0.32\n",
      "Test encoding large to small, MSE=0.68\n",
      "Test encoding large to large, MSE=0.27\n"
     ]
    }
   ],
   "source": [
    "# clean_checkpoints()\n",
    "\n",
    "best_model = MLP.load_checkpoint(filename=\"/home/nuoc/checkpoints/MLP-MLP/MLP-MLP0.0016448386.92.pbz2\")\n",
    "test(best_model, test_sets, nn.functional.mse_loss, [\"default\", \"small\", \"large\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing pos, rotMat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare train data\n",
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data_1 = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n",
    "data_3 = func.load(data_path+\"LOCO_R2-default-locomotion-large.pbz2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"k\":tune.randint(2, 256),\n",
    "    \"lr\": tune.loguniform(1e-3, 1e-7),\n",
    "    \"batch_size\":tune.choice([12, 24, 48])\n",
    "}\n",
    "\n",
    "test_sets = train([data_1, data_2, data_3], [\"pos\", \"rotMat\", ],\n",
    "                  config, n_samples=20, model_name=\"MLP-MLP_POS_ROT\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir:  /home/nuoc/checkpoints\n",
      "Num checkpoints in ['MLP-MLP', 'yo']: 0\n",
      "dir:  /home/nuoc/checkpoints/MLP-MLP\n",
      "Num checkpoints in []: 142\n",
      "dir:  /home/nuoc/checkpoints/yo\n",
      "Num checkpoints in []: 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-b076517c3ec6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbest_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclean_checkpoints\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m# best_model = MLP.load_checkpoint(filename=best_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# test(best_model, test_sets, nn.functional.mse_loss, [\"default\", \"small\", \"large\"])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-9-9994b849d3da>\u001B[0m in \u001B[0;36mclean_checkpoints\u001B[0;34m(num_keep, path)\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Num checkpoints in {}: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msaved_checkpoints\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m         \u001B[0msaved_checkpoints\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m         \u001B[0;31m# for filename in saved_checkpoints[3:]:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[0;31m# os.remove(os.path.join(path, \".\".join(filename)))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-9-9994b849d3da>\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Num checkpoints in {}: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msaved_checkpoints\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m         \u001B[0msaved_checkpoints\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m         \u001B[0;31m# for filename in saved_checkpoints[3:]:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[0;31m# os.remove(os.path.join(path, \".\".join(filename)))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "best_model = clean_checkpoints()\n",
    "# best_model = MLP.load_checkpoint(filename=best_model)\n",
    "# test(best_model, test_sets, nn.functional.mse_loss, [\"default\", \"small\", \"large\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing pos, rotMat, velocity, angularVelocity,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare train data\n",
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data_1 = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n",
    "data_3 = func.load(data_path+\"LOCO_R2-default-locomotion-large.pbz2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"k\":tune.randint(2, 256),\n",
    "    \"lr\": tune.loguniform(1e-3, 1e-7),\n",
    "    \"batch_size\":tune.choice([12, 24, 48])\n",
    "}\n",
    "\n",
    "test_sets = train([data_1, data_2, data_3], [\"pos\", \"rotMat\", \"velocity\", \"angularVelocity\"],\n",
    "                  config, n_samples=20, model_name=\"MLP-MLP_POS_ROT_VEL_AVEL\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = clean_checkpoints()\n",
    "best_model = MLP.load_checkpoint(filename=best_model)\n",
    "test(best_model, test_sets, nn.functional.mse_loss, [\"default\", \"small\", \"large\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare train data\n",
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data_1 = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n",
    "data_3 = func.load(data_path+\"LOCO_R2-default-locomotion-large.pbz2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"k\":tune.randint(2, 256),\n",
    "    \"lr\": tune.loguniform(1e-3, 1e-7),\n",
    "    \"batch_size\":tune.choice([12, 24, 48])\n",
    "}\n",
    "\n",
    "test_sets = train([data_1, data_2, data_3], [\"currentValue\", \"targetValue\"],\n",
    "                  config, n_samples=20, model_name=\"MLP-MLP_cValue,tValue\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = clean_checkpoints()\n",
    "best_model = MLP.load_checkpoint(filename=best_model)\n",
    "test(best_model, test_sets, nn.functional.mse_loss, [\"default\", \"small\", \"large\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}