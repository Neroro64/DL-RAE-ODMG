{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../func\")\n",
    "sys.path.append(\"../autoencoder\")\n",
    "sys.path.append(\"../motion_generation_models\")\n",
    "\n",
    "from MoE import MoE\n",
    "from MoE_Z import MoE as MoE_Z\n",
    "import motion_generation\n",
    "from GRU import GRU\n",
    "from GRU_Z import GRU as GRU_Z\n",
    "from LSTM import LSTM\n",
    "from LSTM_Z import LSTM as LSTM_Z\n",
    "\n",
    "from MotionGeneration import MotionGenerationModel as MoGen\n",
    "from MotionGenerationR import MotionGenerationModel as MoGenR\n",
    "\n",
    "from MotionGenerationEmbedd import MotionGenerationModel as MoGenZ\n",
    "from MotionGenerationEmbeddR import MotionGenerationModel as MoGenZR\n",
    "\n",
    "from MotionGenerationVAE import MotionGenerationModel as MoGenVAE\n",
    "from MotionGenerationVAER import MotionGenerationModel as MoGenVAER\n",
    "\n",
    "from MotionGenerationVAE_Embedd import MotionGenerationModel as MoGenVAE_Z\n",
    "from MotionGenerationVAE_EmbeddR import MotionGenerationModel as MoGenVAE_ZR\n",
    "\n",
    "from MLP import MLP\n",
    "from MLP_Adversarial import MLP_ADV\n",
    "from MLP_MIX import MLP_MIX\n",
    "from MLP_MIX import MLP_layer\n",
    "from RBF import RBF\n",
    "from VAE import VAE\n",
    "from DEC import DEC\n",
    "\n",
    "from rig_agnostic_encoding.functions.DataProcessingFunctions import clean_checkpoints\n",
    "from GlobalSettings import MODEL_PATH, RESULTS_PATH\n",
    "import bz2\n",
    "from cytoolz import concat, sliding_window, accumulate\n",
    "from operator import add\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import func as F\n",
    "import _pickle as pickle\n",
    "import json as js\n",
    "import importlib\n",
    "import random\n",
    "import traceback\n",
    "import time\n",
    "import Extract as ext\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.signal as signal\n",
    "from timeit import default_timer as timer\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import func\n",
    "import MLP_Adversarial\n",
    "import MotionGeneration\n",
    "import MotionGenerationR\n",
    "import MotionGenerationEmbedd\n",
    "import MotionGenerationEmbeddR\n",
    "import MotionGenerationVAE\n",
    "import MotionGenerationVAER\n",
    "import MotionGenerationVAE_Embedd\n",
    "import MotionGenerationVAE_EmbeddR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_dim\": 256,                                  # dimension of the hidden layers\n",
    "    \"k\": 256,                                           # input dimension of the cluster layer\n",
    "    \"z_dim\": 128,                                       # dimension of the embeddings\n",
    "    \"lr\": 1e-4,                                         # learning rate\n",
    "    \"batch_size\": 32,                                   # batch size\n",
    "    \"keep_prob\": 0,                                     # dropout probability\n",
    "    \"loss_fn\":torch.nn.functional.mse_loss,             # use MSE as the default loss function\n",
    "    \"optimizer\":torch.optim.AdamW,                      # use AdamW as the default optimizer\n",
    "    \"scheduler\":torch.optim.lr_scheduler.StepLR,        # use StepLR as the default learning rate scheduler\n",
    "    \"scheduler_param\": {\"step_size\":80, \"gamma\":.9},    # default schedule is to decrease the learning rate with 0.9x after each 80 iterations\n",
    "    \"basis_func\":\"gaussian\",                            # the basis function for RBF layer is gaussian\n",
    "    \"n_centroid\":64,                                    # number of clusters\n",
    "    \"k_experts\": 4,                                     # number of experts\n",
    "    \"gate_size\": 128,                                   # dimension of the gate module\n",
    "    \"g_hidden_dim\": 512,                                # dimension of the experts\n",
    "    \"num_layers\": 4,                                    # number of layers for the LSTM model\n",
    "    \"autoregress_prob\":0,                               # teacher forced learning probability\n",
    "    \"autoregress_inc\":.5,                               # how much the probability is increased after each period\n",
    "    \"autoregress_ep\":10,                                # the period length\n",
    "    \"autoregress_max_prob\":1,                           # specify the maximum teacher forced learning probability\n",
    "    \"cost_hidden_dim\":128,                              # dimension of the hidden layer of the cost encoder\n",
    "    \"seq_len\":13,                                       # batch sequence length, eg. a clip of 299 frames is divided into 23 chunks with 13 frames each. The chunks are composed into a matrix of ( batch_sizex23x13 )\n",
    "    \"device\":\"cpu\",                                     # device on which the network is executed\n",
    "    \"use_label\":False                                   # whether to use the pose label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    \"\"\"\n",
    "    The all-in-one template function for loading pre-processed data, construct the network, train and test it. Save the best 3 checkpoints.\n",
    "    \n",
    "    Parameters: \n",
    "        path (str): the path to the pre-processed dataset\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, cost_dim, target_dim], feature_dims = get_pose_datasets(path)\n",
    "    upper_b = phase_dim+pose_dim\n",
    "    te_x, te_y = [tset[0][:, phase_dim:upper_b] for tset in test_set_p], [tset[1][:, phase_dim:upper_b] for tset in test_set_p]\n",
    "\n",
    "    h_dim = train_set_p[0][0].shape[0]\n",
    "    w_dim = train_set_p[0][0].shape[1]\n",
    "\n",
    "\n",
    "    ae_name = \"AE_R1\"\n",
    "    if \"R2\" in path:    ae_name = ae_name.replace(\"R1\", \"R2\")\n",
    "    elif \"R3\" in path:    ae_name = ae_name.replace(\"R1\", \"R3\")\n",
    "    elif \"R4\" in path:    ae_name = ae_name.replace(\"R1\", \"R4\")\n",
    "    elif \"R5\" in path:    ae_name = ae_name.replace(\"R1\", \"R5\")\n",
    "\n",
    "    ae = MLP_ADV(config=config, dimensions=[pose_dim], h_dim=h_dim, w_dim=w_dim,\n",
    "                 pos_dim=feature_dims[\"pos\"], rot_dim=feature_dims[\"rotMat2\"], vel_dim=feature_dims[\"velocity\"],\n",
    "                 train_set=train_set_p, val_set=val_set_p, test_set=test_set_p, name=ae_name)\n",
    "    logger=TensorBoardLogger(save_dir=\"RESULTS/\", name=ae_name, version=\"0.11\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=RESULTS_PATH,\n",
    "        gpus=1, precision=16,\n",
    "        min_epochs=20,\n",
    "        logger=logger,\n",
    "        max_epochs=200,\n",
    "    )\n",
    "\n",
    "    trainer.fit(ae)\n",
    "    trainer.test(ae)\n",
    "    p = ae.save_checkpoint(best_val_loss=\"final\")\n",
    "    clean_checkpoints(path=os.path.join(MODEL_PATH,ae_name))\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(path, train_prob=0.8):\n",
    "    \"\"\"\n",
    "    loads and partitions the pre-processed dataset\n",
    "    \n",
    "    Parameters:\n",
    "        path (str):         the path to the pre-processed dataset\n",
    "        train_prob (float): % of dataset to be used as training set\n",
    "    \n",
    "    Returns:\n",
    "        datasets (tuple):    (training_set, validation_set, test_set), where each of them is a TensorDataset (ie. list of (x, y))\n",
    "        dims (list):         a list of dimensions of the components of an input vector\n",
    "        feature_dims (dict): a dict of all dimensions of all components\n",
    "    \"\"\"\n",
    "    phase_features = [\"phase_vec_l2\"]\n",
    "    pose_features = [\"pos\", \"rotMat2\", \"velocity\"]\n",
    "    cost_features = [\"posCost\", \"rotCost\"]\n",
    "    target_features = [\"targetPosition\", \"targetRotation\"]\n",
    "\n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    tokens = path.split(\"/\")\n",
    "    tokens = tokens[-1].split(\"_\")\n",
    "    level = tokens[0]\n",
    "    name = tokens[1] + \"_\" + tokens[2]\n",
    "    name = name.replace(\".pbz2\", \"\")\n",
    "\n",
    "    obj = F.load(path)\n",
    "    data = obj[\"data\"]\n",
    "\n",
    "    feature_dims = data[0][1]\n",
    "    clips = [np.copy(d[0]) for d in data]\n",
    "\n",
    "    phase_dim = sum([feature_dims[feature] for feature in phase_features])\n",
    "    pose_dim = sum([feature_dims[feature] for feature in pose_features])\n",
    "    cost_dim = sum([feature_dims[feature] for feature in cost_features])\n",
    "    target_dim = sum([feature_dims[feature] for feature in target_features])\n",
    "    x_tensors = torch.stack([F.normaliseT(torch.from_numpy(clip[:-1])).float() for clip in clips])\n",
    "    y_tensors = torch.stack([torch.from_numpy(clip[1:]).float() for clip in clips])\n",
    "\n",
    "    dataset_p = TensorDataset(x_tensors, y_tensors)\n",
    "    N = len(x_tensors)\n",
    "\n",
    "    train_ratio = int(train_prob * N)\n",
    "    val_ratio = int((N - train_ratio) / 2.0)\n",
    "    test_ratio = N - train_ratio - val_ratio\n",
    "    train_set_p, val_set_p, test_set_p = random_split(dataset_p, [train_ratio, val_ratio, test_ratio],\n",
    "                                                      generator=torch.Generator().manual_seed(2021))\n",
    "    test_set_p += val_set_p\n",
    "    return (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, cost_dim, target_dim], feature_dims\n",
    "\n",
    "def get_pose_datasets(path):\n",
    "    \"\"\"\n",
    "    Same as the above version but it designed for training autoencoders. Only uses pose data. Input and labels are same data.\n",
    "    Loads and partitions the pre-processed pose dataset\n",
    "    \n",
    "    Parameters:\n",
    "        path (str):         the path to the pre-processed dataset\n",
    "    \n",
    "    Returns:\n",
    "        datasets (tuple):    (training_set, validation_set, test_set), where each of them is a TensorDataset (ie. list of (x, y))\n",
    "        dims (list):         a list of dimensions of the components of an input vector\n",
    "        feature_dims (dict): a dict of all dimensions of all components\n",
    "    \"\"\"\n",
    "\n",
    "    phase_features = [\"phase_vec_l2\"]\n",
    "    pose_features = [\"pos\", \"rotMat2\", \"velocity\"]\n",
    "    cost_features = [\"posCost\", \"rotCost\"]\n",
    "    target_features = [\"targetPosition\", \"targetRotation\"]\n",
    "\n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    tokens = path.split(\"/\")\n",
    "    tokens = tokens[-1].split(\"_\")\n",
    "    level = tokens[0]\n",
    "    name = tokens[1] + \"_\" + tokens[2]\n",
    "    name = name.replace(\".pbz2\", \"\")\n",
    "\n",
    "    obj = F.load(path)\n",
    "    data = obj[\"data\"]\n",
    "\n",
    "    feature_dims = data[0][1]\n",
    "    clips = [np.copy(d[0]) for d in data]\n",
    "\n",
    "    phase_dim = sum([feature_dims[feature] for feature in phase_features])\n",
    "    pose_dim = sum([feature_dims[feature] for feature in pose_features])\n",
    "    cost_dim = sum([feature_dims[feature] for feature in cost_features])\n",
    "    target_dim = sum([feature_dims[feature] for feature in target_features])\n",
    "    x_tensors = torch.stack([F.normaliseT(torch.from_numpy(clip[:-1])).float() for clip in clips])\n",
    "    y_tensors = torch.stack([torch.from_numpy(clip[1:]).float() for clip in clips])\n",
    "\n",
    "    pose_data = x_tensors[:, :, phase_dim:phase_dim + pose_dim]\n",
    "    dataset_p = TensorDataset(pose_data, pose_data)\n",
    "    N = len(x_tensors)\n",
    "\n",
    "    train_ratio = int(.8 * N)\n",
    "    val_ratio = int((N - train_ratio) / 2.0)\n",
    "    test_ratio = N - train_ratio - val_ratio\n",
    "    train_set_p, val_set_p, test_set_p = random_split(dataset_p, [train_ratio, val_ratio, test_ratio],\n",
    "                                                      generator=torch.Generator().manual_seed(2021))\n",
    "    test_set_p += val_set_p\n",
    "    return (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, cost_dim, target_dim], feature_dims\n",
    "\n",
    "def get_datasets_reduc(path, train_prob=0.8):\n",
    "    \"\"\"\n",
    "    Designed for training FS models.\n",
    "    loads and partitions the pre-processed dataset\n",
    "    \n",
    "    Parameters:\n",
    "        path (str):         the path to the pre-processed dataset\n",
    "        train_prob (float): % of dataset to be used as training set\n",
    "    \n",
    "    Returns:\n",
    "        datasets (tuple):        (training_set, validation_set, test_set), where each of them is a TensorDataset (ie. list of (x, y))\n",
    "        dims (list):             a list of dimensions of the components of an input vector\n",
    "        feature_dims (dict):     a dict of all dimensions of all components for the FE/FC models\n",
    "        feature_dims2 (dict):    a dict of all dimensions of all components for the FS models\n",
    "        ae (pl.LightningModule): a reference pre-trained autoencoder\n",
    "    \"\"\"\n",
    "\n",
    "    phase_features = [\"phase_vec_l2\"]\n",
    "    pose_features = [\"pos\", \"rotMat2\", \"velocity\"]\n",
    "    cost_features = [\"posCost\", \"rotCost\"]\n",
    "    target_features = [\"targetPosition\", \"targetRotation\"]\n",
    "\n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    tokens = path.split(\"/\")\n",
    "    tokens = tokens[-1].split(\"_\")\n",
    "    level = tokens[0]\n",
    "    name = tokens[1] + \"_\" + tokens[2]\n",
    "    name = name.replace(\".pbz2\", \"\")\n",
    "\n",
    "    path2 = path.replace(level + \"_F\", \"0_F\")\n",
    "    obj = F.load(path)\n",
    "    obj2 = F.load(path2)\n",
    "\n",
    "    data = obj[\"data\"]\n",
    "    data2 = obj2[\"data\"]\n",
    "\n",
    "    feature_dims = data[0][1]\n",
    "    feature_dims2 = data2[0][1]\n",
    "    clips = [np.copy(d[0]) for d in data]\n",
    "    clips2 = [np.copy(d[0]) for d in data2]\n",
    "\n",
    "    phase_dim = sum([feature_dims[feature] for feature in phase_features])\n",
    "    pose_dim = sum([feature_dims[feature] for feature in pose_features])\n",
    "    pose_dim2 = sum([feature_dims2[feature] for feature in pose_features])\n",
    "    cost_dim = sum([feature_dims[feature] for feature in cost_features])\n",
    "    target_dim = sum([feature_dims[feature] for feature in target_features])\n",
    "\n",
    "    x_tensors = torch.stack([F.normaliseT(torch.from_numpy(clip[:-1])).float() for clip in clips])\n",
    "    y_tensors = torch.stack([torch.from_numpy(clip[1:]).float() for clip in clips2])\n",
    "\n",
    "    ae_path = \"../models/AE_0_F_R1/0.0001.256.pbz2\"\n",
    "    ae_path1 = ae_path.replace(\"0_F_R1\", level + \"_\" + name)\n",
    "    ae_path2 = ae_path.replace(\"0_F_R1\", \"0_\" + name)\n",
    "    ae = MLP_ADV.load_checkpoint(ae_path1)\n",
    "    ae2 = MLP_ADV.load_checkpoint(ae_path2)\n",
    "\n",
    "    ae.decoder = ae2.decoder\n",
    "\n",
    "    dataset_p = TensorDataset(x_tensors, y_tensors)\n",
    "    N = len(x_tensors)\n",
    "\n",
    "    train_ratio = int(train_prob * N)\n",
    "    val_ratio = int((N - train_ratio) / 2.0)\n",
    "    test_ratio = N - train_ratio - val_ratio\n",
    "    train_set_p, val_set_p, test_set_p = random_split(dataset_p, [train_ratio, val_ratio, test_ratio],\n",
    "                                                      generator=torch.Generator().manual_seed(2021))\n",
    "    test_set_p += val_set_p\n",
    "    return (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, pose_dim2, cost_dim, target_dim], feature_dims, feature_dims2, ae\n",
    "\n",
    "def get_keyJoints(clip_path):\n",
    "    \"\"\"\n",
    "    returns a list of IDs of the keyjoints\n",
    "    \n",
    "    Parameters:\n",
    "        clip_path (str): path to the a clip sample\n",
    "    \n",
    "    Returns:\n",
    "        keyJoints (list): list of IDs of the keyjoints in the rig\n",
    "    \"\"\"\n",
    "    df = pickle.loads(F.load(clip_path))\n",
    "    keyJoints = []\n",
    "    f0 = df[\"frames\"][0]\n",
    "    for j in range(len(f0)):\n",
    "        if f0[j][\"key\"] and f0[j][\"rotCost\"].sum() > 0:\n",
    "            keyJoints.append(j)\n",
    "    return keyJoints\n",
    "\n",
    "\n",
    "def compute_delta(pose_x, pose_y, feature_dims):\n",
    "    \"\"\"\n",
    "    Given two clip (generated, ground_truth) with only pose data, computes the difference between frames for each of them separately.\n",
    "    Euclidean distance between the joint positions and angular difference between the joint orientations \n",
    "\n",
    "    Parameters:\n",
    "        pose_x (torch.nn.Tensor): the generated clip with only pose data containing the joint positions, rotations and linear velocity\n",
    "        pose_y (torch.nn.Tensor): the ground truth clip with only pose data containing the joint positions, rotations and linear velocity\n",
    "        feature_dims (dict):      a dict of dimensions of all components\n",
    "    Returns:\n",
    "        delta_px (torch.nn.Tensor): positional differences between the frames for pose_x \n",
    "        delta_py (torch.nn.Tensor): positional differences between the frames for pose_y\n",
    "        delta_rx (torch.nn.Tensor): rotational differences between the frames for pose_x\n",
    "        delta_ry (torch.nn.Tensor): rotational differences between the frames for pose_y\n",
    "    \"\"\"\n",
    "    shape = pose_y.shape\n",
    "\n",
    "    pos_dim = feature_dims[\"pos\"]\n",
    "    rot_dim = pos_dim + feature_dims[\"rotMat2\"]\n",
    "\n",
    "    px = pose_x[:, :pos_dim].reshape(shape[0], -1, 3)\n",
    "    py = pose_y[:, :pos_dim].reshape(shape[0], -1, 3)\n",
    "\n",
    "    rx = pose_x[:, pos_dim:rot_dim].reshape(shape[0], -1, 3, 2)\n",
    "    ry = pose_y[:, pos_dim:rot_dim].reshape(shape[0], -1, 3, 2)\n",
    "\n",
    "    delta_px = torch.cat([torch.mean(torch.sqrt(torch.sum((p1-p2)**2, dim=1))).unsqueeze(0) for p1, p2 in zip(px[:-1], px[1:])])\n",
    "    delta_py = torch.cat([torch.mean(torch.sqrt(torch.sum((p1-p2)**2, dim=1))).unsqueeze(0) for p1, p2 in zip(py[:-1], py[1:])])\n",
    "\n",
    "    delta_rx = torch.cat([torch.mean(torch.nan_to_num(\n",
    "        torch.arccos(torch.nn.functional.cosine_similarity(r1, r2, dim=1)),0)).unsqueeze(0) for r1, r2 in zip(rx[:-1], rx[1:])])\n",
    "    delta_ry = torch.cat([torch.mean(torch.nan_to_num(\n",
    "        torch.arccos(torch.nn.functional.cosine_similarity(r1, r2, dim=1)),0)).unsqueeze(0) for r1, r2 in zip(ry[:-1], ry[1:])])\n",
    "    return (delta_px, delta_py), (delta_rx, delta_ry)\n",
    "\n",
    "def compute_acc_cost(out, y, keyJoints, feature_dims, phase_dim, pose_dim):\n",
    "    \"\"\"\n",
    "    given the generate clip and targets, computes the accuracy, position cost, rotation cost \n",
    "    \"\"\"\n",
    "    pos_dim = phase_dim + feature_dims[\"pos\"]\n",
    "    rot_dim = pos_dim + feature_dims[\"rotMat2\"]\n",
    "    phase_pose_dim = phase_dim + pose_dim + feature_dims[\"posCost\"] + feature_dims[\"rotCost\"]\n",
    "    p_cost_dim = phase_pose_dim + feature_dims[\"targetPosition\"]\n",
    "    r_cost_dim = p_cost_dim + feature_dims[\"targetRotation\"]\n",
    "\n",
    "    pos_x = out[:, phase_dim:pos_dim]\n",
    "    pos_y = y[:, phase_dim:pos_dim]\n",
    "    lenKJ = len(keyJoints)\n",
    "\n",
    "    pos_xJ = [pos_x[:, 3*keyJoints[j]:3*keyJoints[j]+3] for j in range(lenKJ)]\n",
    "    pos_yJ = [pos_y[:, 3*keyJoints[j]:3*keyJoints[j]+3] for j in range(lenKJ)]\n",
    "\n",
    "    target_px = out[:, phase_pose_dim:p_cost_dim]\n",
    "    target_py = y[:, phase_pose_dim:p_cost_dim]\n",
    "\n",
    "    target_xJ = [target_px[:, 3*j:3*j+3] for j in range(lenKJ)]\n",
    "    target_yJ = [target_py[:, 3*j:3*j+3] for j in range(lenKJ)]\n",
    "\n",
    "    acc = []\n",
    "    d_sum_x, d_sum_y = [], []\n",
    "    pCost_x, pCost_y = [], []\n",
    "\n",
    "    for xj, txj in zip(pos_xJ, target_xJ):\n",
    "        dist = torch.sqrt(torch.sum((xj-txj)**2, dim=1))\n",
    "        pCost_x.append(torch.sum(dist))\n",
    "        dist[dist>=0.1] = 0\n",
    "        dist[dist>0] = 1\n",
    "        dist_sum = torch.sum(dist)\n",
    "        d_sum_x.append(dist_sum)\n",
    "\n",
    "    for xj, txj in zip(pos_yJ, target_yJ):\n",
    "        dist = torch.sqrt(torch.sum((xj-txj)**2, dim=1))\n",
    "        pCost_y.append(torch.sum(dist))\n",
    "        dist[dist>=0.1] = 0\n",
    "        dist[dist>0] = 1\n",
    "        dist_sum = torch.sum(dist)\n",
    "        d_sum_y.append(dist_sum)\n",
    "\n",
    "    for i in range(lenKJ):\n",
    "        dx, dy = d_sum_x[i], d_sum_y[i]\n",
    "        a = 1 - abs(dx/(dy+1) - 1)\n",
    "        acc.append(a)\n",
    "\n",
    "    rot_x = out[:, pos_dim:rot_dim]\n",
    "    rot_y = y[:, pos_dim:rot_dim]\n",
    "    rot_xJ = [rot_x[:, 6*keyJoints[j]:6*keyJoints[j]+6].reshape(-1, 3, 2) for j in range(lenKJ)]\n",
    "    rot_yJ = [rot_y[:, 6*keyJoints[j]:6*keyJoints[j]+6].reshape(-1, 3, 2) for j in range(lenKJ)]\n",
    "\n",
    "    target_rx = out[:, p_cost_dim:r_cost_dim]\n",
    "    target_ry = y[:, p_cost_dim:r_cost_dim]\n",
    "\n",
    "    target_rxJ = [target_rx[:, 6*j:6*j+6].reshape(-1, 3, 2) for j in range(lenKJ)]\n",
    "    target_ryJ = [target_ry[:, 6*j:6*j+6].reshape(-1, 3, 2) for j in range(lenKJ)]\n",
    "\n",
    "    rCost_x, rCost_y = [], []\n",
    "    for xj, txj in zip(rot_xJ, target_rxJ):\n",
    "        delta_r = torch.nan_to_num(torch.arccos(torch.nn.functional.cosine_similarity(xj, txj, dim=1)),0)\n",
    "        rCost_x.append(torch.sum(delta_r))\n",
    "\n",
    "    for xj, txj in zip(rot_yJ, target_ryJ):\n",
    "        delta_r = torch.nan_to_num(torch.arccos(torch.nn.functional.cosine_similarity(xj, txj, dim=1)),0)\n",
    "        rCost_y.append(torch.sum(delta_r))\n",
    "\n",
    "    return acc, pCost_x, pCost_y, rCost_x, rCost_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ae_results(ae, data_path):\n",
    "    \"\"\"\n",
    "a simple function for testing autoencoders\n",
    "    \"\"\"\n",
    "    (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, cost_dim, target_dim], feature_dims = get_datasets(data_path)\n",
    "    if \"R2\" in data_path:    ae_path = ae_path.replace(\"R1\", \"R2\")\n",
    "    elif \"R3\" in data_path:    ae_path = ae_path.replace(\"R1\", \"R3\")\n",
    "    elif \"R4\" in data_path:    ae_path = ae_path.replace(\"R1\", \"R4\")\n",
    "    elif \"R5\" in data_path:    ae_path = ae_path.replace(\"R1\", \"R5\")\n",
    "\n",
    "    ae = MLP_ADV.load_checkpoint(ae_path)\n",
    "\n",
    "\n",
    "    summary = ae.summarize()\n",
    "    upper_b = phase_dim+pose_dim\n",
    "    te_x, te_y = [tset[0][:, phase_dim:upper_b] for tset in test_set_p], [tset[1][:, phase_dim:upper_b] for tset in test_set_p]\n",
    "    recon_loss, adv_loss, pos_loss = [], [], []\n",
    "    rot_loss, delta_pos, delta_rot = [], [], []\n",
    "    elapsed_time = []\n",
    "    with torch.no_grad():\n",
    "        ae = ae.cpu()\n",
    "        for x, y in zip(te_x, te_y):\n",
    "            start = timer()\n",
    "            out = ae(x)\n",
    "            end = timer()\n",
    "\n",
    "            recon_l, pl, rl = ae.loss(out, y)\n",
    "            adv_l = 0.5 * torch.mean((ae.convDiscriminator(out.reshape(1,1,out.shape[0],-1)) - 1)** 2)\n",
    "            delta_p, delta_r = compute_delta(out, y, feature_dims=feature_dims)\n",
    "\n",
    "            elapsed_time.append(end-start)\n",
    "            recon_loss.append(recon_l)\n",
    "            adv_loss.append(adv_l)\n",
    "            pos_loss.append(pl)\n",
    "            rot_loss.append(rl)\n",
    "            delta_pos.append(delta_p)\n",
    "            delta_rot.append(delta_r)\n",
    "\n",
    "    result = dict(name=ae.name, params=summary.total_parameters, mem=summary.model_size,\n",
    "                  elapsed_times=elapsed_time, recon_error=recon_loss, adv_error=adv_loss,\n",
    "                  pos_error=pos_loss, rot_error=rot_loss, delta_pos=delta_pos, delta_rot=delta_rot)\n",
    "    return result\n",
    "\n",
    "def get_G(path):\n",
    "    \"\"\"\n",
    "    get the motion generator model (MoGenNet)\n",
    "    \"\"\"\n",
    "    if \"MoE\" in path:\n",
    "        if \"ZCAT\" in path:\n",
    "            return MoE_Z\n",
    "        else:\n",
    "            return MoE\n",
    "    elif \"LSTM\" in path:\n",
    "        if \"ZCAT\" in path:\n",
    "            return LSTM_Z\n",
    "        else:\n",
    "            return LSTM\n",
    "def get_C(path, config, pose_dim):\n",
    "    \"\"\"\n",
    "    get the cluster model\n",
    "    \"\"\"\n",
    "    if \"RBF\" in path: return RBF(config=config, input_dims=[pose_dim]).cluster_model\n",
    "    elif \"VAE\" in path: return VAE(config=config, input_dims=[pose_dim]).cluster_model\n",
    "    elif \"DEC\" in path: return DEC(config=config, input_dims=[pose_dim]).cluster_model\n",
    "    else: return MLP_layer()\n",
    "\n",
    "\n",
    "def get_M(path):\n",
    "    \"\"\"\n",
    "    get the MotionGenerationModel (OMG)\n",
    "    \"\"\"\n",
    "    if \"VAE\" in path:\n",
    "        if \"ZCAT\" in path:\n",
    "            return MoGenVAE_Z\n",
    "        else:\n",
    "            return MoGenVAE\n",
    "    else:\n",
    "        if \"ZCAT\" in path:\n",
    "            return MoGenZ\n",
    "        else:\n",
    "            return MoGen\n",
    "\n",
    "def get_template(path):\n",
    "    \"\"\"\n",
    "    Get a animation file template (.json). \n",
    "    \"\"\"\n",
    "    if \"R1\" in path:\n",
    "        template = js.load(open(\"R1_template.json\"))\n",
    "    elif \"R2\" in path:\n",
    "        template = js.load(open(\"R2_template.json\"))\n",
    "    elif \"R3\" in path:\n",
    "        template = js.load(open(\"R3_template.json\"))\n",
    "    elif \"R4\" in path:\n",
    "        template = js.load(open(\"R4_template.json\"))\n",
    "    elif \"R5\" in path:\n",
    "        template = js.load(open(\"R5_template.json\"))\n",
    "    else:\n",
    "        template = \"\"\n",
    "    return template\n",
    "\n",
    "def get_clip(dataPath):\n",
    "    \"\"\"\n",
    "    Get animation clip\n",
    "    \"\"\"\n",
    "    path = \"G:/data/Dataset_R1_Two_1/False_1_0.pbz2\"\n",
    "    candidates = [\"R2\", \"R3\", \"R4\", \"R5\"]\n",
    "    for candy in candidates:\n",
    "        if candy in dataPath:\n",
    "            path = path.replace(\"R1\", candy)\n",
    "            break\n",
    "    return path\n",
    "\n",
    "def compute_model_results(ae, dataset, data_path, model_path, model_name, template, clip_path_for_keyJ):\n",
    "    \"\"\"\n",
    "    A simple function for testing the OMG model\n",
    "    \"\"\"\n",
    "    (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, cost_dim, target_dim], feature_dims = dataset\n",
    "    label = \"R2\"\n",
    "    # if \"R2\" in data_path:    model_path = model_path.replace(\"R1\", \"R2\"); label = \"R2\"\n",
    "    if \"R3\" in data_path:    model_path = model_path.replace(\"R2\", \"R3\");label = \"R3\"\n",
    "    elif \"R4\" in data_path:    model_path = model_path.replace(\"R2\", \"R4\");label = \"R4\"\n",
    "    elif \"R5\" in data_path:    model_path = model_path.replace(\"R2\", \"R5\");label = \"R5\"\n",
    "\n",
    "    gModel = get_G(model_path)\n",
    "    cModel = get_C(model_path, config=config, pose_dim=pose_dim)\n",
    "    M = get_M(model_path)\n",
    "\n",
    "    with bz2.BZ2File(model_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    pose_autoencoder = MLP.load_checkpoint(obj[\"pose_autoencoder_path\"])\n",
    "    cost_encoder = MLP.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "    generationModel = gModel.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "\n",
    "    model = M(config=obj[\"config\"], feature_dims=obj[\"feature_dims\"], Model=gModel, pose_autoencoder=pose_autoencoder,\n",
    "                                      use_advLoss=obj[\"use_adv_loss\"],\n",
    "                                      input_slicers=obj[\"in_slices\"], output_slicers=obj[\"out_slices\"],\n",
    "                                      name=obj[\"name\"])\n",
    "\n",
    "    cModel.load_state_dict(obj[\"middle_layer_dict\"])\n",
    "    pose_autoencoder.convDiscriminator = ae.convDiscriminator\n",
    "\n",
    "    model.middle_layer = cModel\n",
    "    model.in_slices = obj[\"in_slices\"]\n",
    "    model.out_slices = obj[\"out_slices\"]\n",
    "    model.pose_autoencoder = pose_autoencoder\n",
    "    model.cost_encoder = cost_encoder\n",
    "    model.generationModel = generationModel\n",
    "\n",
    "    model = M.load_checkpoint(filename=model_path, Model=gModel, MiddleModel=cModel)\n",
    "    summary = model.summarize()\n",
    "    model = model.cpu()\n",
    "    model.generationModel.device=\"cpu\"\n",
    "    model.generationModel = model.generationModel.cpu()\n",
    "\n",
    "    recon_loss, adv_loss, pos_loss = [], [], []\n",
    "    rot_loss, delta_pos, delta_rot = [], [], []\n",
    "    acc, pCost, rCost, elapsed_time = [], [], [], []\n",
    "    \n",
    "    keyJoints = get_keyJoints(clip_path_for_keyJ)\n",
    "    \n",
    "    use_vae = \"VAE\" in model_path\n",
    "    with torch.no_grad():\n",
    "        for sample in test_set_p:\n",
    "            x, y = sample[0], sample[1]\n",
    "            shape = y.shape\n",
    "    \n",
    "            model.generationModel.reset_hidden(batch_size=y.shape[0])\n",
    "    \n",
    "            if use_vae:\n",
    "                start = timer()\n",
    "                out, z, mu, logvar = model(x)\n",
    "                end = timer()\n",
    "            else:\n",
    "                start = timer()\n",
    "                out, _ = model(x)\n",
    "                end = timer()\n",
    "    \n",
    "            out = torch.cat(out, dim=1)\n",
    "            pose_x = out[:, phase_dim:phase_dim+pose_dim]\n",
    "            pose_y = y[:, phase_dim:phase_dim+pose_dim]\n",
    "            recon_l, pl, rl = model.pose_autoencoder.loss(pose_x, pose_y)\n",
    "            adv_l = 0.5 * torch.mean((model.pose_autoencoder.convDiscriminator(pose_x.reshape(1,1,shape[0],-1)) - 1)** 2)\n",
    "    \n",
    "            delta_p, delta_r = compute_delta(out, y, feature_dims=feature_dims)\n",
    "            acc, pCost_x, pCost_y, rCost_x, rCost_y = compute_acc_cost(out, y, keyJoints, feature_dims, phase_dim, pose_dim)\n",
    "    \n",
    "            elapsed_time.append(end-start)\n",
    "            recon_loss.append(recon_l)\n",
    "            adv_loss.append(adv_l)\n",
    "            pos_loss.append(pl)\n",
    "            rot_loss.append(rl)\n",
    "            delta_pos.append(delta_p)\n",
    "            delta_rot.append(delta_r)\n",
    "            acc.append(acc)\n",
    "            pCost.append((pCost_x, pCost_y))\n",
    "            rCost.append((rCost_x, rCost_y))\n",
    "\n",
    "    F.local_generate_animation(model, test_set_p, feature_dims, template,target_dim, output_path=\"../animations/\"+model_name+\"_\"+label, use_vae=use_vae, n=3)\n",
    "    result = dict(name=model.name, params=summary.total_parameters, mem=summary.model_size,\n",
    "                  elapsed_times=elapsed_time, recon_error=recon_loss, adv_error=adv_loss,\n",
    "                  pos_error=pos_loss, rot_error=rot_loss, delta_pos=delta_pos, delta_rot=delta_rot,\n",
    "                  accuracy=acc, potCost=pCost, rotCost=rCost)\n",
    "    result = 0\n",
    "    return result\n",
    "\n",
    "def compute_model_results_reduc(ae, dataset, data_path, model_path, model_name, template, clip_path_for_keyJ):\n",
    "    \"\"\"\n",
    "    A simple function for testing FS-OMG\n",
    "    \"\"\"\n",
    "    (train_set_p, val_set_p, test_set_p), [phase_dim, pose_dim, pose_dim2, cost_dim, target_dim], feature_dims, feature_dims2, ae2 = dataset\n",
    "    ae3 = MLP(config=config, dimensions=[pose_dim])\n",
    "    pose_dim = pose_dim2\n",
    "    feature_dims=feature_dims2\n",
    "    label = \"R2\"\n",
    "    # if \"R2\" in data_path:    model_path = model_path.replace(label, \"R2\"); label = \"R2\"\n",
    "    if \"R3\" in data_path:    model_path = model_path.replace(label, \"R3\");label = \"R3\"\n",
    "    elif \"R4\" in data_path:    model_path = model_path.replace(label, \"R4\");label = \"R4\"\n",
    "    elif \"R5\" in data_path:    model_path = model_path.replace(label, \"R5\");label = \"R5\"\n",
    "\n",
    "    gModel = get_G(model_path)\n",
    "    cModel = get_C(model_path, config=config, pose_dim=pose_dim)\n",
    "    M = get_M(model_path)\n",
    "\n",
    "    with bz2.BZ2File(model_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    with bz2.BZ2File(obj[\"pose_autoencoder_path\"], \"rb\") as f:\n",
    "        mlp = pickle.load(f)\n",
    "    print(model_path)\n",
    "    # ae2.decoder = ae3.decoder\n",
    "    ae2.encoder = ae3.encoder\n",
    "    ae2.encoder.load_state_dict(mlp[\"encoder\"])\n",
    "    try:\n",
    "        ae2.decoder.load_state_dict(mlp[\"decoder\"])\n",
    "    except:\n",
    "        try:\n",
    "            ae3 = MLP(config=config, dimensions=[pose_dim])\n",
    "            ae2.decoder = ae3.decoder\n",
    "            ae2.decoder.load_state_dict(mlp[\"decoder\"])\n",
    "        except:\n",
    "            ae2.decoder = ae.decoder\n",
    "            ae2.decoder.load_state_dict(mlp[\"decoder\"])\n",
    "\n",
    "    pose_autoencoder = MLP_ADV.load_checkpoint(obj[\"pose_autoencoder_path\"])\n",
    "    ae2.convDiscriminator = ae.convDiscriminator\n",
    "    ae2 = pose_autoencoder\n",
    "    cost_encoder = MLP.load_checkpoint(obj[\"cost_encoder_path\"])\n",
    "    generationModel = gModel.load_checkpoint(obj[\"motionGenerationModelPath\"])\n",
    "\n",
    "    model = M(config=obj[\"config\"], feature_dims=obj[\"feature_dims\"], Model=gModel, pose_autoencoder=ae2,\n",
    "                                      use_advLoss=obj[\"use_adv_loss\"],\n",
    "                                      input_slicers=obj[\"in_slices\"], output_slicers=obj[\"out_slices\"],\n",
    "                                      name=obj[\"name\"])\n",
    "\n",
    "    cModel.load_state_dict(obj[\"middle_layer_dict\"])\n",
    "    pose_autoencoder.convDiscriminator = ae.convDiscriminator\n",
    "\n",
    "    model.middle_layer = cModel\n",
    "    model.in_slices = obj[\"in_slices\"]\n",
    "    model.out_slices = obj[\"out_slices\"]\n",
    "    model.pose_autoencoder = ae2\n",
    "    model.cost_encoder = cost_encoder\n",
    "    model.generationModel = generationModel\n",
    "\n",
    "    model = M.load_checkpoint(filename=model_path, Model=gModel, MiddleModel=cModel)\n",
    "    summary = model.summarize()\n",
    "    model = model.cpu()\n",
    "    model.generationModel.device=\"cpu\"\n",
    "    model.generationModel = model.generationModel.cpu()\n",
    "\n",
    "    recon_loss, adv_loss, pos_loss = [], [], []\n",
    "    rot_loss, delta_pos, delta_rot = [], [], []\n",
    "    acc, pCost, rCost, elapsed_time = [], [], [], []\n",
    "\n",
    "    keyJoints = get_keyJoints(clip_path_for_keyJ)\n",
    "\n",
    "    use_vae = \"VAE\" in model_path\n",
    "    with torch.no_grad():\n",
    "        for sample in test_set_p:\n",
    "            x, y = sample[0], sample[1]\n",
    "            shape = y.shape\n",
    "    \n",
    "            model.generationModel.reset_hidden(batch_size=y.shape[0])\n",
    "    \n",
    "            if use_vae:\n",
    "                start = timer()\n",
    "                out, z, mu, logvar = model(x)\n",
    "                end = timer()\n",
    "            else:\n",
    "                start = timer()\n",
    "                out, _ = model(x)\n",
    "                end = timer()\n",
    "    \n",
    "            out = torch.cat(out, dim=1)\n",
    "            pose_x = out[:, phase_dim:phase_dim+pose_dim]\n",
    "            pose_y = y[:, phase_dim:phase_dim+pose_dim]\n",
    "            recon_l, pl, rl = model.pose_autoencoder.loss(pose_x, pose_y)\n",
    "            adv_l = 0.5 * torch.mean((model.pose_autoencoder.convDiscriminator(pose_x.reshape(1,1,shape[0],-1)) - 1)** 2)\n",
    "    \n",
    "            delta_p, delta_r = compute_delta(out, y, feature_dims=feature_dims)\n",
    "            acc, pCost_x, pCost_y, rCost_x, rCost_y = compute_acc_cost(out, y, keyJoints, feature_dims, phase_dim, pose_dim)\n",
    "    \n",
    "            elapsed_time.append(end-start)\n",
    "            recon_loss.append(recon_l)\n",
    "            adv_loss.append(adv_l)\n",
    "            pos_loss.append(pl)\n",
    "            rot_loss.append(rl)\n",
    "            delta_pos.append(delta_p)\n",
    "            delta_rot.append(delta_r)\n",
    "            acc.append(acc)\n",
    "            pCost.append((pCost_x, pCost_y))\n",
    "            rCost.append((rCost_x, rCost_y))\n",
    "\n",
    "    F.local_generate_animation(model, test_set_p, feature_dims, template,target_dim, output_path=\"../animations/\"+model_name+\"_\"+label, use_vae=use_vae, n=3)\n",
    "    result = dict(name=model.name, params=summary.total_parameters, mem=summary.model_size,\n",
    "                  elapsed_times=elapsed_time, recon_error=recon_loss, adv_error=adv_loss,\n",
    "                  pos_error=pos_loss, rot_error=rot_loss, delta_pos=delta_pos, delta_rot=delta_rot,\n",
    "                  accuracy=acc, potCost=pCost, rotCost=rCost)\n",
    "    result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to the pre-processed datasets \n",
    "data_paths = [\n",
    "    \"../datasets/0_F_R1.pbz2\",\n",
    "    \"../datasets/0_F_R2.pbz2\",\n",
    "    \"../datasets/0_F_R3.pbz2\",\n",
    "    \"../datasets/0_F_R4.pbz2\",\n",
    "    \"../datasets/0_F_R5.pbz2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train autoencoders\n",
    "aes = [train(path) for path in data_paths]\n",
    "for ae in aes:\n",
    "    ae.save_checkpoint(best_val_loss=\"final\")   # save the trained version\n",
    "results = [compute_ae_results(ae, path) for ae, path in zip(aes,data_paths)]    # test the trained AEs\n",
    "F.save(results, \"ae_results\", \"../results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference OMG models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ref_model_paths= [\n",
    "    \"../models/AE_MoE_256_ZIN0_F_R1/final.pbz2\",            #AE+MoE\n",
    "    \"../models/AE_LSTM_256_ZIN0_F_R1_LSTM/final.pbz2\",      #AE+LSTM\n",
    "    \"../models/RBF_LSTM_256_ZCAT0_F_R1_LSTM/final.pbz2\",    #AE+RBF-CAT+LSTM\n",
    "    \"../models/RBF_LSTM_256_ZIN0_F_R1_LSTM/final.pbz2\",     #AE+RBF-IN+LSTM,\n",
    "    \"../models/RBF_MoE_256_ZCAT0_F_R1_ZCAT/final.pbz2\",     #AE+RBF-CAT+MoE\n",
    "    \"../models/RBF_MoE_256_ZINF_R1/final.pbz2\",             #AE+RBF-IN-MoE\n",
    "    \"../models/VAE_LSTM_256_ZCAT0_F_R1_LSTM/final.pbz2\",    #AE+VAE-CAT+LSTM\n",
    "    \"../models/VAE_LSTM_256_ZIN0_F_R1_LSTM/final.pbz2\",     #AE+VAE-IN+LSTM\n",
    "    \"../models/VAE_MoE_256_ZCAT0_F_R1_ZCAT/final.pbz2\",     #AE+VAE-CAT+MoE\n",
    "    \"../models/VAE_MoE_256_ZINF_R1/final.pbz2\",             #AE+VAE-IN+MoE\n",
    "    \"../models/DEC_MoE_256_ZCAT0_F_R1_ZCAT/final.pbz2\",     #AE+DEC-CAT+MoE\n",
    "    \"../models/DEC_MoE_256_ZINF_R1/final.pbz2\",             #AE+DEC-IN+MoE\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"AE+MoE\", \"AE+LSTM\", \"RBF-CAT+LSTM\", \"RBF-IN+LSTM\",\n",
    "    \"RBF-CAT+MoE\", \"RBF-IN+MoE\", \"VAE-CAT+LSTM\", \"VAE-IN+LSTM\",\n",
    "    \"VAE-CAT+MoE\", \"VAE-IN+MoE\", \"DEC-CAT+MoE\", \"DEC-IN+MoE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute results of reference models\n",
    "data = [get_datasets(path) for path in data_paths]\n",
    "results = []\n",
    "for i, dataPath in enumerate(data_paths):\n",
    "    result = {}\n",
    "    dataset = data[i]\n",
    "    ae = aes[i]\n",
    "    template = get_template(dataPath)\n",
    "    clip_path = get_clip(dataPath)\n",
    "\n",
    "    for model_path, model_name in zip(ref_model_paths, model_names):\n",
    "        model_result = compute_model_results(ae, dataset, dataPath, model_path, model_name, template,clip_path)\n",
    "        result[model_name] = model_result\n",
    "    results.append(result)\n",
    "\n",
    "F.save(results, \"reference_results\", \"../results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred OMG models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_models = {\n",
    "    \"AE+LSTM_RAW\":\"../models/AE_LSTM_256_AE_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"AE+MoE_RAW\":\"../models/AE_MoE_256_AE_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"DEC-CAT+MoE_RAW\":\"../models/DEC_MoE_256_ZCAT_0.10_RAW_F_R2_ZCAT/final.pbz2\",\n",
    "    \"DEC-IN+MoE_RAW\":\"../models/DEC_MoE_256_ZIN_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"RBF-IN+LSTM_RAW\":\"../models/RBF_LSTM_256_AE_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"RBF-CAT+LSTM_RAW\":\"../models/RBF_LSTM_256_ZCAT_0.10_RAW_F_R2_ZCAT/final.pbz2\",\n",
    "    \"RBF-CAT+MoE_RAW\":\"../models/RBF_MoE_256_ZCAT_0.10_RAW_F_R2_ZCAT/final.pbz2\",\n",
    "    \"RBF-IN+MoE_RAW\": \"../models/RBF_MoE_256_ZIN_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"VAE-IN+LSTM_RAW\":\"../models/VAE_LSTM_256_AE_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \"VAE-CAT+LSTM_RAW\":\"../models/VAE_LSTM_256_ZCAT_0.10_RAW_F_R2_ZCAT/final.pbz2\",\n",
    "    \"VAE-CAT+MoE_RAW\":\"../models/VAE_MoE_256_ZCAT_0.10_RAW_F_R2_ZCAT/final.pbz2\",\n",
    "    \"VAE-IN+MoE_RAW\":\"../models/VAE_MoE_256_ZIN_0.10_RAW_F_R2_ZIN/final.pbz2\",\n",
    "    \n",
    "    \"AE+LSTM_F\" : \"../models/AE_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"AE+LSTM_T\" : \"../models/AE_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "    \"AE+MoE_F\":\"../models/AE_MoE_256_AE_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"AE+MoE_T\":\"../models/AE_MoE_256_AE_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "\n",
    "    \"DEC-CAT+MoE_F\":\"../models/DEC_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT/final.pbz2\",\n",
    "    \"DEC-CAT+MoE_T\":\"../models/DEC_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT_trainable/final.pbz2\",\n",
    "    \"DEC-IN+MoE_F\":\"../models/DEC_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"DEC-IN+MoE_T\":\"../models/DEC_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "\n",
    "    \"RBF-CAT+LSTM_T\":\"../models/RBF_LSTM_256_ZCAT_0.10_R1_to_F_R2_ZCAT/final.pbz2\",\n",
    "    \"RBF-CAT+LSTM_F\":\"../models/RBF_LSTM_256_ZCAT_0.10_R1_to_F_R2_ZCAT_frozen/final.pbz2\",\n",
    "    \"RBF-IN+LSTM_F\":\"../models/RBF_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"RBF-IN+LSTM_T\":\"../models/RBF_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "    \"RBF-CAT+MoE_F\":\"../models/RBF_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT/final.pbz2\",\n",
    "    \"RBF-CAT+MoE_T\":\"../models/RBF_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT_trainable/final.pbz2\",\n",
    "    \"RBF-IN+MoE_F\":\"../models/RBF_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"RBF-IN+MoE_T\":\"../models/RBF_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "\n",
    "    \"VAE-CAT+LSTM_T\":\"../models/VAE_LSTM_256_ZCAT_0.10_R1_to_F_R2_ZCAT/final.pbz2\",\n",
    "    \"VAE-CAT+LSTM_F\":\"../models/VAE_LSTM_256_ZCAT_0.10_R1_to_F_R2_ZCAT_frozen/final.pbz2\",\n",
    "    \"VAE-IN+LSTM_F\":\"../models/VAE_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"VAE-IN+LSTM_T\":\"../models/VAE_LSTM_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "    \"VAE-CAT+MoE_F\":\"../models/VAE_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT/final.pbz2\",\n",
    "    \"VAE-CAT+MoE_T\":\"../models/VAE_MoE_256_ZCAT_0.10_R1_to_F_R2_ZCAT_trainable/final.pbz2\",\n",
    "    \"VAE-IN+MoE_F\":\"../models/VAE_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN/final.pbz2\",\n",
    "    \"VAE-IN+MoE_T\":\"../models/VAE_MoE_256_ZIN_0.10_R1_to_F_R2_ZIN_trainable/final.pbz2\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute results of the transferred OMG models\n",
    "data2 = [get_datasets(path, train_prob=0.16) for path in data_paths]\n",
    "transfer_results = []\n",
    "for i, dataPath in enumerate(data_paths):\n",
    "    dataPath = data_paths[i]\n",
    "    result = {}\n",
    "    ae = aes[i]\n",
    "    dataset = data2[i]\n",
    "    template = get_template(dataPath)\n",
    "    clip_path = get_clip(dataPath)\n",
    "\n",
    "    for model_name, model_path in t_models.items():\n",
    "        model_result = compute_model_results(ae, dataset, dataPath, model_path, model_name, template,clip_path)\n",
    "        result[model_name] = model_result\n",
    "    transfer_results.append(result)\n",
    "F.save(transfer_results, \"transfer_results\", \"../results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference FS-OMG models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aes_path = [\n",
    "    \"../models/AE_R1/final.256.pbz2\",\n",
    "    \"../models/AE_R2/final.256.pbz2\",\n",
    "    \"../models/AE_R3/final.256.pbz2\",\n",
    "    \"../models/AE_R4/final.256.pbz2\",\n",
    "    \"../models/AE_R5/final.256.pbz2\",\n",
    "]\n",
    "aes = [MLP_ADV.load_checkpoint(path) for path in aes_path]\n",
    "\n",
    "data_paths = [\n",
    "    \"../datasets/2_F_R1.pbz2\",\n",
    "    \"../datasets/2_F_R2.pbz2\",\n",
    "    \"../datasets/2_F_R3.pbz2\",\n",
    "    \"../datasets/2_F_R4.pbz2\",\n",
    "    \"../datasets/2_F_R5.pbz2\",\n",
    "]\n",
    "data = [get_datasets_reduc(path, train_prob=0.8) for path in data_paths]\n",
    "\n",
    "tr_models_ref = {\n",
    "    \"AE+MoE\":\"../models/AE_MoE_256_ZIN2_F_R1/final.pbz2\",\n",
    "    \"VAE-CAT+MoE\":\"../models/VAE_MoE_256_ZCAT2_F_R1_ZCAT/final.pbz2\",\n",
    "    \"DEC-CAT+MoE\":\"../models/DEC_MoE_256_ZCAT2_F_R1_ZCAT/final.pbz2\",\n",
    "    \"RBF-CAT+MoE\":\"../models/RBF_MoE_256_ZCAT2_F_R1_ZCAT/final.pbz2\",\n",
    "}\n",
    "\n",
    "transfer_results_reduc_ref = []\n",
    "\n",
    "for i, dataPath in enumerate(data_paths):\n",
    "    result = {}\n",
    "    ae = aes[i]\n",
    "    dataset = data3[i]\n",
    "    template = get_template(dataPath)\n",
    "    clip_path = get_clip(dataPath)\n",
    "\n",
    "    for model_name, model_path in tr_models_ref.items():\n",
    "        model_result = compute_model_results_reduc(ae, dataset, dataPath, model_path, model_name, template,clip_path)\n",
    "        result[model_name] = model_result\n",
    "    transfer_results_reduc_ref.append(result)\n",
    "F.save(transfer_results_reduc_ref, \"transfer_results_reduc_ref_2-5\", \"../results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred FS-OMG models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tr_models = {\n",
    "    \"AE+MoE_T\":\"../models/AE_MoE_256_AE_0.12_R1_to_F_R2_ZIN_reduced_trainable/final.pbz2\",\n",
    "    \"AE+MoE_F\":\"../models/AE_MoE_256_AE_0.12_R1_to_F_R2_ZIN_reduced/final.pbz2\",\n",
    "    \"DEC-CAT+MoE_F\":\"../models/DEC_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced/final.pbz2\",\n",
    "    \"DEC-CAT+MoE_T\":\"../models/DEC_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced_trainable/final.pbz2\",\n",
    "    \"RBF-CAT+MoE_F\":\"../models/RBF_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced/final.pbz2\",\n",
    "    \"RBF-CAT+MoE_T\":\"../models/RBF_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced_trainable/final.pbz2\",\n",
    "    \"VAE-CAT+MoE_F\":\"../models/VAE_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced/final.pbz2\",\n",
    "    \"VAE-CAT+MoE_T\":\"../models/VAE_MoE_256_ZCAT_0.12_R1_to_F_R2_ZCAT_reduced_trainable/final.pbz2\"\n",
    "}\n",
    "data = [get_datasets_reduc(path, train_prob=0.16) for path in data_paths]\n",
    "transfer_results_reduc = []\n",
    "for i, dataPath in enumerate(data_paths):\n",
    "    dataPath = data_paths[i]\n",
    "    print(i)\n",
    "    result = {}\n",
    "    ae = aes[i]\n",
    "    dataset = data4[i]\n",
    "    template = get_template(dataPath)\n",
    "    clip_path = get_clip(dataPath)\n",
    "\n",
    "    for model_name, model_path in tr_models.items():\n",
    "        model_result = compute_model_results_reduc(ae, dataset, dataPath, model_path, model_name, template,clip_path)\n",
    "        result[model_name] = model_result\n",
    "    transfer_results_reduc.append(result)\n",
    "F.save(transfer_results_reduc, \"transfer_results_reduc\", \"../results\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c42b2a52d5936a0241853e54acb448ce7a20a8615ca3981a651b06cd49090fb"
  },
  "kernelspec": {
   "display_name": "PyCharm (MEX)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
