{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DEC-MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses Deep Embedded Clustering (DEC) model for encoder\n",
    "\n",
    "$\n",
    "enc = dec(x, \\theta) = z, \\quad z \\in Z\n",
    "$\n",
    "\n",
    "$\n",
    "dec = mlp(X, \\theta), \\quad \\theta = W,b\n",
    "$\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from vlukiyanov on GitHub {https://github.com/vlukiyanov/pt-dec/tree/master/ptdec}\n",
    "\"\"\"\n",
    "from typing import Iterable, Tuple, Callable, Optional, Union, List\n",
    "from cytoolz.itertoolz import concat, sliding_window\n",
    "\n",
    "def build_units(\n",
    "    dimensions: Iterable[int], activation: Optional[torch.nn.Module]\n",
    ") -> List[torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Given a list of dimensions and optional activation, return a list of units where each unit is a linear\n",
    "    layer followed by an activation layer.\n",
    "    :param dimensions: iterable of dimensions for the chain\n",
    "    :param activation: activation layer to use e.g. nn.ReLU, set to None to disable\n",
    "    :return: list of instances of Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    def single_unit(in_dimension: int, out_dimension: int) -> torch.nn.Module:\n",
    "        unit = [(\"linear\", nn.Linear(in_dimension, out_dimension))]\n",
    "        if activation is not None:\n",
    "            unit.append((\"activation\", activation))\n",
    "        return nn.Sequential(OrderedDict(unit))\n",
    "\n",
    "    return [\n",
    "        single_unit(embedding_dimension, hidden_dimension)\n",
    "        for embedding_dimension, hidden_dimension in sliding_window(2, dimensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "def default_initialise_weight_bias_(\n",
    "    weight: torch.Tensor, bias: torch.Tensor, gain: float\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Default function to initialise the weights in a the Linear units of the StackedDenoisingAutoEncoder.\n",
    "    :param weight: weight Tensor of the Linear unit\n",
    "    :param bias: bias Tensor of the Linear unit\n",
    "    :param gain: gain for use in initialiser\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    nn.init.xavier_uniform_(weight, gain)\n",
    "    nn.init.constant_(bias, 0)\n",
    "\n",
    "\n",
    "class StackedDenoisingAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: List[int],\n",
    "        activation: torch.nn.Module = nn.ReLU(),\n",
    "        final_activation: Optional[torch.nn.Module] = nn.ReLU(),\n",
    "        weight_init: Callable[\n",
    "            [torch.Tensor, torch.Tensor, float], None\n",
    "        ] = default_initialise_weight_bias_,\n",
    "        gain: float = nn.init.calculate_gain(\"relu\"),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Autoencoder composed of a symmetric decoder and encoder components accessible via the encoder and decoder\n",
    "        attributes. The dimensions input is the list of dimensions occurring in a single stack\n",
    "        e.g. [100, 10, 10, 5] will make the embedding_dimension 100 and the hidden dimension 5, with the\n",
    "        autoencoder shape [100, 10, 10, 5, 10, 10, 100].\n",
    "        :param dimensions: list of dimensions occurring in a single stack\n",
    "        :param activation: activation layer to use for all but final activation, default torch.nn.ReLU\n",
    "        :param final_activation: final activation layer to use, set to None to disable, default torch.nn.ReLU\n",
    "        :param weight_init: function for initialising weight and bias via mutation, defaults to default_initialise_weight_bias_\n",
    "        :param gain: gain parameter to pass to weight_init\n",
    "        \"\"\"\n",
    "        super(StackedDenoisingAutoEncoder, self).__init__()\n",
    "        self.dimensions = dimensions\n",
    "        self.embedding_dimension = dimensions[0]\n",
    "        self.hidden_dimension = dimensions[-1]\n",
    "        # construct the encoder\n",
    "        encoder_units = build_units(self.dimensions[:-1], activation)\n",
    "        encoder_units.extend(\n",
    "            build_units([self.dimensions[-2], self.dimensions[-1]], None)\n",
    "        )\n",
    "        self.encoder = nn.Sequential(*encoder_units)\n",
    "        # construct the decoder\n",
    "        decoder_units = build_units(reversed(self.dimensions[1:]), activation)\n",
    "        decoder_units.extend(\n",
    "            build_units([self.dimensions[1], self.dimensions[0]], final_activation)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(*decoder_units)\n",
    "        # initialise the weights and biases in the layers\n",
    "        for layer in concat([self.encoder, self.decoder]):\n",
    "            weight_init(layer[0].weight, layer[0].bias, gain)\n",
    "\n",
    "    def get_stack(self, index: int) -> Tuple[torch.nn.Module, torch.nn.Module]:\n",
    "        \"\"\"\n",
    "        Given an index which is in [0, len(self.dimensions) - 2] return the corresponding subautoencoder\n",
    "        for layer-wise pretraining.\n",
    "        :param index: subautoencoder index\n",
    "        :return: tuple of encoder and decoder units\n",
    "        \"\"\"\n",
    "        if (index > len(self.dimensions) - 2) or (index < 0):\n",
    "            raise ValueError(\n",
    "                \"Requested subautoencoder cannot be constructed, index out of range.\"\n",
    "            )\n",
    "        return self.encoder[index].linear, self.decoder[-(index + 1)].linear\n",
    "\n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(batch)\n",
    "        return self.decoder(encoded)\n",
    "\n",
    "# Soft Assignment model\n",
    "class ClusterAssignment(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cluster_number: int,\n",
    "        embedding_dimension: int,\n",
    "        alpha: float = 1.0,\n",
    "        cluster_centers: Optional[torch.Tensor] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Module to handle the soft assignment, for a description see in 3.1.1. in Xie/Girshick/Farhadi,\n",
    "        where the Student's t-distribution is used measure similarity between feature vector and each\n",
    "        cluster centroid.\n",
    "        :param cluster_number: number of clusters\n",
    "        :param embedding_dimension: embedding dimension of feature vectors\n",
    "        :param alpha: parameter representing the degrees of freedom in the t-distribution, default 1.0\n",
    "        :param cluster_centers: clusters centers to initialise, if None then use Xavier uniform\n",
    "        \"\"\"\n",
    "        super(ClusterAssignment, self).__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.cluster_number = cluster_number\n",
    "        self.alpha = alpha\n",
    "        if cluster_centers is None:\n",
    "            initial_cluster_centers = torch.zeros(\n",
    "                self.cluster_number, self.embedding_dimension, dtype=torch.float\n",
    "            )\n",
    "            nn.init.xavier_uniform_(initial_cluster_centers)\n",
    "        else:\n",
    "            initial_cluster_centers = cluster_centers\n",
    "        self.cluster_centers = nn.Parameter(initial_cluster_centers)\n",
    "\n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the soft assignment for a batch of feature vectors, returning a batch of assignments\n",
    "        for each cluster.\n",
    "        :param batch: FloatTensor of [batch size, embedding dimension]\n",
    "        :return: FloatTensor [batch size, number of clusters]\n",
    "        \"\"\"\n",
    "        norm_squared = torch.sum((batch.unsqueeze(1) - self.cluster_centers) ** 2, 2)\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2\n",
    "        numerator = numerator ** power\n",
    "        return numerator / torch.sum(numerator, dim=1, keepdim=True)\n",
    "\n",
    "# DEC Layer\n",
    "class DEC(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cluster_number: int,\n",
    "        hidden_dimension: int,\n",
    "        encoder: torch.nn.Module,\n",
    "        alpha: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Module which holds all the moving parts of the DEC algorithm, as described in\n",
    "        Xie/Girshick/Farhadi; this includes the AutoEncoder stage and the ClusterAssignment stage.\n",
    "        :param cluster_number: number of clusters\n",
    "        :param hidden_dimension: hidden dimension, output of the encoder\n",
    "        :param encoder: encoder to use\n",
    "        :param alpha: parameter representing the degrees of freedom in the t-distribution, default 1.0\n",
    "        \"\"\"\n",
    "        super(DEC, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.cluster_number = cluster_number\n",
    "        self.alpha = alpha\n",
    "        self.assignment = ClusterAssignment(\n",
    "            cluster_number, self.hidden_dimension, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the cluster assignment using the ClusterAssignment after running the batch\n",
    "        through the encoder part of the associated AutoEncoder module.\n",
    "        :param batch: [batch size, embedding dimension] FloatTensor\n",
    "        :return: [batch size, number of clusters] FloatTensor\n",
    "        \"\"\"\n",
    "        return self.assignment(self.encoder(batch))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_p(q:torch.Tensor) -> torch.Tensor:\n",
    "        p = q**2 / q.sum(0)\n",
    "        return torch.autograd.Variable((p.t()/p.sum(1)).data, requires_grad=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(q:torch.Tensor, p:torch.Tensor):\n",
    "        return torch.sum(p*torch.log(p/q), dim=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module):\n",
    "        super(DEC_AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 63)\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "all_data = []\n",
    "for d in data:\n",
    "    d = pickle.loads(d)\n",
    "    pos = []\n",
    "    for f in d[\"frames\"]:\n",
    "        p = [jo[\"pos\"] for jo in f]\n",
    "        pos.append(p)\n",
    "    all_data.append(pos)\n",
    "\n",
    "input_data = np.array([np.concatenate([p for p in j]) for pos in all_data for j in pos])\n",
    "print(input_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train:  1007 , Validation:  216 , Test:  217\n"
     ]
    }
   ],
   "source": [
    "data_ratio = (.7, .15, .15) # training, validation, testing\n",
    "SEED = 2021\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(input_data).float()\n",
    "y_tensor = torch.from_numpy(input_data).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "N = len(dataset)\n",
    "\n",
    "train_ratio = int(data_ratio[0]*N)\n",
    "val_ratio = int(data_ratio[1] * N)\n",
    "test_ratio = int(N-train_ratio-val_ratio)\n",
    "print(\"Train: \", train_ratio, \", Validation: \", val_ratio, \", Test: \", test_ratio)\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEC_AE(\n",
      "  (encoder): DEC(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (linear): Linear(in_features=63, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=36, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (assignment): ClusterAssignment()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (linear): Linear(in_features=36, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=63, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_dim = input_data.shape[1]\n",
    "output_dim = input_data.shape[1]\n",
    "latent_dim = 36         # 12 * 3\n",
    "encoder_layer_sizes = [input_dim, 256, 256, latent_dim]\n",
    "decoder_layer_sizes = [latent_dim, 256, 256, output_dim]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "act_fn = \"elu\"\n",
    "keep_prob = .2\n",
    "\n",
    "# model, loss and scheduler\n",
    "ae = StackedDenoisingAutoEncoder(encoder_layer_sizes, activation=nn.ELU(), final_activation=nn.ELU())\n",
    "model = DEC_AE(DEC(36, 36, ae.encoder), ae.decoder)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1884\n",
      "Epoch [2/100], Loss: 0.1721\n",
      "Epoch [3/100], Loss: 0.1721\n",
      "Epoch [4/100], Loss: 0.1721\n",
      "Epoch [5/100], Loss: 0.1721\n",
      "Epoch [6/100], Loss: 0.1721\n",
      "Epoch [7/100], Loss: 0.1721\n",
      "Early stopping at Epoch:  6\n",
      "last training loss: 0.172100\n",
      "achieved best validation loss: 0.1820 after at Epoch 0\n",
      "Test loss: 0.1849\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "i = 0\n",
    "n_epochs_no_improve = 5\n",
    "\n",
    "train_loader_len = float(len(train_loader))\n",
    "val_loader_len = float(len(val_loader))\n",
    "test_loader_len = float(len(test_loader))\n",
    "\n",
    "last_avg_training_loss = 0\n",
    "min_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_after_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # training\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        training_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    last_avg_training_loss = training_loss / train_loader_len\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, last_avg_training_loss))\n",
    "\n",
    "    # early stopping\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            pred_val = model(inputs)\n",
    "            loss_val = criterion(pred_val, labels)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= val_loader_len\n",
    "        if min_loss > val_loss:\n",
    "            min_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_after_epoch = epoch\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve > n_epochs_no_improve:\n",
    "                print(\"Early stopping at Epoch: \", epoch)\n",
    "                print(\"last training loss: {:2f}\".format(last_avg_training_loss))\n",
    "                print(\"achieved best validation loss: {:.4f} after at Epoch {}\".format(min_loss, best_model_after_epoch))\n",
    "                break\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        pred_test = model(inputs)\n",
    "        loss_test = criterion(pred_test, labels)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= test_loader_len\n",
    "    print(\"Test loss: {:.4f}\".format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}