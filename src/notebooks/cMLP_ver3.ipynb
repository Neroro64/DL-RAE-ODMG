{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import shutil\n",
    "import tempfile\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from cytoolz import sliding_window\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses simple Multi-layered perceptron (MLP) for both encoder and decoder.\n",
    "\n",
    "$\n",
    "enc = dec = mlp(X, \\theta), \\quad \\theta = W, b\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-5190da91fac0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Test torch lightning + ray tune\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mMLP\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLightningModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     def __init__(self, config:dict=None, dimensions:list=None, loss_fn=None,\n\u001B[1;32m      5\u001B[0m                  \u001B[0mdataset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_set\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_set\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_set\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-5190da91fac0>\u001B[0m in \u001B[0;36mMLP\u001B[0;34m()\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_checkpoint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0msave_checkpoint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mMODEL_PATH\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m         \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m         model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MODEL_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Test torch lightning + ray tune\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, config:dict=None, dimensions:list=None, loss_fn=None,\n",
    "                 dataset=None, train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, name:str=\"model\", load=False,\n",
    "                 single_module:int=0):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.name = name\n",
    "        self.dimensions = dimensions\n",
    "        self.keep_prob = keep_prob\n",
    "        self.single_module = single_module\n",
    "\n",
    "        if load:\n",
    "            self.build()\n",
    "        else:\n",
    "            self.k = config[\"k\"]\n",
    "            self.learning_rate = config[\"lr\"]\n",
    "            dimensions.append(self.k)\n",
    "            self.dimensions = dimensions\n",
    "            self.loss_fn = loss_fn\n",
    "            self.keep_prob = keep_prob          #   %\n",
    "            self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "            self.dataset = dataset\n",
    "\n",
    "            self.train_set = train_set\n",
    "            self.val_set = val_set\n",
    "            self.test_set = test_set\n",
    "            self.best_val_loss = np.inf\n",
    "\n",
    "            self.build()\n",
    "            if self.train_set is None:\n",
    "                self.setup_data([.7, .15, .15])\n",
    "\n",
    "            self.encoder.apply(self.init_params)\n",
    "            self.decoder.apply(self.init_params)\n",
    "\n",
    "    def build(self):\n",
    "        layer_sizes = list(sliding_window(2, self.dimensions))\n",
    "        if self.single_module == -1 or self.single_module == 0:\n",
    "            layers = []\n",
    "\n",
    "            for i, size in enumerate(layer_sizes):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.encoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.encoder = nn.Sequential()\n",
    "\n",
    "        if self.single_module == 0 or self.single_module == 1:\n",
    "            layers = []\n",
    "            for i, size in enumerate(layer_sizes[-1::-1]):\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[1], size[0])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), nn.ELU()))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "            self.decoder = nn.Sequential(OrderedDict(layers))\n",
    "        else:\n",
    "            self.decoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/val_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "\n",
    "        self.log('ptl/test_loss', loss, prog_bar=True)\n",
    "        return {\"val_loss\":loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir=MODEL_PATH):\n",
    "        path = os.path.join(checkpoint_dir, self.name)\n",
    "        model = {\"k\":self.k, \"dimensions\":self.dimensions,\"keep_prob\":self.keep_prob, \"name\":self.name,\n",
    "                 \"encoder\":self.encoder.state_dict(),\n",
    "                 \"decoder\":self.decoder.state_dict()}\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        with bz2.BZ2File(os.path.join(path,\n",
    "                                      str(self.best_val_loss.cpu().numpy())+\".\"+str(self.k)+\".pbz2\"), \"w\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(filename):\n",
    "        # return torch.load(os.path.join(checkpoint_dir,filename))\n",
    "        with bz2.BZ2File(filename, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        model = MLP(name=obj[\"name\"], dimensions=obj[\"dimensions\"], load=True)\n",
    "        model.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        model.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        return model\n",
    "        # self.encoder.load_state_dict(obj[\"encoder\"])\n",
    "        # self.decoder.load_state_dict(obj[\"decoder\"])\n",
    "        # self.best_val_loss = obj[\"val_loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def setup_data(self, split_ratio, N):\n",
    "        self.n_train_samples= int(split_ratio[0]*N)\n",
    "        self.n_val_samples= int(split_ratio[1] * N)\n",
    "        self.n_test_samples= int(N-self.n_train_samples-self.n_val_samples)\n",
    "        self.train_set, self.val_set, self.test_set = random_split(self.dataset,\n",
    "                                                                   [self.n_train_samples,\n",
    "                                                                    self.n_val_samples,\n",
    "                                                                    self.n_test_samples])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train_tune(config, dimensions:list,  loss_fn=None,\n",
    "                 dataset=None, train_set=None, val_set=None, test_set=None,\n",
    "                 keep_prob:float=.2, num_epochs=300, num_cpus=24, num_gpus=1, model_name=\"model\"):\n",
    "\n",
    "    model = MLP(config, dimensions, loss_fn, dataset, train_set, val_set, test_set, keep_prob, name=model_name)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(save_dir=\"logs/\", name=model_name, version=\"0.0\"),\n",
    "        progress_bar_refresh_rate=20,\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"loss\":\"avg_val_loss\",}, on=\"validation_end\"),\n",
    "            EarlyStopping(monitor=\"avg_val_loss\")\n",
    "        ],\n",
    "        precision=16,\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "def normalise(x:torch.Tensor):\n",
    "    std = torch.std(x)\n",
    "    std[std==0] = 1\n",
    "    return (x - torch.mean(x)) / std\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def prepare_data(datasets:list, featureList:list,\n",
    "                 train_ratio:float=0.8, val_ratio:float=0.2, test_size:int=100, SEED:int=2021):\n",
    "   # process data\n",
    "    data = [func.processData(d, featureList) for d in datasets]\n",
    "    input_data = [np.vstack(d) for d in data]\n",
    "    x_tensors = [func.normaliseT(torch.from_numpy(x).float()) for x in input_data]\n",
    "    y_tensors = [torch.from_numpy(x).float() for x in input_data]\n",
    "\n",
    "    # prepare datasets\n",
    "    test_sets = [(x_tensor[-test_size:], y_tensor[-test_size:]) for x_tensor, y_tensor in zip(x_tensors, y_tensors)]\n",
    "    x_training = torch.vstack([x_tensor[:-test_size] for x_tensor in x_tensors])\n",
    "    y_training = torch.vstack([y_tensor[:-test_size] for y_tensor in y_tensors])\n",
    "    dataset = TensorDataset(x_training, y_training)\n",
    "    N = len(x_training)\n",
    "\n",
    "    train_ratio = int(train_ratio*N)\n",
    "    val_ratio = int(val_ratio*N)\n",
    "    print(\"Train: \", train_ratio, \", Validation: \", val_ratio)\n",
    "    train_set, val_set = random_split(dataset, [train_ratio, val_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "    return train_set, val_set, test_sets\n",
    "\n",
    "def train(train_set:Dataset, val_set:Dataset,\n",
    "          config:dict, EPOCHS:int=300, hidden_dim:int=256,\n",
    "          loss_fn=torch.nn.functional.mse_loss, keep_prob=0.2,\n",
    "          n_gpu=1, n_samples=20, model_name=\"model\",\n",
    "          ):\n",
    "\n",
    "    # define hyperparameters\n",
    "    input_dim = train_set[0][0].size()[-1]\n",
    "    output_dim = input_dim\n",
    "\n",
    "    dimensions = [input_dim, hidden_dim]\n",
    "\n",
    "    scheduler = ASHAScheduler(max_t = EPOCHS, grace_period=1, reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"k\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"],\n",
    "        max_error_rows=5,\n",
    "        max_progress_rows=5,\n",
    "        max_report_frequency=10)\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_tune,\n",
    "            dimensions = dimensions,\n",
    "            loss_fn = loss_fn,\n",
    "            train_set = train_set, val_set = val_set,\n",
    "            keep_prob = keep_prob,\n",
    "            num_epochs = EPOCHS,\n",
    "            num_gpus=n_gpu,\n",
    "            model_name=model_name\n",
    "        ),\n",
    "        resources_per_trial= {\"cpu\":1, \"gpu\":n_gpu},\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=n_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"MLP-MLP\",\n",
    "        verbose=False,\n",
    "        checkpoint_freq=0,\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"loss\",\n",
    "        checkpoint_at_end=True\n",
    "    )\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(\"Done\")\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    print(\"Best achieved loss was: \", analysis.best_result)\n",
    "    print(\"-\"*70)\n",
    "\n",
    "def clean_checkpoints(num_keep=3, path=\"../../models\"):\n",
    "    saved_checkpoints = []\n",
    "    for dir, dname, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            fname = fname.split(\".\")\n",
    "            saved_checkpoints.append(fname)\n",
    "        print(\"Num checkpoints in {}: {}\".format(dir, len(saved_checkpoints)))\n",
    "\n",
    "        saved_checkpoints.sort(key = lambda x: x[0]+x[1])\n",
    "        for filename in saved_checkpoints[num_keep:]:\n",
    "            os.remove(os.path.join(dir,\".\".join(filename)))\n",
    "        break\n",
    "    return os.path.join(path, \".\".join(saved_checkpoints[0]))\n",
    "\n",
    "def test(model:torch.nn.Module, test_sets:list, loss_fn, set_names=list,\n",
    "         save=True, path=\"../../results\"):\n",
    "    # Intra test performance\n",
    "    df = {}\n",
    "    if set_names is None: set_names = np.arange(len(test_sets))\n",
    "    for i,t1 in enumerate(test_sets):\n",
    "        for j, t2 in enumerate(test_sets):\n",
    "            x = t1[0]\n",
    "            y = t2[1]\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(x, y)\n",
    "            df[\"{}-{}\".format(set_names[i], set_names[j])] = [loss.cpu().numpy()]\n",
    "            print(\"Test encoding {} to {}, MSE={:.2f}\".format(set_names[i], set_names[j], loss))\n",
    "    if save:\n",
    "        if not os.path.exists(path): os.mkdir(path)\n",
    "        pd.DataFrame(df).to_csv(os.path.join(path, \"tests.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing pos, rotMat, velocity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Prepare train data\n",
    "DATA_PATH = \"/home/nuoc/Documents/MEX/data\"\n",
    "MODEL_PATH = \"/home/nuoc/Documents/MEX/models\"\n",
    "RESULTS_PATH = \"/home/nuoc/Documents/MEX/results\"\n",
    "\n",
    "def train_single_model(datapaths:list, featureList:list,config:dict=None,\n",
    "                       n_samples:int=30, model_name:str=\"model\", loss_fn=nn.functional.mse_loss,\n",
    "                       dataset_names:list=None):\n",
    "    # load data\n",
    "    datasets = [func.load(os.path.join(DATA_PATH,path)) for path in datapaths]\n",
    "    train_set, val_set, test_set = prepare_data(datasets, featureList)\n",
    "    # train(train_set=train_set, val_set=val_set, config=config, loss_fn=loss_fn,\n",
    "    #       n_samples=n_samples, model_name=model_name)\n",
    "\n",
    "    best_model = clean_checkpoints(path=os.path.join(MODEL_PATH, model_name))\n",
    "    best_model = MLP.load_checkpoint(best_model)\n",
    "    test(best_model, test_set, loss_fn=loss_fn, set_names=dataset_names, path=os.path.join(RESULTS_PATH, model_name))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "datapaths = [\"LOCO_R1-default-locomotion.pbz2\",\n",
    "             \"LOCO_R1-default-locomotion-small.pbz2\",\n",
    "             \"LOCO_R1-default-locomotion-large.pbz2\"]\n",
    "featureList = [\"pos\", \"rotMat\", \"velocity\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-31 11:58:50,610\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8266\u001B[39m\u001B[22m\n",
      "2021-03-31 11:58:54,066\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8266\u001B[39m\u001B[22m\n",
      "2021-03-31 11:58:57,439\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8266\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  3216 , Validation:  804\n",
      "Num checkpoints in /home/nuoc/Documents/MEX/models/MLP-MLP_R1: 2\n",
      "Test encoding default to default, MSE=0.12\n",
      "Test encoding default to small, MSE=0.38\n",
      "Test encoding default to large, MSE=0.16\n",
      "Test encoding small to default, MSE=0.38\n",
      "Test encoding small to small, MSE=0.07\n",
      "Test encoding small to large, MSE=0.46\n",
      "Test encoding large to default, MSE=0.18\n",
      "Test encoding large to small, MSE=0.49\n",
      "Test encoding large to large, MSE=0.13\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"k\":tune.randint(2, 256),\n",
    "    \"lr\": tune.loguniform(1e-3, 1e-7),\n",
    "    \"batch_size\":tune.choice([12, 24, 48])\n",
    "}\n",
    "\n",
    "train_single_model(datapaths=datapaths, featureList=featureList, config=config,\n",
    "                   model_name=\"MLP-MLP_R1\", dataset_names=[\"default\", \"small\", \"large\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}