{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VaDE-MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses Variational Deep Embedding model for encoder\n",
    "\n",
    "$\n",
    "enc = VaDE(x, \\theta) = \\mu, \\sigma, \\quad \\mu, \\sigma = [\\mu_1, \\mu_2, ..., \\mu_c], [\\sigma_1, \\sigma_2, ..., \\sigma_c]\n",
    "$\n",
    "\n",
    "$\n",
    "z_c \\sim  N(\\mu_c, \\sigma_c I)\n",
    "$\n",
    "\n",
    "$\n",
    "dec = mlp(X, \\theta), \\quad \\theta = W,b\n",
    "$\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from eelxpeng {https://github.com/eelxpeng/UnsupervisedDeepLearning-Pytorch/blob/master/udlp/clustering/vade.py}\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def buildNetwork(layers, activation=\"relu\", dropout=0):\n",
    "    net = []\n",
    "    for i in range(1, len(layers)):\n",
    "        net.append(nn.Linear(layers[i-1], layers[i]))\n",
    "        if activation==\"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        elif activation==\"sigmoid\":\n",
    "            net.append(nn.Sigmoid())\n",
    "        if dropout > 0:\n",
    "            net.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "def adjust_learning_rate(init_lr, optimizer, epoch):\n",
    "    lr = max(init_lr * (0.9 ** (epoch//10)), 0.0002)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"l\"] = lr\n",
    "    return lr\n",
    "\n",
    "log2pi = math.log(2*math.pi)\n",
    "def log_likelihood_samples_unit_gaussian(samples):\n",
    "    return -0.5*log2pi*samples.size()[1] - torch.sum(0.5*(samples)**2, 1)\n",
    "\n",
    "def log_likelihood_samplesImean_sigma(samples, mu, logvar):\n",
    "    return -0.5*log2pi*samples.size()[1] - torch.sum(0.5*(samples-mu)**2/torch.exp(logvar) + 0.5*logvar, 1)\n",
    "\n",
    "class VaDE(nn.Module):\n",
    "    def __init__(self, input_dim=784, z_dim=10, n_centroids=10, binary=False,\n",
    "        encodeLayer=[500,500,2000], decodeLayer=[2000,500,500]):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_centroids = n_centroids\n",
    "        self.encoder = buildNetwork([input_dim] + encodeLayer)\n",
    "        self.decoder = buildNetwork([z_dim] + decodeLayer)\n",
    "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        self._enc_log_sigma = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        self._dec = nn.Linear(decodeLayer[-1], input_dim)\n",
    "        self._dec_act = None\n",
    "        if binary:\n",
    "            self._dec_act = nn.Sigmoid()\n",
    "\n",
    "        self.create_gmmparam(n_centroids, z_dim)\n",
    "\n",
    "    def create_gmmparam(self, n_centroids, z_dim):\n",
    "        self.theta_p = nn.Parameter(torch.ones(n_centroids)/n_centroids)\n",
    "        self.u_p = nn.Parameter(torch.zeros(z_dim, n_centroids))\n",
    "        self.lambda_p = nn.Parameter(torch.ones(z_dim, n_centroids))\n",
    "\n",
    "    def initialize_gmm(self, dataloader):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "        self.eval()\n",
    "        data = []\n",
    "        for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            z, outputs, mu, logvar = self.forward(inputs)\n",
    "            data.append(z.data.cpu().numpy())\n",
    "        data = np.concatenate(data)\n",
    "        gmm = GaussianMixture(n_components=self.n_centroids,covariance_type='diag')\n",
    "        gmm.fit(data)\n",
    "        self.u_p.data.copy_(torch.from_numpy(gmm.means_.T.astype(np.float32)))\n",
    "        self.lambda_p.data.copy_(torch.from_numpy(gmm.covariances_.T.astype(np.float32)))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "          std = logvar.mul(0.5).exp_()\n",
    "          eps = Variable(std.data.new(std.size()).normal_())\n",
    "          return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "          return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder(z)\n",
    "        x = self._dec(h)\n",
    "        if self._dec_act is not None:\n",
    "            x = self._dec_act(x)\n",
    "        return x\n",
    "\n",
    "    def get_gamma(self, z, z_mean, z_log_var):\n",
    "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
    "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
    "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
    "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
    "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
    "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
    "\n",
    "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
    "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
    "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    def loss_function(self, recon_x, x, z, z_mean, z_log_var):\n",
    "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
    "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
    "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
    "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
    "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
    "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
    "\n",
    "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
    "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
    "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True) # NxK\n",
    "\n",
    "        BCE = -torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
    "            (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n",
    "        logpzc = torch.sum(0.5*gamma*torch.sum(math.log(2*math.pi)+torch.log(lambda_tensor3)+\\\n",
    "            torch.exp(z_log_var_t)/lambda_tensor3 + (z_mean_t-u_tensor3)**2/lambda_tensor3, dim=1), dim=1)\n",
    "        qentropy = -0.5*torch.sum(1+z_log_var+math.log(2*math.pi), 1)\n",
    "        logpc = -torch.sum(torch.log(theta_tensor2)*gamma, 1)\n",
    "        logqcx = torch.sum(torch.log(gamma)*gamma, 1)\n",
    "\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        loss = torch.mean(BCE + logpzc + qentropy + logpc + logqcx)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self._enc_mu(h)\n",
    "        logvar = self._enc_log_sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, self.decode(z), mu, logvar\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 63)\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "all_data = []\n",
    "for d in data:\n",
    "    d = pickle.loads(d)\n",
    "    pos = []\n",
    "    for f in d[\"frames\"]:\n",
    "        p = [jo[\"pos\"] for jo in f]\n",
    "        pos.append(p)\n",
    "    all_data.append(pos)\n",
    "\n",
    "input_data = np.array([np.concatenate([p for p in j]) for pos in all_data for j in pos])\n",
    "print(input_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train:  1007 , Validation:  216 , Test:  217\n"
     ]
    }
   ],
   "source": [
    "data_ratio = (.7, .15, .15) # training, validation, testing\n",
    "SEED = 2021\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(input_data).float()\n",
    "y_tensor = torch.from_numpy(input_data).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "N = len(dataset)\n",
    "\n",
    "train_ratio = int(data_ratio[0]*N)\n",
    "val_ratio = int(data_ratio[1] * N)\n",
    "test_ratio = int(N-train_ratio-val_ratio)\n",
    "print(\"Train: \", train_ratio, \", Validation: \", val_ratio, \", Test: \", test_ratio)\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEC_AE(\n",
      "  (encoder): DEC(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (linear): Linear(in_features=63, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=36, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (assignment): ClusterAssignment()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (linear): Linear(in_features=36, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=63, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_dim = input_data.shape[1]\n",
    "output_dim = input_data.shape[1]\n",
    "latent_dim = 36         # 12 * 3\n",
    "encoder_layer_sizes = [input_dim, 256, 256, latent_dim]\n",
    "decoder_layer_sizes = [latent_dim, 256, 256, output_dim]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "act_fn = \"elu\"\n",
    "keep_prob = .2\n",
    "\n",
    "# model, loss and scheduler\n",
    "ae = StackedDenoisingAutoEncoder(encoder_layer_sizes, activation=nn.ELU(), final_activation=nn.ELU())\n",
    "model = DEC_AE(DEC(36, 36, ae.encoder), ae.decoder)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1884\n",
      "Epoch [2/100], Loss: 0.1721\n",
      "Epoch [3/100], Loss: 0.1721\n",
      "Epoch [4/100], Loss: 0.1721\n",
      "Epoch [5/100], Loss: 0.1721\n",
      "Epoch [6/100], Loss: 0.1721\n",
      "Epoch [7/100], Loss: 0.1721\n",
      "Early stopping at Epoch:  6\n",
      "last training loss: 0.172100\n",
      "achieved best validation loss: 0.1820 after at Epoch 0\n",
      "Test loss: 0.1849\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "i = 0\n",
    "n_epochs_no_improve = 5\n",
    "\n",
    "train_loader_len = float(len(train_loader))\n",
    "val_loader_len = float(len(val_loader))\n",
    "test_loader_len = float(len(test_loader))\n",
    "\n",
    "last_avg_training_loss = 0\n",
    "min_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_after_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # training\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        training_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    last_avg_training_loss = training_loss / train_loader_len\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, last_avg_training_loss))\n",
    "\n",
    "    # early stopping\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            pred_val = model(inputs)\n",
    "            loss_val = criterion(pred_val, labels)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= val_loader_len\n",
    "        if min_loss > val_loss:\n",
    "            min_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_after_epoch = epoch\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve > n_epochs_no_improve:\n",
    "                print(\"Early stopping at Epoch: \", epoch)\n",
    "                print(\"last training loss: {:2f}\".format(last_avg_training_loss))\n",
    "                print(\"achieved best validation loss: {:.4f} after at Epoch {}\".format(min_loss, best_model_after_epoch))\n",
    "                break\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        pred_test = model(inputs)\n",
    "        loss_test = criterion(pred_test, labels)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= test_loader_len\n",
    "    print(\"Test loss: {:.4f}\".format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}