{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RBF-MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses RBF-layer with Gaussian kernel for encoder\n",
    "\n",
    "$\n",
    "enc = rbf(X, \\theta, k),    \\quad \\theta = W,b \\quad k = c_1, c_2,..., c_k\n",
    "$\n",
    "\n",
    "$\n",
    "rbf(X, \\theta, k) = \\phi(\\sigma||X - C||)\n",
    "$\n",
    "\n",
    "$\n",
    "dec = mlp(X, \\theta), \\quad \\theta = W,b\n",
    "$\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# RBF Layer\n",
    "class RBF_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    from JeremyLinux on GitHub {https://github.com/JeremyLinux/PyTorch-Radial-Basis-Function-Layer/blob/master/Torch%20RBF/torch_rbf.py}\n",
    "\n",
    "    Transforms incoming data using a given radial basis function:\n",
    "    u_{i} = rbf(||x - c_{i}|| / s_{i})\n",
    "    Arguments:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "    Shape:\n",
    "        - Input: (N, in_features) where N is an arbitrary batch size\n",
    "        - Output: (N, out_features) where N is an arbitrary batch size\n",
    "    Attributes:\n",
    "        centres: the learnable centres of shape (out_features, in_features).\n",
    "            The values are initialised from a standard normal distribution.\n",
    "            Normalising inputs to have mean 0 and standard deviation 1 is\n",
    "            recommended.\n",
    "\n",
    "        sigmas: the learnable scaling factors of shape (out_features).\n",
    "            The values are initialised as ones.\n",
    "\n",
    "        basis_func: the radial basis function used to transform the scaled\n",
    "            distances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, basis_func:func):\n",
    "        super(RBF_Layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.centres = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sigmas = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.basis_func = basis_func\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.centres, 0, 1)\n",
    "        nn.init.constant_(self.sigmas, .01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = (x.size(0), self.out_features, self.in_features)\n",
    "        x = x.unsqueeze(1).expand(size)\n",
    "        c = self.centres.unsqueeze(0).expand(size)\n",
    "        distances = (x - c).pow(2).sum(-1).pow(0.5) * self.sigmas.unsqueeze(0)  # ALT. / (2*sigma**2)\n",
    "        return self.basis_func(distances)\n",
    "    def __str__(self):\n",
    "        return \"RFB(in={}, out={}, centers={}, sigma={}\".format(\n",
    "            self.in_features, self.out_features, self.centres.size(), self.sigmas.size())\n",
    "\n",
    "\n",
    "def gaussian(alpha):\n",
    "    phi = torch.exp(-1*alpha.pow(2))\n",
    "    return phi\n",
    "\n",
    "def linear(alpha):\n",
    "    phi = alpha\n",
    "    return phi\n",
    "\n",
    "def quadratic(alpha):\n",
    "    phi = alpha.pow(2)\n",
    "    return phi\n",
    "\n",
    "def inverse_quadratic(alpha):\n",
    "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2))\n",
    "    return phi\n",
    "\n",
    "def multiquadric(alpha):\n",
    "    phi = (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
    "    return phi\n",
    "\n",
    "def inverse_multiquadric(alpha):\n",
    "    phi = torch.ones_like(alpha) / (torch.ones_like(alpha) + alpha.pow(2)).pow(0.5)\n",
    "    return phi\n",
    "\n",
    "def spline(alpha):\n",
    "    phi = (alpha.pow(2) * torch.log(alpha + torch.ones_like(alpha)))\n",
    "    return phi\n",
    "\n",
    "def poisson_one(alpha):\n",
    "    phi = (alpha - torch.ones_like(alpha)) * torch.exp(-alpha)\n",
    "    return phi\n",
    "\n",
    "def poisson_two(alpha):\n",
    "    phi = ((alpha - 2*torch.ones_like(alpha)) / 2*torch.ones_like(alpha)) \\\n",
    "    * alpha * torch.exp(-alpha)\n",
    "    return phi\n",
    "\n",
    "def matern32(alpha):\n",
    "    phi = (torch.ones_like(alpha) + 3**0.5*alpha)*torch.exp(-3**0.5*alpha)\n",
    "    return phi\n",
    "\n",
    "def matern52(alpha):\n",
    "    phi = (torch.ones_like(alpha) + 5**0.5*alpha + (5/3) \\\n",
    "    * alpha.pow(2))*torch.exp(-5**0.5*alpha)\n",
    "    return phi\n",
    "\n",
    "def basis_func_dict():\n",
    "    \"\"\"\n",
    "    A helper function that returns a dictionary containing each RBF\n",
    "    \"\"\"\n",
    "    bases = {'gaussian': gaussian,\n",
    "             'linear': linear,\n",
    "             'quadratic': quadratic,\n",
    "             'inverse quadratic': inverse_quadratic,\n",
    "             'multiquadric': multiquadric,\n",
    "             'inverse multiquadric': inverse_multiquadric,\n",
    "             'spline': spline,\n",
    "             'poisson one': poisson_one,\n",
    "             'poisson two': poisson_two,\n",
    "             'matern32': matern32,\n",
    "             'matern52': matern52}\n",
    "    return bases\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self, dimensions:list, act_fn:str, rbf_index:list, rbf_kernel:str, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(RBF, self).__init__()\n",
    "        self.dimensions = dimensions          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.act= act_fn                     #   func\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "        self.batch_size = batch_size        #   int\n",
    "        self.rbf_index=rbf_index\n",
    "        self.kernel = basis_func_dict()[rbf_kernel]\n",
    "        self.model = []\n",
    "\n",
    "        assert(len(dimensions) >= 2)\n",
    "        assert(batch_size > 0)\n",
    "        assert(act_fn == \"elu\" or act_fn == \"relu\")\n",
    "        assert(keep_prob < 1)\n",
    "        assert(len(rbf_index) > 0)\n",
    "        for e in dimensions: assert(type(e) == int)\n",
    "\n",
    "        self.build()\n",
    "        self.model.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        j = 0\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            if i == self.rbf_index[j]:\n",
    "                layers.append((\"rbf\"+str(j), RBF_Layer(size[0], size[1], self.kernel)))\n",
    "                j+=1\n",
    "            else:\n",
    "                layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "                if i < len(self.dimensions)-2:\n",
    "                    layers.append((\"act\"+str(i), self.activation(self.act)))\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "\n",
    "        self.model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(fn_name):\n",
    "        if fn_name == \"elu\":\n",
    "            return nn.ELU()\n",
    "        elif fn_name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        else:\n",
    "            return nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(.01)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dimensions:list, act_fn, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dimensions = dimensions          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.act= act_fn                     #   func\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "        self.batch_size = batch_size        #   int\n",
    "\n",
    "        self.model = []\n",
    "\n",
    "        assert(len(dimensions) >= 2)\n",
    "        assert(batch_size > 0)\n",
    "        assert(act_fn == \"elu\" or act_fn == \"relu\")\n",
    "        assert(keep_prob < 1)\n",
    "        for e in dimensions: assert(type(e) == int)\n",
    "\n",
    "        self.build()\n",
    "        self.model.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), self.activation(self.act)))\n",
    "                layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "\n",
    "        self.model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(fn_name):\n",
    "        if fn_name == \"elu\":\n",
    "            return nn.ELU()\n",
    "        elif fn_name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        else:\n",
    "            return nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(.01)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class RBF_AE(nn.Module):\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module):\n",
    "        super(RBF_AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 63)\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "all_data = []\n",
    "for d in data:\n",
    "    d = pickle.loads(d)\n",
    "    pos = []\n",
    "    for f in d[\"frames\"]:\n",
    "        p = [jo[\"pos\"] for jo in f]\n",
    "        pos.append(p)\n",
    "    all_data.append(pos)\n",
    "\n",
    "input_data = np.array([np.concatenate([p for p in j]) for pos in all_data for j in pos])\n",
    "print(input_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1007 , Validation:  216 , Test:  217\n"
     ]
    }
   ],
   "source": [
    "data_ratio = (.7, .15, .15) # training, validation, testing\n",
    "SEED = 2021\n",
    "batch_size = 1\n",
    "\n",
    "x_tensor = torch.from_numpy(input_data).float()\n",
    "y_tensor = torch.from_numpy(input_data).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "N = len(dataset)\n",
    "\n",
    "train_ratio = int(data_ratio[0]*N)\n",
    "val_ratio = int(data_ratio[1] * N)\n",
    "test_ratio = int(N-train_ratio-val_ratio)\n",
    "print(\"Train: \", train_ratio, \", Validation: \", val_ratio, \", Test: \", test_ratio)\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF_AE(\n",
      "  (encoder): RBF(\n",
      "    (model): Sequential(\n",
      "      (fc0): Linear(in_features=63, out_features=256, bias=True)\n",
      "      (act0): ELU(alpha=1.0)\n",
      "      (drop1): Dropout(p=0.2, inplace=False)\n",
      "      (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (act1): ELU(alpha=1.0)\n",
      "      (drop2): Dropout(p=0.2, inplace=False)\n",
      "      (rbf0): RBF_Layer()\n",
      "    )\n",
      "  )\n",
      "  (decoder): MLP(\n",
      "    (model): Sequential(\n",
      "      (fc0): Linear(in_features=36, out_features=256, bias=True)\n",
      "      (act0): ELU(alpha=1.0)\n",
      "      (drop1): Dropout(p=0.2, inplace=False)\n",
      "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (act1): ELU(alpha=1.0)\n",
      "      (drop2): Dropout(p=0.2, inplace=False)\n",
      "      (fc2): Linear(in_features=256, out_features=63, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_dim = input_data.shape[1]\n",
    "output_dim = input_data.shape[1]\n",
    "latent_dim = 36         # 12 * 3\n",
    "encoder_layer_sizes = [input_dim, 256, 128, latent_dim]\n",
    "decoder_layer_sizes = [latent_dim, 256, 256, output_dim]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "act_fn = \"elu\"\n",
    "keep_prob = .2\n",
    "\n",
    "# model, loss and scheduler\n",
    "encoder = RBF(encoder_layer_sizes, act_fn, [2], \"gaussian\", keep_prob, batch_size)\n",
    "decoder = MLP(decoder_layer_sizes, act_fn, keep_prob, batch_size)\n",
    "model = RBF_AE(encoder, decoder)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1236\n",
      "Epoch [2/100], Loss: 0.0950\n",
      "Epoch [3/100], Loss: 0.0929\n",
      "Epoch [4/100], Loss: 0.0932\n",
      "Epoch [5/100], Loss: 0.0939\n",
      "Epoch [6/100], Loss: 0.0930\n",
      "Epoch [7/100], Loss: 0.0927\n",
      "Epoch [8/100], Loss: 0.0939\n",
      "Early stopping at Epoch:  7\n",
      "last training loss: 0.093866\n",
      "achieved best validation loss: 0.0955 after at Epoch 1\n",
      "Test loss: 0.1037\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "i = 0\n",
    "n_epochs_no_improve = 5\n",
    "\n",
    "train_loader_len = float(len(train_loader))\n",
    "val_loader_len = float(len(val_loader))\n",
    "test_loader_len = float(len(test_loader))\n",
    "\n",
    "last_avg_training_loss = 0\n",
    "min_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_after_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # training\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        training_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    last_avg_training_loss = training_loss / train_loader_len\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, last_avg_training_loss))\n",
    "\n",
    "    # early stopping\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            pred_val = model(inputs)\n",
    "            loss_val = criterion(pred_val, labels)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= val_loader_len\n",
    "        if min_loss > val_loss:\n",
    "            min_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_after_epoch = epoch\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve > n_epochs_no_improve:\n",
    "                print(\"Early stopping at Epoch: \", epoch)\n",
    "                print(\"last training loss: {:2f}\".format(last_avg_training_loss))\n",
    "                print(\"achieved best validation loss: {:.4f} after at Epoch {}\".format(min_loss, best_model_after_epoch))\n",
    "                break\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        pred_test = model(inputs)\n",
    "        loss_test = criterion(pred_test, labels)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= test_loader_len\n",
    "    print(\"Test loss: {:.4f}\".format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}