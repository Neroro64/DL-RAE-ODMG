{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "import json as js\n",
    "import _pickle as pickle\n",
    "import bz2\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_path = \"../../data/\"\n",
    "# load data\n",
    "data = func.load(data_path+\"LOCO_R2-default-locomotion.pbz2\")\n",
    "data_2 = func.load(data_path+\"LOCO_R2-default-locomotion-small.pbz2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DEC-MLP Autoencoder\n",
    "$\n",
    "f(x,\\theta) = dec(enc(x,\\theta_1), \\theta_2) = x,   \\quad \\theta = (\\theta_1, \\theta_2)\n",
    "$\n",
    "\n",
    "$\n",
    "enc(x, \\theta_1) = z, \\quad   z \\in Z \\quad \\text{ = latent space}\n",
    "$\n",
    "\n",
    "$\n",
    "dec(z, \\theta_2) = x, \\quad   x \\in X \\quad \\text{ = input space}\n",
    "$\n",
    "\n",
    "This model uses Deep Embedded Clustering (DEC) model for encoder\n",
    "\n",
    "$\n",
    "enc = dec(x, \\theta) = z, \\quad z \\in Z\n",
    "$\n",
    "\n",
    "$\n",
    "dec = mlp(X, \\theta), \\quad \\theta = W,b\n",
    "$\n",
    "\n",
    "$\n",
    "mlp(X, W) = f(f(X \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from fabiozinno {https://github.com/electronicarts/character-motion-vaes/blob/main/vae_motion/models.py}\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, frame_size, latent_size, normalization):\n",
    "        super().__init__()\n",
    "        self.frame_size = frame_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.mode = normalization.get(\"mode\")\n",
    "        self.data_max = normalization.get(\"max\")\n",
    "        self.data_min = normalization.get(\"min\")\n",
    "        self.data_avg = normalization.get(\"avg\")\n",
    "        self.data_std = normalization.get(\"std\")\n",
    "\n",
    "        h1 = 256\n",
    "        h2 = 128\n",
    "        # Encoder\n",
    "        # Takes pose | condition (n * poses) as input\n",
    "        self.fc1 = nn.Linear(frame_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, latent_size)\n",
    "\n",
    "        # Decoder\n",
    "        # Takes latent | condition as input\n",
    "        self.fc4 = nn.Linear(latent_size, h2)\n",
    "        self.fc5 = nn.Linear(h2, h1)\n",
    "        self.fc6 = nn.Linear(h1, frame_size)\n",
    "\n",
    "    def normalize(self, t):\n",
    "        if self.mode == \"minmax\":\n",
    "            return 2 * (t - self.data_min) / (self.data_max - self.data_min) - 1\n",
    "        elif self.mode == \"zscore\":\n",
    "            return (t - self.data_avg) / self.data_std\n",
    "        elif self.mode == \"none\":\n",
    "            return t\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization mode\")\n",
    "\n",
    "    def denormalize(self, t):\n",
    "        if self.mode == \"minmax\":\n",
    "            return (t + 1) * (self.data_max - self.data_min) / 2 + self.data_min\n",
    "        elif self.mode == \"zscore\":\n",
    "            return t * self.data_std + self.data_avg\n",
    "        elif self.mode == \"none\":\n",
    "            return t\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization mode\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        return self.decode(latent)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc3(h2)\n",
    "\n",
    "    def decode(self, x):\n",
    "        h4 = F.relu(self.fc4(x))\n",
    "        h5 = F.relu(self.fc5(h4))\n",
    "        return self.fc6(h5)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_size,\n",
    "        latent_size,\n",
    "        hidden_size,\n",
    "        num_condition_frames,\n",
    "        num_future_predictions,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        # Takes pose | condition (n * poses) as input\n",
    "        input_size = frame_size * (num_future_predictions + num_condition_frames)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(frame_size + hidden_size, hidden_size)\n",
    "        self.mu = nn.Linear(frame_size + hidden_size, latent_size)\n",
    "        self.logvar = nn.Linear(frame_size + hidden_size, latent_size)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h1 = F.elu(self.fc1(torch.cat((x, c), dim=1)))\n",
    "        h2 = F.elu(self.fc2(torch.cat((x, h1), dim=1)))\n",
    "        s = torch.cat((x, h2), dim=1)\n",
    "        return self.mu(s), self.logvar(s)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_size,\n",
    "        latent_size,\n",
    "        hidden_size,\n",
    "        num_condition_frames,\n",
    "        num_future_predictions,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Decoder\n",
    "        # Takes latent | condition as input\n",
    "        input_size = latent_size + frame_size * num_condition_frames\n",
    "        output_size = num_future_predictions * frame_size\n",
    "        self.fc4 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(latent_size + hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(latent_size + hidden_size, output_size)\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        h4 = F.elu(self.fc4(torch.cat((z, c), dim=1)))\n",
    "        h5 = F.elu(self.fc5(torch.cat((z, h4), dim=1)))\n",
    "        return self.out(torch.cat((z, h5), dim=1))\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        return self.decode(z, c)\n",
    "\n",
    "\n",
    "class MixedDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_size,\n",
    "        latent_size,\n",
    "        hidden_size,\n",
    "        num_condition_frames,\n",
    "        num_future_predictions,\n",
    "        num_experts,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        input_size = latent_size + frame_size * num_condition_frames\n",
    "        inter_size = latent_size + hidden_size\n",
    "        output_size = num_future_predictions * frame_size\n",
    "        self.decoder_layers = [\n",
    "            (\n",
    "                nn.Parameter(torch.empty(num_experts, input_size, hidden_size)),\n",
    "                nn.Parameter(torch.empty(num_experts, hidden_size)),\n",
    "                F.elu,\n",
    "            ),\n",
    "            (\n",
    "                nn.Parameter(torch.empty(num_experts, inter_size, hidden_size)),\n",
    "                nn.Parameter(torch.empty(num_experts, hidden_size)),\n",
    "                F.elu,\n",
    "            ),\n",
    "            (\n",
    "                nn.Parameter(torch.empty(num_experts, inter_size, output_size)),\n",
    "                nn.Parameter(torch.empty(num_experts, output_size)),\n",
    "                None,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for index, (weight, bias, _) in enumerate(self.decoder_layers):\n",
    "            index = str(index)\n",
    "            torch.nn.init.kaiming_uniform_(weight)\n",
    "            bias.data.fill_(0.01)\n",
    "            self.register_parameter(\"w\" + index, weight)\n",
    "            self.register_parameter(\"b\" + index, bias)\n",
    "\n",
    "        # Gating network\n",
    "        gate_hsize = 64\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, gate_hsize),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(gate_hsize, gate_hsize),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(gate_hsize, num_experts),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        coefficients = F.softmax(self.gate(torch.cat((z, c), dim=1)), dim=1)\n",
    "        layer_out = c\n",
    "\n",
    "        for (weight, bias, activation) in self.decoder_layers:\n",
    "            flat_weight = weight.flatten(start_dim=1, end_dim=2)\n",
    "            mixed_weight = torch.matmul(coefficients, flat_weight).view(\n",
    "                coefficients.shape[0], *weight.shape[1:3]\n",
    "            )\n",
    "\n",
    "            input = torch.cat((z, layer_out), dim=1).unsqueeze(1)\n",
    "            mixed_bias = torch.matmul(coefficients, bias).unsqueeze(1)\n",
    "            out = torch.baddbmm(mixed_bias, input, mixed_weight).squeeze(1)\n",
    "            layer_out = activation(out) if activation is not None else out\n",
    "\n",
    "        return layer_out\n",
    "\n",
    "\n",
    "class PoseMixtureVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_size,\n",
    "        latent_size,\n",
    "        num_condition_frames,\n",
    "        num_future_predictions,\n",
    "        normalization,\n",
    "        num_experts,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.frame_size = frame_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_condition_frames = num_condition_frames\n",
    "        self.num_future_predictions = num_future_predictions\n",
    "\n",
    "        self.mode = normalization.get(\"mode\")\n",
    "        self.data_max = normalization.get(\"max\")\n",
    "        self.data_min = normalization.get(\"min\")\n",
    "        self.data_avg = normalization.get(\"avg\")\n",
    "        self.data_std = normalization.get(\"std\")\n",
    "\n",
    "        hidden_size = 256\n",
    "        args = (\n",
    "            frame_size,\n",
    "            latent_size,\n",
    "            hidden_size,\n",
    "            num_condition_frames,\n",
    "            num_future_predictions,\n",
    "        )\n",
    "\n",
    "        self.encoder = Encoder(*args)\n",
    "        self.decoder = MixedDecoder(*args, num_experts)\n",
    "\n",
    "    def normalize(self, t):\n",
    "        if self.mode == \"minmax\":\n",
    "            return 2 * (t - self.data_min) / (self.data_max - self.data_min) - 1\n",
    "        elif self.mode == \"zscore\":\n",
    "            return (t - self.data_avg) / self.data_std\n",
    "        elif self.mode == \"none\":\n",
    "            return t\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization mode\")\n",
    "\n",
    "    def denormalize(self, t):\n",
    "        if self.mode == \"minmax\":\n",
    "            return (t + 1) * (self.data_max - self.data_min) / 2 + self.data_min\n",
    "        elif self.mode == \"zscore\":\n",
    "            return t * self.data_std + self.data_avg\n",
    "        elif self.mode == \"none\":\n",
    "            return t\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization mode\")\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        _, mu, logvar = self.encoder(x, c)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        z, mu, logvar = self.encoder(x, c)\n",
    "        return self.decoder(z, c), mu, logvar\n",
    "\n",
    "    def sample(self, z, c, deterministic=False):\n",
    "        return self.decoder(z, c)\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # self.embedding = nn.Embedding(self.num_embeddings, self.latent_size)\n",
    "        # self.embedding.weight.data.normal_()\n",
    "\n",
    "        embed = torch.randn(latent_size, num_embeddings)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "        self.commitment_cost = 0.25\n",
    "        self.decay = 0.99\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate distances\n",
    "        dist = (\n",
    "            inputs.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * inputs @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.num_embeddings).type(inputs.dtype)\n",
    "        embed_ind = embed_ind.view(*inputs.shape[:-1])\n",
    "        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n",
    "\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                1 - self.decay, embed_onehot.sum(0)\n",
    "            )\n",
    "\n",
    "            embed_sum = inputs.transpose(0, 1) @ embed_onehot\n",
    "            self.embed_avg.data.mul_(self.decay).add_(1 - self.decay, embed_sum)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.epsilon)\n",
    "                / (n + self.num_embeddings * self.epsilon)\n",
    "                * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        loss = (quantize.detach() - inputs).pow(2).mean()\n",
    "        quantize = inputs + (quantize - inputs).detach()\n",
    "\n",
    "        avg_probs = embed_onehot.mean(dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * (avg_probs + 1e-10).log()))\n",
    "\n",
    "        return quantize, loss, perplexity, embed_ind"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Enc(nn.Module):\n",
    "    def __init__(self, dimensions:list, k:int, act_fn:str, keep_prob=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dimensions = dimensions\n",
    "        self.act_fn = act_fn\n",
    "        self.keep_prob = keep_prob\n",
    "        self.k = k\n",
    "\n",
    "        self.model = None\n",
    "        self.mu = None\n",
    "        self.logvar = None\n",
    "\n",
    "        self.build()\n",
    "        self.model.apply(self.init_params)\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), self.activation(self.act)))\n",
    "                if (self.keep_prob > 0):\n",
    "                    layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "        self.model = nn.Sequential(OrderedDict(layers))\n",
    "        self.mu = nn.Linear(self.dimensions[-1], self.k)\n",
    "        self.logvar = nn.Linear(self.dimensions[-1], self.k)\n",
    "\n",
    "    def activation(self):\n",
    "        if self.act_fn == \"elu\":\n",
    "            return nn.ELU()\n",
    "        elif self.act_fn == \"relu\":\n",
    "            return nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m:nn.Module):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(.01)\n",
    "\n",
    "    def reparameterize(self, mu:torch.Tensor, logvar:torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x:torch.Tensor) -> (torch.Tensor,torch.Tensor):\n",
    "        encoded = self.model(x)\n",
    "        return self.mu(encoded), self.logvar(encoded)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dimensions:list, act_fn:str, keep_prob:float=.2, batch_size:int=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dimensions = dimensions          #   [(in, h1), (h1, h2), ..., (hn, out)]\n",
    "        self.act= act_fn                     #   func\n",
    "        self.keep_prob = keep_prob          #   %\n",
    "        self.batch_size = batch_size        #   int\n",
    "\n",
    "        self.model = []\n",
    "\n",
    "        assert(len(dimensions) >= 2)\n",
    "        assert(batch_size > 0)\n",
    "        assert(act_fn == \"elu\" or act_fn == \"relu\")\n",
    "        assert(keep_prob < 1)\n",
    "        for e in dimensions: assert(type(e) == int)\n",
    "\n",
    "        self.build()\n",
    "        self.model.apply(self.init_params)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        layers = []\n",
    "        for i, size in enumerate(zip(self.dimensions[0:], self.dimensions[1:])):\n",
    "            layers.append((\"fc\"+str(i), nn.Linear(size[0], size[1])))\n",
    "            if i < len(self.dimensions)-2:\n",
    "                layers.append((\"act\"+str(i), self.activation(self.act)))\n",
    "                layers.append((\"drop\"+str(i+1), nn.Dropout(self.keep_prob)))\n",
    "\n",
    "        self.model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(fn_name):\n",
    "        if fn_name == \"elu\":\n",
    "            return nn.ELU()\n",
    "        elif fn_name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        else:\n",
    "            return nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_params(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(.01)\n",
    "\n",
    "\n",
    "class VAE_AE(nn.Module):\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module):\n",
    "        super(VAE_AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 63)\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "all_data = []\n",
    "for d in data:\n",
    "    d = pickle.loads(d)\n",
    "    pos = []\n",
    "    for f in d[\"frames\"]:\n",
    "        p = [jo[\"pos\"] for jo in f]\n",
    "        pos.append(p)\n",
    "    all_data.append(pos)\n",
    "\n",
    "input_data = np.array([np.concatenate([p for p in j]) for pos in all_data for j in pos])\n",
    "print(input_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train:  1007 , Validation:  216 , Test:  217\n"
     ]
    }
   ],
   "source": [
    "data_ratio = (.7, .15, .15) # training, validation, testing\n",
    "SEED = 2021\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(input_data).float()\n",
    "y_tensor = torch.from_numpy(input_data).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "N = len(dataset)\n",
    "\n",
    "train_ratio = int(data_ratio[0]*N)\n",
    "val_ratio = int(data_ratio[1] * N)\n",
    "test_ratio = int(N-train_ratio-val_ratio)\n",
    "print(\"Train: \", train_ratio, \", Validation: \", val_ratio, \", Test: \", test_ratio)\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_ratio, val_ratio, test_ratio], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEC_AE(\n",
      "  (encoder): DEC(\n",
      "    (encoder): Sequential(\n",
      "      (0): Sequential(\n",
      "        (linear): Linear(in_features=63, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (linear): Linear(in_features=256, out_features=36, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (assignment): ClusterAssignment()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (linear): Linear(in_features=36, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (linear): Linear(in_features=256, out_features=63, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_dim = input_data.shape[1]\n",
    "output_dim = input_data.shape[1]\n",
    "latent_dim = 36         # 12 * 3\n",
    "encoder_layer_sizes = [input_dim, 256, 256, latent_dim]\n",
    "decoder_layer_sizes = [latent_dim, 256, 256, output_dim]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "act_fn = \"elu\"\n",
    "keep_prob = .2\n",
    "\n",
    "# model, loss and scheduler\n",
    "ae = StackedDenoisingAutoEncoder(encoder_layer_sizes, activation=nn.ELU(), final_activation=nn.ELU())\n",
    "model = DEC_AE(DEC(36, 36, ae.encoder), ae.decoder)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1884\n",
      "Epoch [2/100], Loss: 0.1721\n",
      "Epoch [3/100], Loss: 0.1721\n",
      "Epoch [4/100], Loss: 0.1721\n",
      "Epoch [5/100], Loss: 0.1721\n",
      "Epoch [6/100], Loss: 0.1721\n",
      "Epoch [7/100], Loss: 0.1721\n",
      "Early stopping at Epoch:  6\n",
      "last training loss: 0.172100\n",
      "achieved best validation loss: 0.1820 after at Epoch 0\n",
      "Test loss: 0.1849\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "i = 0\n",
    "n_epochs_no_improve = 5\n",
    "\n",
    "train_loader_len = float(len(train_loader))\n",
    "val_loader_len = float(len(val_loader))\n",
    "test_loader_len = float(len(test_loader))\n",
    "\n",
    "last_avg_training_loss = 0\n",
    "min_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_after_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # training\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        training_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    last_avg_training_loss = training_loss / train_loader_len\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, last_avg_training_loss))\n",
    "\n",
    "    # early stopping\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            pred_val = model(inputs)\n",
    "            loss_val = criterion(pred_val, labels)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= val_loader_len\n",
    "        if min_loss > val_loss:\n",
    "            min_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_after_epoch = epoch\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve > n_epochs_no_improve:\n",
    "                print(\"Early stopping at Epoch: \", epoch)\n",
    "                print(\"last training loss: {:2f}\".format(last_avg_training_loss))\n",
    "                print(\"achieved best validation loss: {:.4f} after at Epoch {}\".format(min_loss, best_model_after_epoch))\n",
    "                break\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        pred_test = model(inputs)\n",
    "        loss_test = criterion(pred_test, labels)\n",
    "        test_loss += loss_test.item()\n",
    "\n",
    "    test_loss /= test_loader_len\n",
    "    print(\"Test loss: {:.4f}\".format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}